from flask import Blueprint, jsonify, request
import os
import sys
import subprocess
from pathlib import Path
from datetime import datetime
from services.crawling_service.urlCrawling import main as crawl_urls
from routes.urlLoad_routes import get_root_urls_from_firebase, save_crawling_url_to_firebase, get_urls_from_firebase, save_document_url_to_firebase, save_urls_batch
from services.crawling_service.crawling_and_structuring import main as crawling_and_structuring
from firebase_admin import firestore
from services.crawling_service import line1

crawling_bp = Blueprint('crawling', __name__)

# Firestore í´ë¼ì´ì–¸íŠ¸
db = firestore.client()

#url í¬ë¡¤ë§ ì‹œì‘
@crawling_bp.route('/start-crawling/<page_id>', methods=['POST'])
def start_url_crawling(page_id):
    """ì €ì¥ëœ URLë“¤ì„ ê°€ì ¸ì™€ì„œ í¬ë¡¤ë§ ì‹œì‘"""
    try:
        print(f"ğŸš€ URL í¬ë¡¤ë§ ì‹œì‘: {page_id}")
        
        # 1. Firebaseì—ì„œ ì €ì¥ëœ URLë“¤ ê°€ì ¸ì˜¤ê¸°
        saved_urls = get_root_urls_from_firebase(page_id)
        
        print(f"ğŸ“‹ Firebaseì—ì„œ ê°€ì ¸ì˜¨ root URL ê°œìˆ˜: {len(saved_urls) if saved_urls else 0}")
        if saved_urls:
            for i, url_info in enumerate(saved_urls):
                print(f"   {i+1}. {url_info.get('url', 'N/A')} (ë‚ ì§œ: {url_info.get('date', 'N/A')})")
        
        if not saved_urls:
            return jsonify({"success": False, "error": "í¬ë¡¤ë§í•  URLì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € URLì„ ì¶”ê°€í•´ì£¼ì„¸ìš”."}), 400
        
        # 2. forë¬¸ìœ¼ë¡œ saved_urls ìˆœíšŒí•˜ë©° í¬ë¡¤ë§
        for url in saved_urls:
            start_url = url['url']
            print(f"ğŸ” URL í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘: {start_url}")

            # 3. url í¬ë¡¤ë§ ì‹¤í–‰            
            crawling_results = crawl_urls(
                start_url=start_url,
            )
            
            # crawling_resultsëŠ” ì§ì ‘ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ì…ë‹ˆë‹¤
            if crawling_results and "error" not in crawling_results:
                print(f"âœ… URL í¬ë¡¤ë§ ì„±ê³µ: {len(crawling_results.get('page_urls', []))}ê°œ í˜ì´ì§€ ë°œê²¬")
                
                # 4. url í¬ë¡¤ë§ ê²°ê³¼ë¥¼ Firebaseì— ì €ì¥
                results_info = {
                    "page_id": page_id,
                    "start_url": start_url,
                    "base_domain": crawling_results.get('base_domain', ''),
                    "scope_patterns": crawling_results.get('scope_patterns', []),
                    "total_pages": crawling_results.get('total_pages_discovered', 0),
                    "total_documents": crawling_results.get('total_documents_discovered', 0),
                    "results_dir": crawling_results.get('results_dir', ''),
                    "execution_time": crawling_results.get('execution_time', 0),
                    "timestamp": datetime.now().isoformat()
                }

                # page_urlsì—ì„œ URL ëª©ë¡ì„ ê°€ì ¸ì™€ì„œ Firebaseì— ì €ì¥ (ë°°ì¹˜ ì²˜ë¦¬)
                page_urls = crawling_results.get('page_urls', [])
                saved_count = save_urls_batch(page_id, page_urls, "crawled")
                
                # doc_urlsì—ì„œ ë¬¸ì„œ URL ëª©ë¡ì„ ê°€ì ¸ì™€ì„œ Firebaseì— ì €ì¥ (ë°°ì¹˜ ì²˜ë¦¬)
                doc_urls = crawling_results.get('doc_urls', [])
                doc_saved_count = save_urls_batch(page_id, doc_urls, "document")
                
                print(f"ğŸ’¾ Firebaseì— {saved_count}ê°œ í˜ì´ì§€ URL, {doc_saved_count}ê°œ ë¬¸ì„œ URL ì €ì¥ ì™„ë£Œ")
                print(f"ğŸ“ ë°œê²¬ëœ ë¬¸ì„œ URL: {crawling_results.get('total_documents_discovered', 0)}ê°œ")
            
                return jsonify({
                    "success": True,
                    "message": "URL í¬ë¡¤ë§ ì™„ë£Œ",
                    "results": results_info
                }), 200
            else:
                print(f"âŒ URL í¬ë¡¤ë§ ì‹¤íŒ¨: {crawling_results.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                return jsonify({
                    "success": False, 
                    "error": f"í¬ë¡¤ë§ ì‹¤íŒ¨: {crawling_results.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
                }), 500
            
    except Exception as e:
        print(f"URL í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return jsonify({"success": False, "error": f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}), 500

@crawling_bp.route('/crawl-and-structure/<page_id>', methods=['POST'])
def crawl_and_structure(page_id):
    """ì›¹ í¬ë¡¤ë§ ë° êµ¬ì¡°í™” ì‹œì‘ (crawling_and_structuring.py)"""
    try:
        print(f"crawling_routes.py: ğŸ”„ ì›¹ í¬ë¡¤ë§ ë° êµ¬ì¡°í™” ì‹œì‘: {page_id}")
        
        # 1. Firebaseì—ì„œ ì €ì¥ëœ URLë“¤ ê°€ì ¸ì˜¤ê¸° (í¬ë¡¤ë§ëœ ëª¨ë“  URL)
        saved_urls = get_urls_from_firebase(page_id)
        
        if not saved_urls:
            print(f"crawling_routes.py: ğŸ”„ í¬ë¡¤ë§í•  URLì´ ì—†ìŠµë‹ˆë‹¤.")
            return jsonify({"success": False, "error": "í¬ë¡¤ë§í•  URLì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € URL í¬ë¡¤ë§ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”."}), 400
        
        print(f"crawling_routes.py: ğŸ”„ í¬ë¡¤ë§í•  URL ê°œìˆ˜: {len(saved_urls)}")
        
        # 2. URL ë¦¬ìŠ¤íŠ¸ë¥¼ crawling_and_structuring í•¨ìˆ˜ì— ì „ë‹¬
        result = crawling_and_structuring(page_id, saved_urls)
        
        if result and result.get('success', False):
            return jsonify({
                "success": True,
                "message": "ì›¹ í¬ë¡¤ë§ ë° êµ¬ì¡°í™” ì™„ë£Œ",
                "results": result.get('results', {})
            }), 200
        else:
            return jsonify({
                "success": False,
                "error": f"ì›¹ í¬ë¡¤ë§ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
            }), 500
            
    except Exception as e:
        print(f"ì›¹ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return jsonify({
            "success": False, 
            "error": f"ì›¹ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        }), 500


@crawling_bp.route('/line1/<page_id>', methods=['POST'])
def cleanup_text_files(page_id):
    """í…ìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬ (line1.py)"""
    try:
        print(f"ğŸ§¹ í…ìŠ¤íŠ¸ ì •ë¦¬ ì‹œì‘: {page_id}")
        
        # URL ì…ë ¥ ê²½ë¡œ ê³„ì‚° - document_routes.pyì™€ ë™ì¼í•œ ë°©ì‹ ì‚¬ìš©
        # Flaskê°€ backendì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ ../data/input/ ì‚¬ìš©
        url_base_path = Path(f"../data/input/{page_id}_url")
        url_input_path = url_base_path / "input"
        
        # ê²½ë¡œ ì¡´ì¬ í™•ì¸
        if not url_input_path.exists():
            return jsonify({
                "success": False,
                "error": f"URL ì…ë ¥ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {url_input_path.resolve()}"
            }), 400
        
        print(f"ğŸ“ í…ìŠ¤íŠ¸ ì •ë¦¬ ëŒ€ìƒ ê²½ë¡œ: {url_input_path.resolve()}")
        
        # line1 ëª¨ë“ˆì˜ main í•¨ìˆ˜ ì‹¤í–‰ (ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜)
        abs_url_input_path = str(url_input_path.resolve())
        result = line1.main(abs_url_input_path, page_id)
        
        if result.get("success", False):
            return jsonify({
                "success": True,
                "message": "í…ìŠ¤íŠ¸ ì •ë¦¬ ì™„ë£Œ",
                "results": result
            }), 200
        else:
            return jsonify({
                "success": False,
                "error": f"í…ìŠ¤íŠ¸ ì •ë¦¬ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
            }), 500
            
    except Exception as e:
        print(f"í…ìŠ¤íŠ¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return jsonify({
            "success": False, 
            "error": f"í…ìŠ¤íŠ¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        }), 500


# @crawling_bp.route('/get-crawling-status/<page_id>', methods=['GET'])
# def get_crawling_status(page_id):
#     """í¬ë¡¤ë§ ìƒíƒœ í™•ì¸"""
#     try:
#         # ë°ì´í„° ë””ë ‰í† ë¦¬ì—ì„œ í¬ë¡¤ë§ ê²°ê³¼ í™•ì¸
#         url_base_path = f'../data/input/{page_id}_url'  
#         url_input_path = os.path.join(url_base_path, 'input')
        
        