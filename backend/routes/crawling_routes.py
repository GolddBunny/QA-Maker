from flask import Blueprint, jsonify, request
import os
import sys
import subprocess
from pathlib import Path
from datetime import datetime
from services.crawling_service.urlCrawling import main as crawl_urls
from urlLoad_routes import get_root_urls_from_firebase, save_crawling_url_to_firebase

crawling_bp = Blueprint('crawling', __name__)

# ê²½ë¡œ ì„¤ì •
current_dir = Path(__file__).parent
backend_dir = current_dir.parent
crawling_service_dir = backend_dir / "services" / "crawling_service"
sys.path.append(str(crawling_service_dir))

#url í¬ë¡¤ë§ ì‹œì‘
@crawling_bp.route('/start-crawling/<page_id>', methods=['POST'])
def start_url_crawling(page_id):
    """ì €ì¥ëœ URLë“¤ì„ ê°€ì ¸ì™€ì„œ í¬ë¡¤ë§ ì‹œì‘"""
    try:
        # 1. Firebaseì—ì„œ ì €ì¥ëœ URLë“¤ ê°€ì ¸ì˜¤ê¸°
        saved_urls = get_root_urls_from_firebase(page_id)
        
        if not saved_urls:
            return jsonify({"success": False, "error": "í¬ë¡¤ë§í•  URLì´ ì—†ìŠµë‹ˆë‹¤."}), 400
        
        # 2. forë¬¸ìœ¼ë¡œ saved_urls ìˆœíšŒí•˜ë©° í¬ë¡¤ë§
        for url in saved_urls:
            start_url = url['url']

            # 3. url í¬ë¡¤ë§ ì‹¤í–‰            
            crawling_results = crawl_urls(
                start_url=start_url,
            )
            #TODO: URL/ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ë°°ì—´ë¡œ ë°›ì•„ì„œ firebaseì— ì €ì¥í•´ì•¼ í•¨.
            json_data = crawling_results.get('json_data', {})

            #TODO: url/ë¬¸ì„œ í¬ë¡¤ë§ logì— ì°í ë•Œë§ˆë‹¤, uiì—ì„œë„ ë„ë¡œë¡ë„ë¡œë¡ ìˆ«ì ì¦ê°€í•˜ë„ë¡ í•´ì•¼í•¨. -> firebaseì— lengthë§Œ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¦ê°€í•˜ë„ë¡ ì²˜ë¦¬
            if crawling_results and "error" not in crawling_results:
                # 4. url í¬ë¡¤ë§ ê²°ê³¼ë¥¼ Firebaseì— ì €ì¥
                
                results_info = {
                    "page_id": page_id,
                    "start_url": start_url,
                    "base_domain": json_data.get('base_domain', ''),
                    "scope_patterns": json_data.get('scope_patterns', []),
                    "total_pages": json_data.get('total_pages_discovered', 0),
                    "total_documents": json_data.get('total_documents_discovered', 0),
                    "results_dir": json_data.get('results_dir', ''),
                    "execution_time": json_data.get('execution_time_seconds', 0),
                    "timestamp": datetime.now().isoformat()
                }


                for url in json_data.get('page_urls', []):
                    save_crawling_url_to_firebase(page_id, url)
            
            return jsonify({
                "success": True,
                "message": "URL í¬ë¡¤ë§ ì™„ë£Œ",
                "results": results_info
            }), 200
        else:
            return jsonify({
                "success": False, 
                "error": f"í¬ë¡¤ë§ ì‹¤íŒ¨: {crawling_results.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
            }), 500
            
    except Exception as e:
        print(f"URL í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return jsonify({"success": False, "error": f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}), 500

@crawling_bp.route('/start-structuring/<page_id>', methods=['POST'])
def start_web_structuring(page_id):
    """ì›¹ í¬ë¡¤ë§ ë° êµ¬ì¡°í™” ì‹œì‘ (crawling_and_structuring.py)"""
    try:
        print(f"ğŸ”„ ì›¹ í¬ë¡¤ë§ ë° êµ¬ì¡°í™” ì‹œì‘: {page_id}")
        
        # crawling_and_structuring.py ì‹¤í–‰
        script_path = crawling_service_dir / "crawling_and_structuring.py"
        
        # ë°ì´í„° ë””ë ‰í† ë¦¬ì—ì„œ ìµœì‹  í¬ë¡¤ë§ ê²°ê³¼ ì°¾ê¸°
        data_dir = backend_dir.parent / "data" / "crawling"
        
        # ê°€ì¥ ìµœê·¼ í¬ë¡¤ë§ í´ë” ì°¾ê¸°
        crawling_folders = [d for d in data_dir.iterdir() if d.is_dir()]
        if not crawling_folders:
            return jsonify({
                "success": False, 
                "error": "í¬ë¡¤ë§ ê²°ê³¼ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }), 400
        
        # ê°€ì¥ ìµœê·¼ í´ë” ì„ íƒ (ì´ë¦„ ê¸°ì¤€ ì •ë ¬)
        latest_folder = sorted(crawling_folders, key=lambda x: x.name)[-1]
        url_file = latest_folder / f"page_urls_{latest_folder.name.split('_')[1]}.txt"
        
        if not url_file.exists():
            return jsonify({
                "success": False, 
                "error": f"URL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {url_file}"
            }), 400
        
        # Python ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        result = subprocess.run([
            sys.executable, str(script_path)
        ], capture_output=True, text=True, cwd=str(crawling_service_dir))
        
        if result.returncode == 0:
            return jsonify({
                "success": True,
                "message": "ì›¹ í¬ë¡¤ë§ ë° êµ¬ì¡°í™” ì™„ë£Œ",
                "results": {
                    "url_file": str(url_file),
                    "output_dir": str(latest_folder),
                    "stdout": result.stdout
                }
            }), 200
        else:
            return jsonify({
                "success": False,
                "error": f"ì›¹ í¬ë¡¤ë§ ì‹¤íŒ¨: {result.stderr}"
            }), 500
            
    except Exception as e:
        print(f"ì›¹ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return jsonify({
            "success": False, 
            "error": f"ì›¹ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        }), 500

@crawling_bp.route('/cleanup-text/<page_id>', methods=['POST'])
def cleanup_text_files(page_id):
    """í…ìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬ (line1.py)"""
    try:
        print(f"ğŸ§¹ í…ìŠ¤íŠ¸ ì •ë¦¬ ì‹œì‘: {page_id}")
        
        # line1.py ì‹¤í–‰
        script_path = crawling_service_dir / "line1.py"
        
        # Python ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        result = subprocess.run([
            sys.executable, str(script_path)
        ], capture_output=True, text=True, cwd=str(crawling_service_dir))
        
        if result.returncode == 0:
            return jsonify({
                "success": True,
                "message": "í…ìŠ¤íŠ¸ ì •ë¦¬ ì™„ë£Œ",
                "results": {
                    "stdout": result.stdout
                }
            }), 200
        else:
            return jsonify({
                "success": False,
                "error": f"í…ìŠ¤íŠ¸ ì •ë¦¬ ì‹¤íŒ¨: {result.stderr}"
            }), 500
            
    except Exception as e:
        print(f"í…ìŠ¤íŠ¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return jsonify({
            "success": False, 
            "error": f"í…ìŠ¤íŠ¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        }), 500

@crawling_bp.route('/get-crawling-status/<page_id>', methods=['GET'])
def get_crawling_status(page_id):
    """í¬ë¡¤ë§ ìƒíƒœ í™•ì¸"""
    try:
        # ë°ì´í„° ë””ë ‰í† ë¦¬ì—ì„œ í¬ë¡¤ë§ ê²°ê³¼ í™•ì¸
        data_dir = backend_dir.parent / "data" / "crawling"
        
        crawling_folders = [d for d in data_dir.iterdir() if d.is_dir()]
        
        if crawling_folders:
            latest_folder = sorted(crawling_folders, key=lambda x: x.name)[-1]
            
            # URL íŒŒì¼ê³¼ í¬ë¡¤ë§ ê²°ê³¼ íŒŒì¼ë“¤ í™•ì¸
            url_files = list(latest_folder.glob("page_urls_*.txt"))
            jina_files = list(latest_folder.glob("**/jina_crawling/*.txt"))
            
            return jsonify({
                "success": True,
                "status": {
                    "latest_folder": str(latest_folder),
                    "url_files_count": len(url_files),
                    "jina_files_count": len(jina_files),
                    "has_results": len(jina_files) > 0
                }
            }), 200
        else:
            return jsonify({
                "success": True,
                "status": {
                    "latest_folder": None,
                    "url_files_count": 0,
                    "jina_files_count": 0,
                    "has_results": False
                }
            }), 200
            
    except Exception as e:
        return jsonify({
            "success": False, 
            "error": f"ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        }), 500 