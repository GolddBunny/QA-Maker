import time
from flask import Blueprint, jsonify, request
import os
import sys
import subprocess
from pathlib import Path
from datetime import datetime
from services.crawling_service.urlCrawling import main as crawl_urls
from routes.urlLoad_routes import get_general_urls_from_firebase, get_urls_from_firebase, save_url_to_firebase, save_urls_batch
from services.crawling_service.crawling_and_structuring import main as crawling_and_structuring
from firebase_admin import firestore
# from services.crawling_service import line1
from urllib.parse import urlparse
from services.execution_time_service import get_tracker

crawling_bp = Blueprint('crawling', __name__)

# Firestore í´ë¼ì´ì–¸íŠ¸
db = firestore.client()

#url í¬ë¡¤ë§ ì‹œì‘
@crawling_bp.route('/start-crawling/<page_id>', methods=['POST'])
def start_url_crawling(page_id):
    """ì €ì¥ëœ URLë“¤ì„ ê°€ì ¸ì™€ì„œ í¬ë¡¤ë§ ì‹œì‘"""
    
    # ì‹¤í–‰ ì‹œê°„ íŠ¸ë˜ì»¤ ì‹œì‘
    tracker = get_tracker(page_id)
    
    try:
        start_time = time.time()
        
        # 1. Firebaseì—ì„œ ì €ì¥ëœ URLë“¤ ê°€ì ¸ì˜¤ê¸°
        saved_urls = get_general_urls_from_firebase(page_id)
        
        print(f"ğŸ“‹ Firebaseì—ì„œ ê°€ì ¸ì˜¨ ì¼ë°˜ URL ê°œìˆ˜: {len(saved_urls) if saved_urls else 0}")
        if saved_urls:
            for i, url_info in enumerate(saved_urls):
                print(f"   {i+1}. {url_info.get('url', 'N/A')} (ë‚ ì§œ: {url_info.get('date', 'N/A')})")
        
        if not saved_urls:
            return jsonify({"success": False, "error": "í¬ë¡¤ë§í•  URLì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € URLì„ ì¶”ê°€í•´ì£¼ì„¸ìš”."}), 400

        # 2. forë¬¸ìœ¼ë¡œ saved_urls ìˆœíšŒí•˜ë©° í¬ë¡¤ë§
        for url in saved_urls:
            start_url = url['url']
            print(f"ğŸ” URL í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘: {start_url}")

            # URLì—ì„œ ìë™ìœ¼ë¡œ scope íŒ¨í„´ ì¶”ì¶œ
            parsed_url = urlparse(start_url)
            path_parts = [part for part in parsed_url.path.split('/') if part and part != 'index.do']
            
            # ë²”ìš©ì ì¸ scope íŒ¨í„´ ì¶”ì¶œ ë£°
            scope_patterns = []
            
            # ì œì™¸í•  íŒŒì¼ í™•ì¥ì ëª©ë¡
            exclude_extensions = ['.do', '.jsp', '.php', '.html', '.htm', '.asp', '.aspx', '.action']
            
            # ì œì™¸í•  ì¼ë°˜ì ì¸ ì›¹ì‚¬ì´íŠ¸ ê²½ë¡œ
            exclude_paths = ['web', 'www', 'sites', 'admin', 'common', 'include', 'images', 'css', 'js', 'static']
            
            for part in path_parts:
                # 1. íŒŒì¼ í™•ì¥ìê°€ ìˆëŠ” ê²½ìš° ì œì™¸
                has_extension = any(part.lower().endswith(ext) for ext in exclude_extensions)
                if has_extension:
                    continue
                    
                # 2. ì¼ë°˜ì ì¸ ì›¹ì‚¬ì´íŠ¸ ê²½ë¡œ ì œì™¸
                if part.lower() in exclude_paths:
                    continue
                    
                # 3. ë„ˆë¬´ ì§§ì€ ê²½ë¡œ ì œì™¸ (1-2ê¸€ì)
                if len(part) <= 2:
                    continue
                    
                # 4. ìˆœìˆ˜ ìˆ«ìë¡œë§Œ êµ¬ì„±ëœ ê²½ìš° - IDë¡œ ê°„ì£¼í•˜ì—¬ ì„ íƒì  í¬í•¨
                if part.isdigit():
                    # 4ìë¦¬ ì´ìƒì˜ ìˆ«ìëŠ” ì˜ë¯¸ìˆëŠ” IDë¡œ ê°„ì£¼ (ì˜ˆ: 10727)
                    if len(part) >= 4:
                        scope_patterns.append(part)
                    continue
                
                # 5. ì˜ë¯¸ìˆëŠ” ë””ë ‰í† ë¦¬ëª… ì¶”ê°€
                scope_patterns.append(part)
            
            # ë¡œê¹…
            if scope_patterns:
                print(f"ğŸ¯ ìë™ ì¶”ì¶œëœ ë²”ìœ„ íŒ¨í„´: {scope_patterns}")
            else:
                print("ğŸŒ ë²”ìœ„ íŒ¨í„´ ì—†ìŒ - ë„ë©”ì¸ ì „ì²´ í¬ë¡¤ë§")

            # 3. url í¬ë¡¤ë§ ì‹¤í–‰            
            crawling_results = crawl_urls(
                start_url=start_url,
                scope=scope_patterns if scope_patterns else None
            )
            
            # crawling_resultsëŠ” ì§ì ‘ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ì…ë‹ˆë‹¤
            if crawling_results and "error" not in crawling_results:
                print(f"âœ… URL í¬ë¡¤ë§ ì„±ê³µ: {len(crawling_results.get('page_urls', []))}ê°œ í˜ì´ì§€ ë°œê²¬")
                
                # 4. url í¬ë¡¤ë§ ê²°ê³¼ë¥¼ Firebaseì— ì €ì¥
                results_info = {
                    "page_id": page_id,
                    "start_url": start_url,
                    "base_domain": crawling_results.get('base_domain', ''),
                    "scope_patterns": crawling_results.get('scope_patterns', []),
                    "total_pages": crawling_results.get('total_pages_discovered', 0),
                    "total_documents": crawling_results.get('total_documents_discovered', 0),
                    "results_dir": crawling_results.get('results_dir', ''),
                    "execution_time": crawling_results.get('execution_time', 0),
                    "timestamp": datetime.now().isoformat()
                }

                # page_urlsì—ì„œ URL ëª©ë¡ì„ ê°€ì ¸ì™€ì„œ Firebaseì— ì €ì¥ (ë°°ì¹˜ ì²˜ë¦¬)
                page_urls = crawling_results.get('page_urls', [])
                # set ê°ì²´ì¸ ê²½ìš° listë¡œ ë³€í™˜ (ì˜¤ë¥˜ ë°©ì§€)
                if isinstance(page_urls, set):
                    page_urls = list(page_urls)
                saved_count = save_urls_batch(page_id, page_urls, "crawled")
                
                # doc_urlsì—ì„œ ë¬¸ì„œ URL ëª©ë¡ì„ ê°€ì ¸ì™€ì„œ Firebaseì— ì €ì¥ (ë°°ì¹˜ ì²˜ë¦¬)
                doc_urls = crawling_results.get('doc_urls', [])
                # set ê°ì²´ì¸ ê²½ìš° listë¡œ ë³€í™˜ (ì˜¤ë¥˜ ë°©ì§€)
                if isinstance(doc_urls, set):
                    doc_urls = list(doc_urls)
                doc_saved_count = save_urls_batch(page_id, doc_urls, "document")
                
                print(f"ğŸ’¾ Firebaseì— {saved_count}ê°œ í˜ì´ì§€ URL, {doc_saved_count}ê°œ ë¬¸ì„œ URL ì €ì¥ ì™„ë£Œ")
                print(f"ğŸ“ ë°œê²¬ëœ ë¬¸ì„œ URL: {crawling_results.get('total_documents_discovered', 0)}ê°œ")
            
                print(f"ğŸ’¾ Firebaseì— {saved_count}ê°œ URL ì €ì¥ ì™„ë£Œ")
                end_time = time.time()
                execution_time = round(end_time - start_time)
                
                # ì‹¤í–‰ ì‹œê°„ íŠ¸ë˜ì»¤ì— ê¸°ë¡
                additional_data = {
                    "start_url": start_url,
                    "total_pages_discovered": crawling_results.get('total_pages_discovered', 0),
                    "total_documents_discovered": crawling_results.get('total_documents_discovered', 0),
                    "scope_patterns": scope_patterns,
                    "saved_pages": saved_count,
                    "saved_documents": doc_saved_count
                }
                tracker.record_step('url_crawling', execution_time, additional_data)
                
                return jsonify({
                    "success": True,
                    "message": "URL í¬ë¡¤ë§ ì™„ë£Œ",
                    "results": results_info,
                    'execution_time': execution_time
                }), 200
            else:
                print(f"âŒ URL í¬ë¡¤ë§ ì‹¤íŒ¨: {crawling_results.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                return jsonify({
                    "success": False, 
                    "error": f"í¬ë¡¤ë§ ì‹¤íŒ¨: {crawling_results.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
                }), 500
            
    except Exception as e:
        print(f"URL í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return jsonify({"success": False, "error": f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}), 500

@crawling_bp.route('/crawl-and-structure/<page_id>', methods=['POST'])
def crawl_and_structure(page_id):
    """ì›¹ í¬ë¡¤ë§ ë° êµ¬ì¡°í™” ì‹œì‘ (crawling_and_structuring.py)"""
    
    # ì‹¤í–‰ ì‹œê°„ íŠ¸ë˜ì»¤ ê°€ì ¸ì˜¤ê¸°
    tracker = get_tracker(page_id)
    
    try:
        print(f"crawling_routes.py: ğŸ”„ ì›¹ í¬ë¡¤ë§ ë° êµ¬ì¡°í™” ì‹œì‘: {page_id}")
        start_time = time.time()
        # 1. Firebaseì—ì„œ ì €ì¥ëœ URLë“¤ ê°€ì ¸ì˜¤ê¸° (í¬ë¡¤ë§ëœ ëª¨ë“  URL)
        saved_urls = get_urls_from_firebase(page_id)
        
        if not saved_urls:
            print(f"crawling_routes.py: ğŸ”„ í¬ë¡¤ë§í•  URLì´ ì—†ìŠµë‹ˆë‹¤.")
            return jsonify({"success": False, "error": "í¬ë¡¤ë§í•  URLì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € URL í¬ë¡¤ë§ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”."}), 400
        
        print(f"crawling_routes.py: ğŸ”„ í¬ë¡¤ë§í•  URL ê°œìˆ˜: {len(saved_urls)}")
        
        # 2. URL ë¦¬ìŠ¤íŠ¸ë¥¼ crawling_and_structuring í•¨ìˆ˜ì— ì „ë‹¬
        result = crawling_and_structuring(page_id, saved_urls)
        end_time = time.time()
        execution_time = round(end_time - start_time)
        
        # ì‹¤í–‰ ì‹œê°„ íŠ¸ë˜ì»¤ì— ê¸°ë¡
        additional_data = {
            "processed_urls": len(saved_urls),
            "result_success": result.get('success', False) if result else False
        }
        if result and result.get('results'):
            additional_data.update(result.get('results', {}))
        
        tracker.record_step('web_structuring', execution_time, additional_data)
        
        if result and result.get('success', False):
            return jsonify({
                "success": True,
                "message": "ì›¹ í¬ë¡¤ë§ ë° êµ¬ì¡°í™” ì™„ë£Œ",
                "results": result.get('results', {}),
                'execution_time': execution_time
            }), 200
        else:
            return jsonify({
                "success": False,
                "error": f"ì›¹ í¬ë¡¤ë§ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
            }), 500
            
    except Exception as e:
        print(f"ì›¹ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return jsonify({
            "success": False, 
            "error": f"ì›¹ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        }), 500

