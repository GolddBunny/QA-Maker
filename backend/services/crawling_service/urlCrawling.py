# ê²½ê³  í•„í„° ì„¤ì • (soupsieve deprecated ê²½ê³  ë¬´ì‹œ)
import warnings
warnings.filterwarnings("ignore", message="The pseudo class ':contains' is deprecated", category=FutureWarning)

# ì›¹ í¬ë¡¤ë§ ê´€ë ¨
from bs4 import BeautifulSoup   # html íŒŒì‹± + ë°ì´í„° ì¶”ì¶œ : find_all(ì¡°ê±´ì— ë§ëŠ” ëª¨ë“  íƒœê·¸ ì°¾ê¸°), select(css ì„ íƒì ì‚¬ìš©)
import requests                                         # ì •ì  ì›¹í˜ì´ì§€ í¬ë¡¤ë§: ì›¹ ìš”ì²­ ì²˜ë¦¬(get, post) + ì›¹ í˜ì´ì§€ ë‹¤ìš´ë¡œë“œ(text, json)
from requests.exceptions import Timeout, HTTPError, RequestException
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.common.exceptions import TimeoutException
from webdriver_manager.chrome import ChromeDriverManager

# ì‹œìŠ¤í…œ ë° ìœ í‹¸ë¦¬í‹°
import os, time, random, re, json, logging, base64                 
from urllib.parse import urlparse, urljoin, parse_qs
from datetime import datetime
from typing import Set, List, Dict, Tuple, Optional, Any
import threading                                        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±ì„ ìœ„í•œ ë½

from pathlib import Path
try:
    from .simple_visualizer import SimpleTreeVisualizer
    from .crawler_constants import (
        BASE_DIR, USER_AGENTS, DOC_EXTENSIONS, EXCLUDE_PATTERNS, 
        LIST_PAGE_PATTERNS, EXCLUDE_EXTENSIONS, PAGINATION_SELECTORS,
        PAGE_NUMBER_PATTERNS, ATTACHMENT_CLASSES, DOWNLOAD_PATTERNS,
        EXPLICIT_DOWNLOAD_PATTERNS, ERROR_PATTERNS
    )
except ImportError:
    # ì§ì ‘ ì‹¤í–‰í•  ë•Œë¥¼ ìœ„í•œ ì ˆëŒ€ import
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from simple_visualizer import SimpleTreeVisualizer
    from crawler_constants import (
        BASE_DIR, USER_AGENTS, DOC_EXTENSIONS, EXCLUDE_PATTERNS, 
        LIST_PAGE_PATTERNS, EXCLUDE_EXTENSIONS, PAGINATION_SELECTORS,
        PAGE_NUMBER_PATTERNS, ATTACHMENT_CLASSES, DOWNLOAD_PATTERNS,
        EXPLICIT_DOWNLOAD_PATTERNS, ERROR_PATTERNS
    )

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("crawler.log")
    ]
)
logger = logging.getLogger("scope_crawler")

"""URL ì •ê·œí™” ê²°ê³¼ë¥¼ ìºì‹±í•˜ëŠ” ìŠ¤ë ˆë“œ ì•ˆì „í•œ ìºì‹œ êµ¬í˜„"""
class URLNormalizationCache:
    def __init__(self):
        self.cache = {}  # ë‹¨ìˆœ ë”•ì…”ë„ˆë¦¬ - URL ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ë¬´ì œí•œ ì €ì¥
        self.lock = threading.RLock()   # ìŠ¤ë ˆë“œ ì•ˆì „ì„±ì„ ìœ„í•œ ì¬ì§„ì… ê°€ëŠ¥í•œ ë½
    
    #ìºì‹œ ì¡°íšŒ
    def get(self, key: str) -> Optional[str]:
        with self.lock:
            return self.cache.get(key)
    
    #ìºì‹œ ì¶”ê°€
    def put(self, key: str, value: str) -> None:
        with self.lock:
            self.cache[key] = value
    
    #ìºì‹œ ì´ˆê¸°í™”
    def clear(self) -> None:
        with self.lock:
            self.cache.clear()
    
    #ìºì‹œ í¬ê¸° ë°˜í™˜
    def size(self) -> int:
        with self.lock:
            return len(self.cache)

"""URLë“¤ì„ íŠ¸ë¦¬ ë…¸ë“œë¡œ í‘œí˜„í•˜ëŠ” í´ë˜ìŠ¤"""
class URLTreeNode:
    #URL, ë¶€ëª¨, ê¹Šì´ ì´ˆê¸°í™”
    def __init__(self, url: str, parent: Optional['URLTreeNode'] = None, depth: int = 0):
        self.url = url
        self.parent = parent
        self.children: List['URLTreeNode'] = []
        self.depth = depth
        self.is_document = False
        self.page_title = ""
        self.visited_at = None          #ë°©ë¬¸ ì‹œê°„
        self.doc_links: List[str] = []
        
        # ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„ì„ ìœ„í•œ ì¶”ê°€ ì†ì„±ë“¤
        self.page_type = ""  # "main", "category", "board", "article", "document" ë“±
        self.breadcrumb = "unknown"  # í˜„ì¬ í˜ì´ì§€ì˜ ê³„ì¸µì  êµ¬ì¡° (ë¸Œë ˆë“œí¬ëŸ¼)
        self.link_count = 0  # í•´ë‹¹ í˜ì´ì§€ì—ì„œ ë°œê²¬ëœ ë§í¬ ìˆ˜

        self.load_time = 0.0  # í˜ì´ì§€ ë¡œë”© ì‹œê°„
        self.file_size = 0  # í˜ì´ì§€ í¬ê¸° (bytes)
        
        # ë…¼ë¦¬ì  êµ¬ì¡° vs ë¬¼ë¦¬ì  ì—°ê²° êµ¬ë¶„
        self.logical_parent = None  # ë„¤ë¹„ê²Œì´ì…˜ ê¸°ë°˜ ë…¼ë¦¬ì  ë¶€ëª¨
        self.logical_children = []  # ë„¤ë¹„ê²Œì´ì…˜ ê¸°ë°˜ ë…¼ë¦¬ì  ìì‹ë“¤
        self.navigation_links = []  # ë„¤ë¹„ê²Œì´ì…˜/ë©”ë‰´ì—ì„œ ë°œê²¬ëœ ë§í¬ë“¤
        self.content_links = []    # ì½˜í…ì¸ ì—ì„œ ë°œê²¬ëœ ë§í¬ë“¤
        self.navigation_level = 0  # ë„¤ë¹„ê²Œì´ì…˜ ê¸°ë°˜ ê³„ì¸µ ë ˆë²¨
        self.is_navigation_node = False  # ë„¤ë¹„ê²Œì´ì…˜ êµ¬ì¡°ì˜ í•µì‹¬ ë…¸ë“œì¸ì§€
        self.menu_position = ""    # ë©”ë‰´ì—ì„œì˜ ìœ„ì¹˜ (header, sidebar, footer ë“±)
    
    #ìì‹ ë…¸ë“œ ì¶”ê°€
    def add_child(self, child_url: str) -> 'URLTreeNode':
        
        child_node = URLTreeNode(child_url, self, self.depth + 1)
        
        # URL íŒ¨í„´ìœ¼ë¡œ ë¬¸ì„œ íŒŒì¼ ì—¬ë¶€ ë¯¸ë¦¬ í™•ì¸
        if self._is_likely_document_url(child_url):
            child_node.is_document = True
            child_node.page_type = "document"
            # ë¬¸ì„œ íŒŒì¼ì˜ ê²½ìš° ê¸°ë³¸ ë¸Œë ˆë“œí¬ëŸ¼ ì„¤ì •
            if self.breadcrumb and self.breadcrumb not in ["unknown", "í™ˆ"]:
                child_node.breadcrumb = f"{self.breadcrumb}/ì²¨ë¶€íŒŒì¼"
            else:
                child_node.breadcrumb = "ì²¨ë¶€íŒŒì¼"
        
        self.children.append(child_node)
        return child_node
    
    #URL íŒ¨í„´ìœ¼ë¡œ ë¬¸ì„œ íŒŒì¼ ê°€ëŠ¥ì„± í™•ì¸
    def _is_likely_document_url(self, url: str) -> bool:
        """URL íŒ¨í„´ìœ¼ë¡œ ë¬¸ì„œ íŒŒì¼ ê°€ëŠ¥ì„± í™•ì¸"""
        try:
            url_lower = url.lower()
            
            # ëª…í™•í•œ ë‹¤ìš´ë¡œë“œ íŒ¨í„´
            if any(pattern in url_lower for pattern in ['download.do', 'filedown.do', 'getfile.do']):
                return True
            
            # íŒŒì¼ í™•ì¥ì í™•ì¸
            from urllib.parse import urlparse
            parsed = urlparse(url)
            path = parsed.path.lower()
            
            # ë¬¸ì„œ íŒŒì¼ í™•ì¥ì
            doc_extensions = ['.pdf', '.docx', '.doc', '.hwp', '.txt', '.hwpx', '.xls', '.xlsx', '.ppt', '.pptx']
            if any(path.endswith(ext) for ext in doc_extensions):
                return True
            
            # ì²¨ë¶€íŒŒì¼ ê´€ë ¨ ê²½ë¡œ
            if any(keyword in path for keyword in ['/attach', '/file', '/download']):
                return True
            
            return False
            
        except Exception:
            return False

    #ë£¨íŠ¸ë¶€í„° í˜„ì¬ ë…¸ë“œê¹Œì§€ì˜ ê²½ë¡œ ë°˜í™˜
    def get_path_from_root(self) -> List[str]:
        path = []
        current = self
        while current:
            path.insert(0, current.url)
            current = current.parent
        return path
    
    #URLì—ì„œ ì˜ë¯¸ìˆëŠ” ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ë©”ë‰´ ê²½ë¡œ êµ¬ì„±ì— í™œìš©
    def _extract_meaningful_url_segment(self) -> str:
        """URLì—ì„œ ì˜ë¯¸ìˆëŠ” ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ë©”ë‰´ ê²½ë¡œ êµ¬ì„±ì— í™œìš©"""
        try:
            from urllib.parse import urlparse, parse_qs
            parsed = urlparse(self.url)
            
            # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ì—ì„œ ì˜ë¯¸ìˆëŠ” ì •ë³´ ì¶”ì¶œ
            query_params = parse_qs(parsed.query)
            
            # artclView.do ê°™ì€ ê²Œì‹œê¸€ ë³´ê¸° í˜ì´ì§€ì¸ ê²½ìš°
            if 'artclView.do' in self.url:
                if 'artclNo' in query_params:
                    return f"ê²Œì‹œê¸€_{query_params['artclNo'][0]}"
                return "ê²Œì‹œê¸€"
            
            # ë‹¤ìš´ë¡œë“œ íŒŒì¼ì¸ ê²½ìš°
            if self.is_document or any(pattern in self.url.lower() for pattern in ['download', 'file']):
                return "ì²¨ë¶€íŒŒì¼"
            
            # URL ê²½ë¡œì—ì„œ ì˜ë¯¸ìˆëŠ” ë¶€ë¶„ ì¶”ì¶œ
            path_parts = [p for p in parsed.path.split('/') if p and p not in ['index.do', 'main.do']]
            if path_parts:
                last_part = path_parts[-1]
                # íŒŒì¼ í™•ì¥ì ì œê±°
                clean_part = re.sub(r'\.(do|html|htm|jsp|php)$', '', last_part)
                
                # ì˜ë¯¸ìˆëŠ” ì´ë¦„ìœ¼ë¡œ ë³€í™˜
                segment_mappings = {
                    'notice': 'ê³µì§€ì‚¬í•­',
                    'news': 'ì†Œì‹',
                    'intro': 'ì†Œê°œ',
                    'about': 'ì†Œê°œ'
                }
                
                return segment_mappings.get(clean_part.lower(), clean_part)
            
            return ""
            
        except Exception as e:
            logger.debug(f"URL ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ""
    
    #í˜ì´ì§€ íƒ€ì… ë¶„ë¥˜
    def classify_page_type(self, soup: BeautifulSoup, start_url: str = None) -> str:
        """í˜ì´ì§€ íƒ€ì…ì„ 4ê°œ ì¹´í…Œê³ ë¦¬ë¡œ ê°„ì†Œí™”í•˜ì—¬ ë¶„ë¥˜: Main, document, board, general"""
        try:
            url_lower = self.url.lower()
            
            # ğŸ  Main í˜ì´ì§€: start_urlê³¼ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ë§Œ (One and Only)
            if start_url and self.url == start_url:
                return "main"
            
            # ğŸ“„ Document: ë‹¤ìš´ë¡œë“œ/íŒŒì¼ ê´€ë ¨ íŒ¨í„´ (ìµœìš°ì„ )
            if any(pattern in url_lower for pattern in EXPLICIT_DOWNLOAD_PATTERNS):
                return "document"
            
            # ğŸ“„ Document: ë¬¸ì„œ íŒŒì¼ ì†ì„± ê¸°ë°˜
            if self.is_document:
                return "document"
            
            # ğŸ“‹ Board: ê²Œì‹œíŒ ê´€ë ¨ íŒ¨í„´
            if any(pattern in url_lower for pattern in ['board', 'bbs', 'list', 'artcllist']):
                # ë‹¤ìš´ë¡œë“œ íŒ¨í„´ì´ í¬í•¨ëœ ê²Œì‹œíŒ URLì€ ë¬¸ì„œë¡œ ë¶„ë¥˜
                if any(dl_pattern in url_lower for dl_pattern in EXPLICIT_DOWNLOAD_PATTERNS):
                    return "document"
                else:
                    return "board"
            
            # ğŸ“‹ Board: ì½˜í…ì¸  ê¸°ë°˜ ê²Œì‹œíŒ ê°ì§€ (í…Œì´ë¸” êµ¬ì¡°)
            if soup:
                page_text = soup.get_text().lower()
                tables = soup.find_all('table')
                if len(tables) >= 2:
                    # ê²Œì‹œíŒ íŠ¹ì„± í‚¤ì›Œë“œ í™•ì¸
                    board_keywords = ['ë²ˆí˜¸', 'ì œëª©', 'ì‘ì„±ì', 'ë‚ ì§œ', 'ì¡°íšŒ', 'subject', 'date', 'view']
                    if any(keyword in page_text for keyword in board_keywords):
                        return "board"
            
            # ğŸŒ General: ë‚˜ë¨¸ì§€ ëª¨ë“  í˜ì´ì§€
            return "general"
            
        except Exception as e:
            logger.debug(f"í˜ì´ì§€ íƒ€ì… ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜: {e}")
            return "general"
    
    """ì „ì²´ì ì¸ ë¸Œë ˆë“œí¬ëŸ¼ ì¶”ì¶œ í”„ë¡œì„¸ìŠ¤ë¥¼ ê´€ë¦¬í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ë¡œ ì™¸ë¶€ì—ì„œ í˜¸ì¶œ"""
    #í˜„ì¬ í˜ì´ì§€ì˜ ê³„ì¸µì  êµ¬ì¡° ë°˜í™˜. 1. ë„¤ë¹„ê²Œì´ì…˜ ë©”ë‰´ì—ì„œ ì°¾ê¸° 2. URL íŒ¨í„´ ê¸°ë°˜ ì¶”ì •
    def extract_breadcrumb(self, soup: BeautifulSoup, current_url: str) -> str:
        try:
            if not soup:
                return "unknown"
            
            # ë©”ì¸ ë„¤ë¹„ê²Œì´ì…˜ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
            nav_containers = soup.select([
                'nav', '.nav', '.main-menu', '.gnb', '.lnb', '.menu',
                '[class*="menu"]', '[id*="menu"]', '[class*="nav"]', '[id*="nav"]'
            ])
            
            for nav in nav_containers:
                hierarchy_path = self._extract_hierarchy_from_nav(nav, current_url)
                if hierarchy_path != "unknown":
                    logger.debug(f"ğŸ¯ ë©”ë‰´ ê³„ì¸µ ì¶”ì¶œ ì„±ê³µ: {hierarchy_path} for {current_url}")
                    return hierarchy_path
            
            # ë„¤ë¹„ê²Œì´ì…˜ ë©”ë‰´ì—ì„œ ì°¾ì§€ ëª»í•œ ê²½ìš° URL íŒ¨í„´ ê¸°ë°˜ ì¶”ì •
            url_based_path = self._infer_path_from_url(current_url)
            if url_based_path != "unknown":
                logger.debug(f"ğŸ” URL íŒ¨í„´ ê¸°ë°˜ ë¸Œë ˆë“œí¬ëŸ¼ ì¶”ì •: {url_based_path} for {current_url}")
                return url_based_path
            
            return "unknown"
            
        except Exception as e:
            logger.debug(f"ë©”ë‰´ ê³„ì¸µ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return "unknown"
    
    #1.ë„¤ë¹„ê²Œì´ì…˜ ìš”ì†Œì—ì„œ ul/li ê³„ì¸µ êµ¬ì¡° ë¶„ì„.
    def _extract_hierarchy_from_nav(self, nav_element, current_url: str) -> str:
        """ë„¤ë¹„ê²Œì´ì…˜ ìš”ì†Œì—ì„œ ul/li ê³„ì¸µ êµ¬ì¡° ë¶„ì„ (ë¬´ì œí•œ ê¹Šì´ ì¬ê·€)"""
        try:
            # ìµœìƒìœ„ ë©”ë‰´ í•­ëª©ë“¤ ì°¾ê¸°
            top_items = nav_element.find_all('li', recursive=False)
            if not top_items:
                # liê°€ ì§ì ‘ì ìœ¼ë¡œ ì—†ìœ¼ë©´ ul í•˜ìœ„ì—ì„œ ì°¾ê¸°
                top_ul = nav_element.find('ul')
                if top_ul:
                    top_items = top_ul.find_all('li', recursive=False)
            
            for top_item in top_items:
                top_link = top_item.find('a')
                if not top_link:
                    continue
                
                top_text = top_link.get_text().strip()
                top_href = top_link.get('href', '')
                
                # í˜„ì¬ í˜ì´ì§€ê°€ ìµœìƒìœ„ ë©”ë‰´ í•­ëª©ì¸ì§€ í™•ì¸
                if self._is_same_page(top_href, current_url):
                    return top_text
                
                # í•˜ìœ„ ë©”ë‰´ë“¤ì—ì„œ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰
                sub_menus = top_item.find_all('ul')
                for sub_menu in sub_menus:
                    result = self._check_breadcrumb_hierarchy_recursive(sub_menu, current_url, top_text)
                    if result != "unknown":
                        return result
            
            return "unknown"
            
        except Exception as e:
            logger.debug(f"ë„¤ë¹„ê²Œì´ì…˜ ê³„ì¸µ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return "unknown"
    
    #1.1 ì¬ê·€ì ìœ¼ë¡œ ëª¨ë“  ê¹Šì´ì˜ ë©”ë‰´ ê³„ì¸µ íƒìƒ‰
    def _check_breadcrumb_hierarchy_recursive(self, menu_ul, current_url: str, parent_path: str = "", max_depth: int = 10) -> str:
        """ì¬ê·€ì ìœ¼ë¡œ ëª¨ë“  ê¹Šì´ì˜ ë©”ë‰´ ê³„ì¸µ íƒìƒ‰"""
        if max_depth <= 0:  # ë¬´í•œ ì¬ê·€ ë°©ì§€
            logger.debug(f"ìµœëŒ€ ë©”ë‰´ ê¹Šì´ ë„ë‹¬: {parent_path}")
            return "unknown"
        
        try:
            items = menu_ul.find_all('li', recursive=False)
            
            for item in items:
                link = item.find('a')
                if not link:
                    continue
                
                text = link.get_text().strip()
                href = link.get('href', '')
                current_path = f"{parent_path}/{text}" if parent_path else text
                
                # í˜„ì¬ í˜ì´ì§€ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                if self._is_same_page(href, current_url):
                    logger.debug(f"ğŸ¯ ë©”ë‰´ ê²½ë¡œ ë°œê²¬: {current_path}")
                    return current_path
                
                # í•˜ìœ„ ë©”ë‰´ë“¤ì—ì„œ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰
                sub_menus = item.find_all('ul')
                for sub_menu in sub_menus:
                    result = self._check_breadcrumb_hierarchy_recursive(
                        sub_menu, current_url, current_path, max_depth - 1
                    )
                    if result != "unknown":
                        return result
            
            return "unknown"
            
        except Exception as e:
            logger.debug(f"ì¬ê·€ ë©”ë‰´ íƒìƒ‰ ì¤‘ ì˜¤ë¥˜ (depth: {10-max_depth}): {e}")
            return "unknown"
    


    
    def _is_same_page(self, href: str, current_url: str) -> bool:
        """ë‘ URLì´ ê°™ì€ í˜ì´ì§€ë¥¼ ê°€ë¦¬í‚¤ëŠ”ì§€ í™•ì¸"""
        try:
            if not href or href in ['#', 'javascript:', 'mailto:', 'tel:']:
                return False
            
            # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            if href.startswith('/'):
                full_href = f"https://{urlparse(current_url).netloc}{href}"
            elif href.startswith('http'):
                full_href = href
            else:
                # í˜„ì¬ URL ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ í•´ê²°
                from urllib.parse import urljoin
                full_href = urljoin(current_url, href)
            
            # URL ì •ê·œí™”í•˜ì—¬ ë¹„êµ
            normalized_href = self._normalize_url_for_comparison(full_href)
            normalized_current = self._normalize_url_for_comparison(current_url)
            
            return normalized_href == normalized_current
            
        except Exception as e:
            logger.debug(f"URL ë¹„êµ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _normalize_url_for_comparison(self, url: str) -> str:
        """URL ë¹„êµë¥¼ ìœ„í•œ ì •ê·œí™”"""
        try:
            from urllib.parse import urlparse, urlunparse
            parsed = urlparse(url)
            
            # ì¿¼ë¦¬ì™€ í”„ë˜ê·¸ë¨¼íŠ¸ ì œê±°, ê²½ë¡œ ì •ê·œí™”
            normalized_path = parsed.path.rstrip('/')
            if not normalized_path:
                normalized_path = '/'
            
            return urlunparse((
                parsed.scheme,
                parsed.netloc.lower(),
                normalized_path,
                '',  # params
                '',  # query
                ''   # fragment
            ))
            
        except Exception:
            return url.lower()
    
    def _infer_path_from_url(self, url: str) -> str:
        """URL íŒ¨í„´ì—ì„œ ë…¼ë¦¬ì  ê²½ë¡œ ì¶”ì • (ê°œì„ ëœ ë²„ì „)"""
        try:
            from urllib.parse import urlparse, parse_qs
            parsed = urlparse(url)
            path_parts = [p for p in parsed.path.split('/') if p and p != 'index.do']
            
            # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ë¶„ì„
            query_params = parse_qs(parsed.query)
            
            # URL íŒ¨í„´ ë§¤í•‘
            path_mappings = {
                'notice': 'ê³µì§€ì‚¬í•­',
                'news': 'ì†Œì‹',
                'intro': 'ì†Œê°œ',
                'about': 'ì†Œê°œ'
            }
            
            # ê²½ë¡œ ë¶€ë¶„ì—ì„œ ë§¤í•‘ í™•ì¸
            for part in path_parts:
                clean_part = re.sub(r'\.(do|html|htm|jsp|php)$', '', part.lower())
                if clean_part in path_mappings:
                    return path_mappings[clean_part]
            
            # íŠ¹ì • íŒ¨í„´ ë¶„ì„
            url_lower = url.lower()

            # ë‹¤ìš´ë¡œë“œ ê´€ë ¨ íŒ¨í„´
            if any(pattern in url_lower for pattern in ['download', 'filedown', 'getfile']):
                return 'ì²¨ë¶€íŒŒì¼/ë‹¤ìš´ë¡œë“œ'
            
            return "unknown"
            
        except Exception as e:
            logger.debug(f"URL íŒ¨í„´ ì¶”ì • ì¤‘ ì˜¤ë¥˜: {e}")
            return "unknown"
    
    def to_dict(self) -> Dict[str, Any]:
        """ë…¸ë“œë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (JSON êµ¬ì¡°)"""
        # breadcrumb ì²˜ë¦¬: page_title ì‚¬ìš©, unknownì€ ê²½ë¡œë¯¸ìƒìœ¼ë¡œ í‘œì‹œ
        breadcrumb = self.breadcrumb
        if self.depth == 0:  # Root ë…¸ë“œ
            # Root ë…¸ë“œì˜ ê²½ìš° page_titleì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ "í™ˆ"
            if self.page_title and self.page_title != "ì œëª©ì—†ìŒ":
                clean_title = self.page_title.strip()
                if ' - ' in clean_title:
                    clean_title = clean_title.split(' - ')[0].strip()
                breadcrumb = clean_title
            else:
                breadcrumb = "í™ˆ"
        elif not breadcrumb or breadcrumb == "unknown":
            # ë¬¸ì„œ íŒŒì¼ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
            if self.is_document:
                if self.parent and self.parent.breadcrumb and self.parent.breadcrumb not in ["unknown", "í™ˆ"]:
                    breadcrumb = f"{self.parent.breadcrumb}/ì²¨ë¶€íŒŒì¼"
                else:
                    breadcrumb = "ì²¨ë¶€íŒŒì¼"
            else:
                # ì¼ë°˜ í˜ì´ì§€ì˜ ê²½ìš° ë¶€ëª¨ ë…¸ë“œì˜ ê²½ë¡œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì • ì‹œë„
                if self.parent and self.parent.breadcrumb and self.parent.breadcrumb not in ["unknown", "í™ˆ"]:
                    # URLì—ì„œ ì˜ë¯¸ìˆëŠ” ë¶€ë¶„ ì¶”ì¶œí•˜ì—¬ ë¶€ëª¨ ê²½ë¡œì— ì¶”ê°€
                    url_segment = self._extract_meaningful_url_segment()
                    if url_segment:
                        breadcrumb = f"{self.parent.breadcrumb}/{url_segment}"
                    else:
                        breadcrumb = f"{self.parent.breadcrumb}/í•˜ìœ„í˜ì´ì§€"
                else:
                    breadcrumb = "ê²½ë¡œë¯¸ìƒ"
        
        return {
            "url": self.url,
            "depth": self.depth,
            "page_title": self.page_title if self.page_title else "ì œëª©ì—†ìŒ",
            "breadcrumb": breadcrumb,
            "is_document": self.is_document,
            "children_count": len(self.children),
            "children": [child.to_dict() for child in self.children] if self.children else [],

        }

class ScopeLimitedCrawler:
    def __init__(self, max_pages: int = 100000, delay: float = 1.0, timeout: int = 20, use_requests: bool = True, max_depth: int = 10):
        """í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”.
        
        Args:
            max_pages: í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜
            delay: ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„(ì´ˆ)
            timeout: í˜ì´ì§€ ë¡œë”© ì‹œê°„ ì œí•œ(ì´ˆ)
            use_requests: ê°„ë‹¨í•œ í˜ì´ì§€ëŠ” requests ì‚¬ìš©, JSê°€ ë§ì€ í˜ì´ì§€ëŠ” selenium ì‚¬ìš©
            max_depth: ìµœëŒ€ ê¹Šì´ ì œí•œ
        """
        # í†µí•©ëœ ìŠ¤ë ˆë“œ ì•ˆì „ì„±ì„ ìœ„í•œ RLock ì‚¬ìš©
        self.url_lock = threading.RLock()
        
        # URL ê´€ë¦¬ë¥¼ ìœ„í•œ ìë£Œêµ¬ì¡° (ìŠ¤ë ˆë“œ ì•ˆì „)
        self.visited_urls: Set[str] = set()  # ë°©ë¬¸í•œ URL ì§‘í•©
        self.excluded_urls: Set[str] = set()  # ì œì™¸ëœ URL ì§‘í•©
        self.all_page_urls: Set[str] = set() # ëª¨ë“  í˜ì´ì§€ URL
        self.all_doc_urls: Set[str] = set()  # ëª¨ë“  ë¬¸ì„œ URL
        
        # URL ì •ê·œí™” ìºì‹œ
        self.normalization_cache = URLNormalizationCache()
        
        self.base_domain: str = ""  # ê¸°ë³¸ ë„ë©”ì¸
        self.scope_patterns: List[str] = []  # í¬ë¡¤ë§ ë²”ìœ„ íŒ¨í„´
        self.max_pages: int = max_pages  # ìµœëŒ€ í˜ì´ì§€ ìˆ˜
        self.delay: float = delay  # ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„
        self.timeout: int = timeout  # í˜ì´ì§€ ë¡œë”© ì‹œê°„ ì œí•œ
        self.use_requests: bool = use_requests  # requests ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ì—¬ë¶€
        self.session = requests.Session()  # ì„¸ì…˜ ìƒì„±
        self.session.headers.update({"User-Agent": random.choice(USER_AGENTS)})  # ëœë¤ User-Agent ì„¤ì •
        
        # Selenium ë“œë¼ì´ë²„ ì ‘ê·¼ì„ ìœ„í•œ ìŠ¤ë ˆë“œ ë½ ì¶”ê°€
        self.driver_lock = threading.Lock()

        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        os.makedirs(BASE_DIR, exist_ok=True)
        
        # Selenium ì´ˆê¸°í™” (í•„ìš”í•  ë•Œë§Œ)
        self.driver = None

        # DFSë¥¼ ìœ„í•œ ìƒˆë¡œìš´ ì†ì„±ë“¤
        self.max_depth = max_depth  # ìµœëŒ€ ê¹Šì´ ì œí•œ
        self.url_tree: Optional[URLTreeNode] = None  # URL íŠ¸ë¦¬ ë£¨íŠ¸
        self.url_to_node: Dict[str, URLTreeNode] = {}  # URL -> ë…¸ë“œ ë§¤í•‘
        self.visit_order: List[str] = []  # ë°©ë¬¸ ìˆœì„œ ê¸°ë¡
        
        # ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ DFSë¥¼ ìœ„í•œ ì†ì„±ë“¤
        self.global_navigation_map: Dict[str, Any] = {}  # ì „ì—­ ë„¤ë¹„ê²Œì´ì…˜ êµ¬ì¡°
        self.page_contexts: Dict[str, Dict[str, Any]] = {}  # í˜ì´ì§€ë³„ ì»¨í…ìŠ¤íŠ¸
        self.used_breadcrumbs: Set[str] = set()  # ì‚¬ìš©ëœ ë¸Œë ˆë“œí¬ëŸ¼ ê²½ë¡œë“¤
        
    def __enter__(self):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ ì‹œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.cleanup()

    def cleanup(self):
        """ëª¨ë“  ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.close_driver()
        if hasattr(self, 'session') and self.session:
            self.session.close()
            logger.info("HTTP ì„¸ì…˜ì´ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤")
        
        # ìºì‹œ ì •ë¦¬
        if hasattr(self, 'normalization_cache'):
            self.normalization_cache.clear()
            logger.info("ì •ê·œí™” ìºì‹œê°€ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤")

    def _init_selenium(self) -> None:
        """Selenium WebDriverë¥¼ ì´ˆê¸°í™” (ìŠ¤ë ˆë“œ ì•ˆì „)"""
        with self.driver_lock:
            if self.driver is not None:
                # ê¸°ì¡´ ë“œë¼ì´ë²„ê°€ ì‘ë‹µí•˜ëŠ”ì§€ í™•ì¸
                try:
                    self.driver.current_url  # ìƒíƒœ í™•ì¸
                    return  # ì •ìƒ ì‘ë™ ì¤‘ì´ë©´ ì¬ì‚¬ìš©
                except:
                    # ì‘ë‹µí•˜ì§€ ì•Šìœ¼ë©´ ì¬ìƒì„±
                    try:
                        self.driver.quit()
                    except:
                        pass
                    self.driver = None
                    
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument(f'user-agent={random.choice(USER_AGENTS)}')
            chrome_options.add_argument('--disable-extensions')
            chrome_options.add_argument('--ignore-certificate-errors')
            chrome_options.add_argument('--blink-settings=imagesEnabled=false')
            chrome_options.add_argument('--disk-cache-size=52428800')
            
            # JavaScript ì˜¤ë¥˜ ë¬´ì‹œ ì˜µì…˜ ì¶”ê°€
            chrome_options.add_argument('--disable-popup-blocking')
            chrome_options.add_argument('--disable-notifications')
            
            # ì„±ëŠ¥ ìµœì í™” ì„¤ì • - ì´ë¯¸ì§€, ì•Œë¦¼, ìŠ¤íƒ€ì¼ì‹œíŠ¸, ì¿ í‚¤, ìë°”ìŠ¤í¬ë¦½íŠ¸, í”ŒëŸ¬ê·¸ì¸, íŒì—…, ì§€ë¦¬ì •ë³´, ë¯¸ë””ì–´ìŠ¤íŠ¸ë¦¼. 1ì€ í—ˆìš©, 2ëŠ” ë¹„í—ˆìš©
            prefs = {
                "profile.managed_default_content_settings.images": 2,
                "profile.default_content_setting_values.notifications": 2,
                "profile.managed_default_content_settings.stylesheets": 2,
                "profile.managed_default_content_settings.cookies": 1,
                "profile.managed_default_content_settings.javascript": 1,
                "profile.managed_default_content_settings.plugins": 1,
                "profile.managed_default_content_settings.popups": 2,
                "profile.managed_default_content_settings.geolocation": 2,
                "profile.managed_default_content_settings.media_stream": 2,
            }
            chrome_options.add_experimental_option("prefs", prefs)
            
            try:
                service = Service(ChromeDriverManager().install())
                self.driver = webdriver.Chrome(service=service, options=chrome_options)
                self.driver.set_page_load_timeout(self.timeout)
            except Exception as e:
                logger.error(f"Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„
                try:
                    time.sleep(1)
                    service = Service(ChromeDriverManager().install())
                    self.driver = webdriver.Chrome(service=service, options=chrome_options)
                    self.driver.set_page_load_timeout(self.timeout)
                except Exception as e:
                    logger.error(f"Selenium ë“œë¼ì´ë²„ ì¬ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    raise

    def __del__(self) -> None:
        """ê°ì²´ê°€ ì†Œë©¸ë  ë•Œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (ë°±ì—…ìš©)"""
        try:
            # cleanupì´ ì´ë¯¸ í˜¸ì¶œë˜ì—ˆëŠ”ì§€ í™•ì¸
            if hasattr(self, 'driver') and self.driver is not None:
                logger.warning("__del__ì—ì„œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ - cleanup()ì´ ëª…ì‹œì ìœ¼ë¡œ í˜¸ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                self.cleanup()
        except Exception as e:
            # __del__ì—ì„œëŠ” ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚¤ì§€ ì•ŠìŒ
            pass

    def close_driver(self) -> None:
        """Selenium ë“œë¼ì´ë²„ ì•ˆì „í•˜ê²Œ ì¢…ë£Œ"""
        if hasattr(self, 'driver') and self.driver is not None:
            try:
                self.driver.quit()
                self.driver = None
                logger.info("ì›¹ë“œë¼ì´ë²„ê°€ ì„±ê³µì ìœ¼ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤")
            except Exception as e:
                logger.error(f"ì›¹ë“œë¼ì´ë²„ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def check_page_title(self, html_content: str) -> bool:
        """í˜ì´ì§€ head ì˜ì—­ì—ì„œ ì˜¤ë¥˜ ë©”ì‹œì§€ í™•ì¸ (Alert 404, Alert 500, ê´€ë¦¬ëª¨ë“œ ë“±)"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # 1. í˜ì´ì§€ ì œëª© í™•ì¸
            title_tag = soup.find('title')
            if title_tag and title_tag.string:
                title_text = title_tag.string.strip().lower()
                
                for pattern in ERROR_PATTERNS:
                    if pattern.lower() in title_text:
                        logger.warning(f"ì˜¤ë¥˜ í˜ì´ì§€ ì œëª© ê°ì§€ (íŒ¨í„´: '{pattern}'): {title_text}")
                        return True
            
            # 2. head ì˜ì—­ì˜ meta íƒœê·¸ í™•ì¸
            head_tag = soup.find('head')
            if head_tag:
                # meta description, keywords ë“± í™•ì¸
                meta_tags = head_tag.find_all('meta')
                for meta in meta_tags:
                    content = meta.get('content', '').lower()
                    name = meta.get('name', '').lower()
                    
                    for pattern in ERROR_PATTERNS:
                        if pattern.lower() in content or pattern.lower() in name:
                            logger.warning(f"ì˜¤ë¥˜ í˜ì´ì§€ meta íƒœê·¸ ê°ì§€ (íŒ¨í„´: '{pattern}')")
                            return True
            
            # 3. íŠ¹ì • ì˜¤ë¥˜ ê´€ë ¨ í´ë˜ìŠ¤ë‚˜ ID í™•ì¸ (head ì˜ì—­ë§Œ)
            error_selectors = [
                'head .error', 'head #error', 'head .alert-error', 'head .error-message', 
                'head .not-found', 'head .page-not-found', 'head .admin-mode', 'head .ê´€ë¦¬ëª¨ë“œ'
            ]
            
            for selector in error_selectors:
                try:
                    error_elements = soup.select(selector)
                    if error_elements:
                        logger.warning(f"ì˜¤ë¥˜ í˜ì´ì§€ ìš”ì†Œ ê°ì§€ (ì„ íƒì: '{selector}')")
                        return True
                except Exception as selector_error:
                    # CSS ì„ íƒì ì˜¤ë¥˜ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
                    logger.debug(f"CSS ì„ íƒì ì˜¤ë¥˜ ë¬´ì‹œ: {selector} - {selector_error}")
                    continue
            
            return False
            
        except Exception as e:
            logger.error(f"í˜ì´ì§€ head ì˜ì—­ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False

    def normalize_url(self, url: str) -> str:
        """URL ì •ê·œí™”í•˜ì—¬ í”„ë˜ê·¸ë¨¼íŠ¸ì™€ í›„í–‰ ìŠ¬ë˜ì‹œ ì œê±°, í˜ì´ì§€ë„¤ì´ì…˜ ê³ ë ¤ (ìºì‹œ ì ìš©)"""
        if not url:
            return ""
        
        # ìºì‹œì—ì„œ í™•ì¸
        cached_result = self.normalization_cache.get(url)
        if cached_result is not None:
            return cached_result
        
        original_url = url
        
        # í”„ë˜ê·¸ë¨¼íŠ¸ ì œê±°
        if '#' in url:
            url = url.split('#')[0]
        
        # í”„ë¡œí† ì½œ í™•ì¸
        if not (url.startswith('http://') or url.startswith('https://')):
            url = 'https://' + url
        
        # URL íŒŒì‹±
        parsed = urlparse(url)
        path = parsed.path
        query_params = parse_qs(parsed.query)
        
        # íŒŒì¼ í™•ì¥ì í™•ì¸í•˜ì—¬ ì œì™¸ (ìºì‹œì— ì €ì¥í•˜ì§€ ì•ŠìŒ)
        if any(path.lower().endswith(ext) for ext in EXCLUDE_EXTENSIONS):
            return ""
        
        # ê¸°ë³¸ URL (ê²½ë¡œê¹Œì§€)
        base_url = f"{parsed.scheme}://{parsed.netloc}{path}"
        
        # ë„ë©”ì¸ ë£¨íŠ¸ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
        if parsed.path == '/':
            self.normalization_cache.put(original_url, url)
            return url
        
        # httpì™€ https í‘œì¤€í™” (https ì‚¬ìš©)
        if parsed.scheme == 'http':
            base_url = base_url.replace('http://', 'https://')
        
        # wwwì™€ non-www í‘œì¤€í™” (ë™ì¼í•œ ì‚¬ì´íŠ¸ë¡œ ì²˜ë¦¬)
        if parsed.netloc.startswith('www.'):
            non_www = base_url.replace(f"{parsed.scheme}://www.", f"{parsed.scheme}://")
            base_url = non_www
        else:
            www_version = base_url.replace(f"{parsed.scheme}://", f"{parsed.scheme}://www.")
            if self.base_domain.startswith('www.') and not parsed.netloc.startswith('www.'):
                base_url = www_version
        
        # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì •ê·œí™”
        normalized_query_params = {}
        
        # ì§ì ‘ì ì¸ í˜ì´ì§€ íŒŒë¼ë¯¸í„°ê°€ ìˆëŠ” ê²½ìš°
        if 'page' in query_params:
            page_num = query_params['page'][0]
            # page=1ì¸ ê²½ìš° ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±° (ê¸°ë³¸ URLë§Œ ë°˜í™˜)
            if page_num == '1':
                result = base_url
            else:
                result = f"{base_url}?page={page_num}"
            self.normalization_cache.put(original_url, result)
            return result
        
        # enc íŒŒë¼ë¯¸í„°ì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ ì‹œë„
        if 'enc' in query_params:
            enc_value = query_params['enc'][0]
            try:
                # Base64 ë””ì½”ë”© ì‹œë„
                decoded = base64.b64decode(enc_value).decode('utf-8')
                
                # í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œì„ ìœ„í•œ ì •ê·œì‹
                page_match = re.search(r'page%3D(\d+)', decoded)
                if page_match:
                    page_num = page_match.group(1)
                    # page=1ì¸ ê²½ìš° ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±° (ê¸°ë³¸ URLë§Œ ë°˜í™˜)
                    if page_num == '1':
                        result = base_url
                    else:
                        result = f"{base_url}?page={page_num}"
                    self.normalization_cache.put(original_url, result)
                    return result
            except Exception as e:
                logger.debug(f"Base64 ë””ì½”ë”© ì‹¤íŒ¨: {e}")
                # ë””ì½”ë”© ì‹¤íŒ¨ ì‹œ ê³„ì† ì§„í–‰
                pass
        
        # ë‹¤ë¥¸ ì¼ë°˜ì ì¸ í˜ì´ì§€ë„¤ì´ì…˜ íŒŒë¼ë¯¸í„° í™•ì¸
        for page_param in ['pageNo', 'pageIndex', 'p', 'pg', 'pageNum']:
            if page_param in query_params:
                page_num = query_params[page_param][0]
                # page ë²ˆí˜¸ê°€ 1ì¸ ê²½ìš° ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±° (ê¸°ë³¸ URLë§Œ ë°˜í™˜)
                if page_num == '1':
                    result = base_url
                else:
                    result = f"{base_url}?page={page_num}"  # í‘œì¤€í™”ëœ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                self.normalization_cache.put(original_url, result)
                return result
        
        # ì¤‘ìš”í•œ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ë³´ì¡´ (ê²€ìƒ‰, ì¹´í…Œê³ ë¦¬ ë“±)
        important_params = ['q', 'query', 'search', 'category', 'type', 'id', 'no']
        for param in important_params:
            if param in query_params:
                normalized_query_params[param] = query_params[param][0]
        
        # ì •ê·œí™”ëœ ì¿¼ë¦¬ ë¬¸ìì—´ ìƒì„±
        if normalized_query_params:
            query_string = '&'.join([f"{k}={v}" for k, v in sorted(normalized_query_params.items())])
            result = f"{base_url}?{query_string}"
        else:
            # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ê°€ ì—†ëŠ” URLì˜ í›„í–‰ ìŠ¬ë˜ì‹œ ì œê±°
            result = base_url.rstrip('/')
        
        # ìºì‹œì— ì €ì¥
        self.normalization_cache.put(original_url, result)
        return result

    def is_in_scope(self, url: str) -> bool:
        """URLì´ ì •ì˜ëœ í¬ë¡¤ë§ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸ (ëª¨ë“  íŒ¨í„´ í¬í•¨ í•„ìˆ˜)"""
        try:
            parsed = urlparse(url)
            
            # ë„ë©”ì¸ í™•ì¸ - ê¸°ë³¸ ë„ë©”ì¸ì— ì†í•´ì•¼ í•¨
            if self.base_domain not in parsed.netloc:
                return False
            
            # scope_patternsê°€ ë¹„ì–´ìˆê±°ë‚˜ ë¹ˆ ë¬¸ìì—´ë§Œ ìˆìœ¼ë©´ ë„ë©”ì¸ ì „ì²´ í—ˆìš©
            if not self.scope_patterns or (len(self.scope_patterns) == 1 and self.scope_patterns[0] == ''):
                return True
            
            # URLì„ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ íŒ¨í„´ ë§¤ì¹­
            url_lower = url.lower()
            url_path = parsed.path.lower()
            url_query = parsed.query.lower()
            
            # ğŸ¯ NEW: ëª¨ë“  íŒ¨í„´ì´ URLì— í¬í•¨ë˜ì–´ì•¼ í•¨ (AND ì¡°ê±´)
            matched_patterns = []
            
            for pattern in self.scope_patterns:
                pattern_lower = pattern.lower()
                
                # íŒ¨í„´ì´ URLì˜ ì–´ë””ë“  í¬í•¨ë˜ë©´ ë§¤ì¹­
                if (pattern_lower in url_path or 
                    pattern_lower in url_query or 
                    pattern_lower in url_lower):
                    matched_patterns.append(pattern)
            
            # ğŸ¯ ëª¨ë“  íŒ¨í„´ì´ ë§¤ì¹­ë˜ì–´ì•¼ ë²”ìœ„ ë‚´ë¡œ íŒë‹¨
            is_all_matched = len(matched_patterns) == len(self.scope_patterns)
            
            if is_all_matched:
                logger.debug(f"URLì´ ë²”ìœ„ ë‚´ (ëª¨ë“  íŒ¨í„´ ë§¤ì¹­): {url}")
                logger.debug(f"í•„ìš” íŒ¨í„´: {self.scope_patterns}")
                logger.debug(f"ë§¤ì¹­ëœ íŒ¨í„´: {matched_patterns}")
                return True
            else:
                logger.debug(f"URLì´ ë²”ìœ„ ë°– (ì¼ë¶€ íŒ¨í„´ë§Œ ë§¤ì¹­): {url}")
                logger.debug(f"í•„ìš” íŒ¨í„´: {self.scope_patterns}")
                logger.debug(f"ë§¤ì¹­ëœ íŒ¨í„´: {matched_patterns}")
                logger.debug(f"ëˆ„ë½ëœ íŒ¨í„´: {set(self.scope_patterns) - set(matched_patterns)}")
                return False
        
        except Exception as e:
            logger.error(f"URL ë²”ìœ„ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False
    
    def should_exclude_url(self, url: str) -> bool:
        """URLì´ ì •ì˜ëœ íŒ¨í„´ì— ë”°ë¼ ì œì™¸ë˜ì–´ì•¼ í•˜ëŠ”ì§€ í™•ì¸ (ìŠ¤ë ˆë“œ ì•ˆì „)"""
        with self.url_lock:
            # ì´ë¯¸ ì œì™¸ë¡œ í™•ì¸ëœ URLì¸ì§€ ê²€ì‚¬
            if url in self.excluded_urls:
                return True
                
            lower_url = url.lower()
            
            # rssList.doë¡œ ëë‚˜ëŠ” URL ì œì™¸
            if lower_url.endswith('rsslist.do'):
                logger.debug(f"rssList.do íŒ¨í„´ìœ¼ë¡œ URL ì œì™¸: {url}")
                self.excluded_urls.add(url)
                return True
            
            # ì œì™¸ íŒ¨í„´ í™•ì¸
            for pattern in EXCLUDE_PATTERNS:
                if pattern in lower_url:
                    logger.debug(f"ì œì™¸ íŒ¨í„´ '{pattern}' ë§¤ì¹­ìœ¼ë¡œ URL ì œì™¸: {url}")
                    self.excluded_urls.add(url)  # ì œì™¸ URL ëª©ë¡ì— ì¶”ê°€
                    return True
                    
            return False

    def is_list_page(self, url: str) -> bool:
        """URLì´ ê²Œì‹œíŒ ëª©ë¡ í˜ì´ì§€ì¸ì§€ í™•ì¸"""
        lower_url = url.lower()
        for pattern in LIST_PAGE_PATTERNS:
            if pattern in lower_url:
                return True
        return False

    def is_valid_file_url(self, href: str, base_url: str) -> bool:
        """URLì´ ìœ íš¨í•œ ë¬¸ì„œ íŒŒì¼ URLì¸ì§€ í™•ì¸ (DOC_EXTENSIONS + íŠ¹ì • ë‹¤ìš´ë¡œë“œ íŒ¨í„´)"""
        if not href or href == '#' or href.startswith(('javascript:', 'mailto:', 'tel:')):
            return False
        
        lower_url = href.lower()
        
        # ì œì™¸í•  í™•ì¥ì í™•ì¸
        if any(lower_url.endswith(ext) for ext in EXCLUDE_EXTENSIONS):
            return False
        
        # ì œì™¸ íŒ¨í„´ í™•ì¸
        for pattern in EXCLUDE_PATTERNS:
            if pattern in lower_url:
                return False
        
        # 1. ë¬¸ì„œ íŒŒì¼ í™•ì¥ì í™•ì¸ (DOC_EXTENSIONSì— í•´ë‹¹í•˜ëŠ” ê²ƒë§Œ)
        for ext in DOC_EXTENSIONS:
            if lower_url.endswith(ext):
                return True
        
        # 2. ëª…í™•í•œ ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ íŒ¨í„´ í™•ì¸ (.do íŒŒì¼ë“¤ ìµœìš°ì„ )
        # ğŸ”¥ ëª…í™•í•œ ë‹¤ìš´ë¡œë“œ íŒ¨í„´ë“¤ì€ í•­ìƒ ë¬¸ì„œë¡œ ë¶„ë¥˜
        for pattern in EXPLICIT_DOWNLOAD_PATTERNS:
            if pattern in lower_url:
                return True
        
        # 3. ì¼ë°˜ì ì¸ ë‹¤ìš´ë¡œë“œ íŒ¨í„´ í™•ì¸ (origin ë°©ì‹ ì ìš©)
        for pattern in DOWNLOAD_PATTERNS:
            if pattern in lower_url:
                # ëª…í™•íˆ ì œì™¸í•´ì•¼ í•  íŒ¨í„´ë“¤ í™•ì¸
                exclude_keywords = ['software', 'program', 'app', 'installer', 'setup']
                if not any(keyword in lower_url for keyword in exclude_keywords):
                    # download.doì™€ fileDown.doëŠ” í•­ìƒ ë¬¸ì„œë¡œ ë¶„ë¥˜ (originê³¼ ë™ì¼)
                    if pattern in ['/download.do', 'download.do', 'fileDown.do']:
                        return True
                    # ê²Œì‹œíŒì´ë‚˜ ì²¨ë¶€íŒŒì¼ ê´€ë ¨ ê²½ë¡œì¸ì§€ í™•ì¸
                    elif (any(keyword in lower_url for keyword in ['/bbs/', '/board/', '/attach', '/file', '/document']) or
                        not any(keyword in lower_url for keyword in ['software', 'media', 'image', 'video'])):
                        return True
        
        # 4. URL ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ì—ì„œ íŒŒì¼ ê´€ë ¨ ì •ë³´ í™•ì¸ (ê°œì„ )
        if '?' in href:
            query_part = href.split('?', 1)[1]
            # íŒŒì¼ ê´€ë ¨ íŒŒë¼ë¯¸í„°ë“¤
            file_params = ['file', 'filename', 'attach', 'download', 'doc', 'document']
            if any(param in query_part.lower() for param in file_params):
                return True
            
            # ê²Œì‹œíŒ ì²¨ë¶€íŒŒì¼ íŒ¨í„´ (í•œêµ­ ì‚¬ì´íŠ¸ íŠ¹í™”)
            korean_patterns = ['ì²¨ë¶€', 'íŒŒì¼', 'ìë£Œ', 'ë¬¸ì„œ']
            if any(pattern in query_part for pattern in korean_patterns):
                return True
        
        return False

    def add_url_atomically(self, url: str, url_set: Set[str]) -> bool:
        """URLì„ ì›ìì ìœ¼ë¡œ ì§‘í•©ì— ì¶”ê°€ (ì¤‘ë³µ ì²´í¬ í¬í•¨)"""
        with self.url_lock:
            if url not in url_set:
                url_set.add(url)
                return True
            return False

    def extract_links(self, soup: BeautifulSoup, base_url: str) -> Tuple[Set[str], Set[str], Set[str], Set[str]]:
        """í˜ì´ì§€ì—ì„œ ë§í¬ë¥¼ êµ¬ë¶„í•˜ì—¬ ì¶”ì¶œ: (ë„¤ë¹„ê²Œì´ì…˜ ë§í¬, ì½˜í…ì¸  ë§í¬, ë¬¸ì„œ ë§í¬, ë©”ë‰´ ì •ë³´)"""
        nav_links = set()      # ë„¤ë¹„ê²Œì´ì…˜/ë©”ë‰´ ë§í¬
        content_links = set()  # ì½˜í…ì¸  ì˜ì—­ ë§í¬
        doc_links = set()      # ë¬¸ì„œ íŒŒì¼ ë§í¬
        menu_info = set()      # ë©”ë‰´ êµ¬ì¡° ì •ë³´
        
        try:
            # ğŸ†• 1. ul/li ê³„ì¸µ êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ ë„¤ë¹„ê²Œì´ì…˜ ë§í¬ ì¶”ì¶œ (ìš°ì„  ì²˜ë¦¬)
            hierarchical_nav_links, hierarchical_menu_info = self._extract_hierarchical_navigation_links(soup, base_url)
            nav_links.update(hierarchical_nav_links)
            menu_info.update(hierarchical_menu_info)
            
            # 2. ì¼ë°˜ ë§í¬ ì¶”ì¶œ - origin ë°©ì‹ í†µí•© (ê³„ì¸µ êµ¬ì¡°ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ëœ ë§í¬ëŠ” ì œì™¸)
            for anchor in soup.find_all('a', href=True):
                href = anchor['href']
                
                # ë¹ˆ ë§í¬, ìë°”ìŠ¤í¬ë¦½íŠ¸, íŠ¹ìˆ˜ í”„ë¡œí† ì½œ, ë¯¸ë””ì–´ íŒŒì¼ ë“± ê±´ë„ˆë›°ê¸°
                if (not href or 
                    href.startswith(('javascript:', 'mailto:', 'tel:')) or 
                    href == '#' or
                    href.startswith('#')):  # í˜ì´ì§€ ë‚´ ì•µì»¤ ë§í¬
                    continue
                
                # ì ˆëŒ€ URL ìƒì„± - ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬ ê°œì„ 
                if href.startswith('/'):
                    full_url = f"https://{self.base_domain}{href}"
                else:
                    full_url = urljoin(base_url, href)
                
                # URL ì •ê·œí™” (ìºì‹œ ì ìš©)
                normalized_url = self.normalize_url(full_url)
                if not normalized_url:  # ì •ê·œí™” ì‹¤íŒ¨ ì‹œ ê±´ë„ˆë›°ê¸°
                    continue
                
                # ì œì™¸ íŒ¨í„´ í™•ì¸ (ì •ê·œí™”ëœ URLë¡œ)
                if self.should_exclude_url(normalized_url):
                    continue
                
                # ì´ë¯¸ ê³„ì¸µ êµ¬ì¡°ì—ì„œ ì²˜ë¦¬ëœ ë„¤ë¹„ê²Œì´ì…˜ ë§í¬ëŠ” ê±´ë„ˆë›°ê¸°
                if normalized_url in nav_links:
                    continue
                
                # ë¬¸ì„œ íŒŒì¼ì¸ì§€ í™•ì¸ - origin ë°©ì‹ ì ìš©
                is_doc = False
                lower_url = normalized_url.lower()

                # 1. í™•ì¥ì ê¸°ë°˜ ë¬¸ì„œ íŒŒì¼ í™•ì¸
                for ext in DOC_EXTENSIONS:
                    if lower_url.endswith(ext):
                        doc_links.add(normalized_url)
                        is_doc = True
                        break
                
                # 2. ì„ ë³„ì  ë‹¤ìš´ë¡œë“œ íŒ¨í„´ í™•ì¸ (ê²Œì‹œíŒ ì²¨ë¶€íŒŒì¼ ë“±) - origin ë¡œì§ ë³µì‚¬
                if not is_doc:
                    for pattern in DOWNLOAD_PATTERNS:
                        if pattern in lower_url:
                            # ëª…í™•íˆ ì œì™¸í•´ì•¼ í•  íŒ¨í„´ë“¤ í™•ì¸
                            exclude_keywords = ['software', 'program', 'app', 'installer', 'setup']
                            if not any(keyword in lower_url for keyword in exclude_keywords):
                                # download.doì™€ fileDown.doëŠ” í•­ìƒ ë¬¸ì„œë¡œ ë¶„ë¥˜
                                if pattern in ['/download.do', 'download.do', 'fileDown.do']:
                                    doc_links.add(normalized_url)
                                    is_doc = True
                                    break
                                # ê¸°íƒ€ íŒ¨í„´ì€ ê²Œì‹œíŒì´ë‚˜ ì²¨ë¶€íŒŒì¼ ê´€ë ¨ ê²½ë¡œì¸ì§€ í™•ì¸
                                elif (any(keyword in lower_url for keyword in ['/bbs/', '/board/', '/attach', '/file', '/document']) or
                                    not any(keyword in lower_url for keyword in ['software', 'media', 'image', 'video'])):
                                    doc_links.add(normalized_url)
                                    is_doc = True
                                    break
                
                # ë²”ìœ„ í™•ì¸ ë° ë„¤ë¹„ê²Œì´ì…˜/ì½˜í…ì¸  ë¶„ë¥˜
                if not is_doc and self.is_in_scope(normalized_url):
                    # ë„¤ë¹„ê²Œì´ì…˜ ë§í¬ í™•ì¸
                    link_text = anchor.get_text().strip()
                    if self._is_valid_navigation_link(href, link_text):
                        nav_links.add(normalized_url)
                        # ë©”ë‰´ êµ¬ì¡° ì •ë³´ ì €ì¥
                        menu_info.add(f"general:{normalized_url}:{link_text}")
                    else:
                        content_links.add(normalized_url)

            # 2. ë©”ë‰´ ë° ë„¤ë¹„ê²Œì´ì…˜ ë§í¬ íŠ¹ë³„ ì²˜ë¦¬ - origin ë°©ì‹ ì¶”ê°€
            nav_selectors = [
                'nav a', '.nav a', '.navigation a', '.menu a', '.gnb a', '.lnb a',
                '.main-menu a', '.sub-menu a', '.sidebar a', '.footer a',
                '[class*="menu"] a', '[class*="nav"] a', '[id*="menu"] a', '[id*="nav"] a'
            ]
            
            for selector in nav_selectors:
                nav_elements = soup.select(selector)
                for link in nav_elements:
                    if link.has_attr('href'):
                        href = link['href']
                        if href and href != '#' and not href.startswith(('javascript:', 'mailto:', 'tel:')):
                            if href.startswith('/'):
                                full_url = f"https://{self.base_domain}{href}"
                            else:
                                full_url = urljoin(base_url, href)
                            
                            normalized_url = self.normalize_url(full_url)
                            if (normalized_url and 
                                not self.should_exclude_url(normalized_url) and 
                                self.is_in_scope(normalized_url)):
                                
                                # ë¬¸ì„œì¸ì§€ í™•ì¸
                                if self.is_valid_file_url(href, base_url):
                                    doc_links.add(normalized_url)
                                else:
                                    nav_links.add(normalized_url)
                                    link_text = link.get_text().strip()
                                    menu_info.add(f"nav:{normalized_url}:{link_text}")

            # 3. ì²¨ë¶€íŒŒì¼ ì¶”ê°€ ì²˜ë¦¬
            attachment_links = self.extract_attachments(soup, base_url)
            doc_links.update(attachment_links)
            
            logger.debug(f"ë§í¬ ì¶”ì¶œ ì™„ë£Œ - ë„¤ë¹„ê²Œì´ì…˜: {len(nav_links)}ê°œ, ì½˜í…ì¸ : {len(content_links)}ê°œ, ë¬¸ì„œ: {len(doc_links)}ê°œ")
            return nav_links, content_links, doc_links, menu_info
            
        except Exception as e:
            logger.error(f"ë§í¬ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return set(), set(), set(), set()
    
    def _is_valid_navigation_link(self, href: str, link_text: str) -> bool:
        """ë„¤ë¹„ê²Œì´ì…˜ ë§í¬ì˜ ìœ íš¨ì„± ê²€ì‚¬"""
        if (not href or href == '#' or 
            href.startswith(('javascript:', 'mailto:', 'tel:')) or
            href.startswith('#')):
            return False
        
        # ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ë©”ë‰´ í•­ëª©ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
        return len(link_text) <= 20
    
    def _build_full_url(self, href: str, base_url: str) -> str:
        """ìƒëŒ€/ì ˆëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜"""
        if href.startswith('/'):
            return f"https://{self.base_domain}{href}"
        else:
            return urljoin(base_url, href)
    
    def _should_include_link(self, normalized_url: str) -> bool:
        """ë§í¬ í¬í•¨ ì—¬ë¶€ ê²€ì‚¬"""
        return (normalized_url and 
                not self.should_exclude_url(normalized_url) and 
                self.is_in_scope(normalized_url))
    
    def _extract_hierarchical_navigation_links(self, soup: BeautifulSoup, base_url: str) -> Tuple[Set[str], Set[str]]:
        """ul/li ê³„ì¸µ êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ ë„¤ë¹„ê²Œì´ì…˜ ë§í¬ ì¶”ì¶œ"""
        nav_links = set()
        menu_info = set()
        
        try:
            # ë„¤ë¹„ê²Œì´ì…˜ ê´€ë ¨ ul ìš”ì†Œë“¤ ì°¾ê¸°
            nav_ul_selectors = [
                'nav ul', '.nav ul', '.navigation ul', '.menu ul', 
                '.gnb ul', '.lnb ul', '.main-menu ul', '.sub-menu ul',
                'ul.flex', 'ul.pageNavigation', 'ul[class*="menu"]', 
                'ul[class*="nav"]', 'ul[id*="menu"]', 'ul[id*="nav"]'
            ]
            
            processed_uls = set()  # ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€
            
            for selector in nav_ul_selectors:
                ul_elements = soup.select(selector)
                
                for ul in ul_elements:
                    # ì´ë¯¸ ì²˜ë¦¬ëœ ul ìš”ì†ŒëŠ” ê±´ë„ˆë›°ê¸°
                    ul_id = id(ul)
                    if ul_id in processed_uls:
                        continue
                    processed_uls.add(ul_id)
                    
                    # ul/li ê³„ì¸µ êµ¬ì¡° ë¶„ì„
                    extracted_links, extracted_menu_info = self._analyze_ul_li_hierarchy(ul, base_url, depth=0)
                    nav_links.update(extracted_links)
                    menu_info.update(extracted_menu_info)
            
            logger.debug(f"ğŸ—ï¸ ê³„ì¸µ êµ¬ì¡° ê¸°ë°˜ ë„¤ë¹„ê²Œì´ì…˜ ë§í¬ ì¶”ì¶œ: {len(nav_links)}ê°œ")
            return nav_links, menu_info
            
        except Exception as e:
            logger.error(f"ê³„ì¸µ êµ¬ì¡° ë„¤ë¹„ê²Œì´ì…˜ ë§í¬ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return set(), set()
    
    def _analyze_ul_li_hierarchy(self, ul_element, base_url: str, depth: int = 0, parent_path: str = "") -> Tuple[Set[str], Set[str]]:
        """ul ìš”ì†Œì˜ li ê³„ì¸µ êµ¬ì¡°ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ë¶„ì„"""
        nav_links = set()
        menu_info = set()
        
        try:
            # ìµœëŒ€ ê¹Šì´ ì œí•œ (ë¬´í•œ ì¬ê·€ ë°©ì§€)
            if depth > 5:
                return nav_links, menu_info
            
            # ì§ì ‘ì ì¸ li ìì‹ë“¤ë§Œ ì²˜ë¦¬ (recursive=False)
            direct_li_children = ul_element.find_all('li', recursive=False)
            
            for li in direct_li_children:
                # li ë‚´ì˜ ì§ì ‘ì ì¸ a íƒœê·¸ ì°¾ê¸°
                direct_link = li.find('a', recursive=False)
                
                if direct_link and direct_link.has_attr('href'):
                    href = direct_link['href']
                    link_text = direct_link.get_text().strip()
                    
                    # ìœ íš¨í•œ ë§í¬ì¸ì§€ í™•ì¸
                    if (href and href != '#' and 
                        not href.startswith(('javascript:', 'mailto:', 'tel:'))):
                        
                        # ì ˆëŒ€ URL ìƒì„±
                        if href.startswith('/'):
                            full_url = f"https://{self.base_domain}{href}"
                        else:
                            full_url = urljoin(base_url, href)
                        
                        # URL ì •ê·œí™”
                        normalized_url = self.normalize_url(full_url)
                        
                        if (normalized_url and 
                            not self.should_exclude_url(normalized_url) and 
                            self.is_in_scope(normalized_url)):
                            
                            nav_links.add(normalized_url)
                            
                            # ê³„ì¸µ ê²½ë¡œ êµ¬ì„±
                            current_path = f"{parent_path}/{link_text}" if parent_path else link_text
                            menu_info.add(f"hierarchical:{normalized_url}:{current_path}:depth_{depth}")
                            
                            logger.debug(f"ğŸ”— ê³„ì¸µ ë§í¬ ë°œê²¬ (ê¹Šì´ {depth}): {current_path} â†’ {normalized_url}")
                
                # í•˜ìœ„ ul ìš”ì†Œë“¤ ì¬ê·€ ì²˜ë¦¬
                child_uls = li.find_all('ul', recursive=False)  # ì§ì ‘ì ì¸ ul ìì‹ë“¤ë§Œ
                
                for child_ul in child_uls:
                    # í˜„ì¬ liì˜ í…ìŠ¤íŠ¸ë¥¼ ë¶€ëª¨ ê²½ë¡œë¡œ ì‚¬ìš©
                    current_li_text = ""
                    if direct_link:
                        current_li_text = direct_link.get_text().strip()
                    else:
                        # a íƒœê·¸ê°€ ì—†ëŠ” ê²½ìš° liì˜ ì§ì ‘ í…ìŠ¤íŠ¸ ì‚¬ìš©
                        li_texts = []
                        for text in li.find_all(text=True, recursive=False):
                            clean_text = text.strip()
                            if clean_text:
                                li_texts.append(clean_text)
                        current_li_text = ' '.join(li_texts) if li_texts else f"ë©”ë‰´{depth}"
                    
                    child_parent_path = f"{parent_path}/{current_li_text}" if parent_path else current_li_text
                    
                    # ì¬ê·€ í˜¸ì¶œ
                    child_links, child_menu_info = self._analyze_ul_li_hierarchy(
                        child_ul, base_url, depth + 1, child_parent_path
                    )
                    
                    nav_links.update(child_links)
                    menu_info.update(child_menu_info)
            
            return nav_links, menu_info
            
        except Exception as e:
            logger.error(f"ul/li ê³„ì¸µ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ (ê¹Šì´ {depth}): {e}")
            return nav_links, menu_info

    def extract_attachments(self, soup: BeautifulSoup, base_url: str) -> Set[str]:
        """í˜ì´ì§€ì—ì„œ ì²¨ë¶€íŒŒì¼ ë§í¬ ì¶”ì¶œ"""
        doc_links = set()
        
        try:
            # 1. ì²¨ë¶€íŒŒì¼ í´ë˜ìŠ¤ í™•ì¸ (ATTACHMENT_CLASSES ìƒìˆ˜ í™œìš©)
            for class_name in ATTACHMENT_CLASSES:
                # í´ë˜ìŠ¤ëª…ì´ í¬í•¨ëœ ëª¨ë“  ìš”ì†Œ ì°¾ê¸°
                sections = soup.find_all(class_=lambda c: c and class_name in str(c).lower())
                for section in sections:
                    links = section.find_all('a', href=True)
                    for link in links:
                        href = link['href']
                        # synapview.do ë§í¬ ì œì™¸
                        if 'synapView.do' in href.lower():
                            continue

                        if self.is_valid_file_url(href, base_url):
                            if href.startswith('/'):
                                full_url = f"https://{self.base_domain}{href}"
                            else:
                                full_url = urljoin(base_url, href)
                            
                            # ë¬¸ì„œ URLë„ ì •ê·œí™”í•˜ì—¬ ì¶”ê°€
                            normalized_url = self.normalize_url(full_url)
                            if normalized_url:
                                doc_links.add(normalized_url)
            
            # 2. íŒŒì¼ ê´€ë ¨ ì†ì„± í™•ì¸ (ATTACHMENT_CLASSES ê¸°ë°˜)
            file_keywords = ['file', 'attach', 'download'] + ATTACHMENT_CLASSES
            file_related_elements = soup.find_all(
                ['a', 'span', 'div', 'li'], 
                attrs=lambda attr: attr and any(
                    keyword in str(attr).lower() 
                    for keyword in file_keywords
                )
            )
            
            for element in file_related_elements:
                links = [element] if element.name == 'a' and element.has_attr('href') else element.find_all('a', href=True)
                for link in links:
                    href = link['href']
                    if self.is_valid_file_url(href, base_url):
                        if href.startswith('/'):
                            full_url = f"https://{self.base_domain}{href}"
                        else:
                            full_url = urljoin(base_url, href)
                            
                        # ë¬¸ì„œ URLë„ ì •ê·œí™”í•˜ì—¬ ì¶”ê°€
                        normalized_url = self.normalize_url(full_url)
                        if normalized_url:
                            doc_links.add(normalized_url)
            
            # 3. DOWNLOAD_PATTERNSë¥¼ í¬í•¨í•˜ëŠ” ëª¨ë“  ë§í¬ ì§ì ‘ ê²€ìƒ‰ - origin ë°©ì‹ ì ìš©
            for link in soup.find_all('a', href=True):
                href = link['href']
                lower_href = href.lower()
                
                # DOWNLOAD_PATTERNS ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ë©´ ê²€ì‚¬
                if any(pattern in lower_href for pattern in DOWNLOAD_PATTERNS):
                    if self.is_valid_file_url(href, base_url):
                        if href.startswith('/'):
                            full_url = f"https://{self.base_domain}{href}"
                        else:
                            full_url = urljoin(base_url, href)
                            
                        # ë¬¸ì„œ URLë„ ì •ê·œí™”í•˜ì—¬ ì¶”ê°€
                        normalized_url = self.normalize_url(full_url)
                        if normalized_url:
                            doc_links.add(normalized_url)
                            
            return doc_links
            
        except Exception as e:
            logger.error(f"ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return set()

    def detect_pagination(self, soup: BeautifulSoup) -> Tuple[bool, Optional[Any]]:
        """í˜ì´ì§€ì—ì„œ í˜ì´ì§€ë„¤ì´ì…˜ ìš”ì†Œ ê°ì§€ (Origin ìŠ¤íƒ€ì¼)"""
        try:
            for selector in PAGINATION_SELECTORS:
                elements = soup.select(selector)
                if elements:
                    return True, elements[0]
                    
            return False, None
        except Exception as e:
            logger.error(f"BeautifulSoupìœ¼ë¡œ í˜ì´ì§€ë„¤ì´ì…˜ ê°ì§€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False, None

    def handle_pagination(self, soup: BeautifulSoup, current_url: str) -> List[str]:
        """í˜ì´ì§€ë„¤ì´ì…˜ì„ ì²˜ë¦¬í•˜ì—¬ ëª¨ë“  í˜ì´ì§€ URL ë°˜í™˜ (Originì—ì„œ ì´ì‹)"""
        pagination_urls = []
        
        # í˜„ì¬ URL ì •ê·œí™”
        current_url = self.normalize_url(current_url)
        
        # 1. í˜ì´ì§€ë„¤ì´ì…˜ ìš”ì†Œ ê°ì§€
        has_pagination, pagination_element = self.detect_pagination(soup)
        
        # í˜ì´ì§€ë„¤ì´ì…˜ ìš”ì†Œê°€ ê°ì§€ë˜ì—ˆì„ ë•Œ ë¡œê·¸ ì¶œë ¥
        if has_pagination:
            logger.debug(f"í˜ì´ì§€ë„¤ì´ì…˜ ìš”ì†Œ ê°ì§€: {has_pagination}, URL: {current_url}")
        
        if not has_pagination:
            # í˜ì´ì§€ë„¤ì´ì…˜ì´ ì—†ì–´ë„ "ë”ë³´ê¸°", "ë‹¤ìŒ" ë“±ì˜ ë§í¬ ì°¾ê¸°
            more_links = soup.find_all('a', href=True, string=re.compile(r'(ë”ë³´ê¸°|ë”\s*ë³´ê¸°|ë‹¤ìŒ|next|more)', re.IGNORECASE))
            for link in more_links:
                href = link['href']
                if href and href != '#' and not href.startswith('javascript:'):
                    full_url = urljoin(current_url, href)
                    if self.is_in_scope(full_url):
                        normalized_url = self.normalize_url(full_url)
                        if normalized_url not in pagination_urls and normalized_url != current_url:
                            pagination_urls.append(normalized_url)
                            logger.debug(f"'ë”ë³´ê¸°' ë§í¬ ë°œê²¬: {normalized_url}")
            return pagination_urls
        
        # 2. í˜ì´ì§€ URL íŒ¨í„´ê³¼ ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ íŒŒì•…
        url_pattern, last_page = self._extract_pagination_pattern(soup, current_url, pagination_element)
        
        # í˜ì´ì§€ ìˆ˜ë¥¼ 25ë¡œ ì œí•œ
        last_page = min(last_page, 25)
        
        logger.debug(f"URL íŒ¨í„´: {url_pattern}, ë§ˆì§€ë§‰ í˜ì´ì§€: {last_page}")
        
        # 3. ì§ì ‘ í˜ì´ì§€ ë§í¬ ì¶”ê°€
        if pagination_element:
            for a in pagination_element.find_all('a', href=True):
                href = a['href']
                if href and href != '#':
                    if href.startswith('javascript:'):
                        # JavaScript í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬ëŠ” ë³µì¡í•˜ë¯€ë¡œ ì¼ë‹¨ ìŠ¤í‚µ
                        continue
                    else:
                        # ì¼ë°˜ ë§í¬
                        full_url = urljoin(current_url, href)
                        if self.is_in_scope(full_url):
                            # URL ì •ê·œí™” ì ìš©
                            normalized_url = self.normalize_url(full_url)
                            if normalized_url not in pagination_urls and normalized_url != current_url:
                                pagination_urls.append(normalized_url)
        
        # 4. URL íŒ¨í„´ì´ ìˆê³  ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ê°€ ìˆëŠ” ê²½ìš°, ëª¨ë“  í˜ì´ì§€ URL ìƒì„±
        if url_pattern and last_page > 0:
            for page_num in range(1, min(last_page + 1, 26)):  # ìµœëŒ€ 25í˜ì´ì§€ë¡œ ì œí•œ
                page_url = url_pattern.replace('{page}', str(page_num))
                normalized_url = self.normalize_url(page_url)
                if normalized_url not in pagination_urls and normalized_url != current_url:
                    pagination_urls.append(normalized_url)
                    logger.debug(f"íŒ¨í„´ ê¸°ë°˜ í˜ì´ì§€ë„¤ì´ì…˜ URL ì¶”ê°€: {normalized_url}")
        
        # í˜ì´ì§€ë„¤ì´ì…˜ ì •ë³´ ë¡œê¹…
        if pagination_urls:
            logger.debug(f"í˜ì´ì§€ë„¤ì´ì…˜ ë°œê²¬: {len(pagination_urls)}ê°œ í˜ì´ì§€")
        
        return pagination_urls

    def _extract_pagination_pattern(self, soup: BeautifulSoup, current_url: str, pagination_element) -> Tuple[str, int]:
        """í˜ì´ì§€ë„¤ì´ì…˜ URL íŒ¨í„´ê³¼ ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ (Originì—ì„œ ì´ì‹)"""
        # ê¸°ë³¸ê°’ ì„¤ì •
        url_pattern = None
        last_page = 0
        
        try:
            # 1. í˜ì´ì§€ ë²ˆí˜¸ê°€ í¬í•¨ëœ ë§í¬ ì°¾ê¸°
            page_links = []
            if pagination_element:
                page_links = pagination_element.find_all('a', href=True)
            
            # 2. í˜ì´ì§€ ë²ˆí˜¸ì™€ í•´ë‹¹ URL ì¶”ì¶œ
            page_numbers = []
            page_urls = {}
            
            for link in page_links:
                # í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ ì‹œë„
                try:
                    page_num_text = link.get_text().strip()
                    if page_num_text.isdigit():
                        page_num = int(page_num_text)
                        href = link['href']
                        if href and href != '#' and not href.startswith('javascript:'):
                            full_url = urljoin(current_url, href)
                            page_numbers.append(page_num)
                            page_urls[page_num] = full_url
                        elif href.startswith('javascript:'):
                            # JavaScript í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬ - í˜ì´ì§€ ë²ˆí˜¸ë§Œ ê¸°ë¡
                            page_numbers.append(page_num)
                            # ì„ì‹œ URL íŒ¨í„´ ìƒì„± (ì‹¤ì œë¡œëŠ” ì‹¤í–‰ë˜ì§€ ì•ŠìŒ)
                            page_urls[page_num] = f"{current_url}?page={page_num}"
                except (ValueError, TypeError):
                    continue
            
            # 3. ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸°
            if page_numbers:
                last_page = max(page_numbers)
            
            # 4. í˜ì´ì§€ URL íŒ¨í„´ ì°¾ê¸°
            if page_urls:
                # í˜ì´ì§€ ë²ˆí˜¸ ë° URL íŒ¨í„´ íŒŒì•…
                url_pattern = self._find_url_pattern(page_urls)
            
            # 5. ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ê°€ ì—†ëŠ” ê²½ìš° ëŒ€ì²´ ë°©ë²•
            if last_page == 0:
                # "ë§ˆì§€ë§‰" ë˜ëŠ” "Last" ë§í¬ ì°¾ê¸°
                last_links = soup.select('a.last, a.end, a:contains("ë§ˆì§€ë§‰"), a:contains("Last")')
                for link in last_links:
                    href = link.get('href', '')
                    if href:
                        # URLì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ ì‹œë„
                        try:
                            # JavaScript ë§í¬ ì²˜ë¦¬
                            if href.startswith('javascript:'):
                                js_match = re.search(r"page_link\('?(\d+)'?\)", href)
                                if js_match:
                                    last_page = int(js_match.group(1))
                                    break
                            
                            # ì¼ë°˜ URL ì²˜ë¦¬
                            parsed_url = urlparse(href)
                            query_params = parse_qs(parsed_url.query)
                            
                            for param in ['page', 'pageNo', 'pageIndex', 'p', 'pg']:
                                if param in query_params:
                                    last_page = int(query_params[param][0])
                                    break
                        except:
                            pass
                # JavaScript í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ í˜ì´ì§€ ê°œìˆ˜ ì¶”ì • (ì¶”ê°€)
                if last_page == 0:
                    js_links = [a['href'] for a in pagination_element.find_all('a', href=True) 
                            if a['href'].startswith('javascript:page_link')]
                    if js_links:
                        last_page = len(js_links)
            
            if url_pattern:
                logger.debug(f"í˜ì´ì§€ë„¤ì´ì…˜ íŒ¨í„´ ì¶”ì¶œ: {url_pattern}, ë§ˆì§€ë§‰ í˜ì´ì§€: {last_page}")
            else:
                logger.debug(f"í˜ì´ì§€ë„¤ì´ì…˜ íŒ¨í„´ ì¶”ì¶œ ì‹¤íŒ¨, ëŒ€ì²´ ë°©ë²• ì‚¬ìš©")
                
            return url_pattern, last_page
            
        except Exception as e:
            logger.error(f"í˜ì´ì§€ë„¤ì´ì…˜ íŒ¨í„´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return None, 0

    def _find_url_pattern(self, page_urls: Dict[int, str]) -> str:
        """í˜ì´ì§€ URLì—ì„œ ì¼ê´€ëœ íŒ¨í„´ ì°¾ê¸° (Originì—ì„œ ì´ì‹)"""
        if len(page_urls) < 2:
            return None
        
        # ì •ë ¬ëœ í˜ì´ì§€ ë²ˆí˜¸ì™€ URL
        sorted_pages = sorted(page_urls.items())
        
        # URL ë¶„ì„
        patterns = []
        for page_num, url in sorted_pages:
            parsed = urlparse(url)
            path = parsed.path
            query = parse_qs(parsed.query)
            
            # ì¿¼ë¦¬ ë§¤ê°œë³€ìˆ˜ì—ì„œ í˜ì´ì§€ ë§¤ê°œë³€ìˆ˜ ì°¾ê¸°
            for param_name in ['page', 'pageNo', 'pageIndex', 'p', 'pg']:
                if param_name in query:
                    # URL íŒ¨í„´ êµ¬ì„±
                    query_copy = {k: v[0] if len(v) == 1 else v for k, v in query.items()}
                    query_copy[param_name] = '{page}'
                    
                    # ìƒˆ ì¿¼ë¦¬ ë¬¸ìì—´ ìƒì„±
                    query_parts = []
                    for k, v in query_copy.items():
                        if isinstance(v, list):
                            for item in v:
                                query_parts.append(f"{k}={item}")
                        else:
                            query_parts.append(f"{k}={v}")
                    
                    new_query = '&'.join(query_parts)
                    pattern = f"{parsed.scheme}://{parsed.netloc}{path}?{new_query}"
                    patterns.append(pattern)
                    break
        
        # ê°€ì¥ ë§ì´ ë°œê²¬ëœ íŒ¨í„´ ë°˜í™˜
        if patterns:
            return max(set(patterns), key=patterns.count)
        
        return None

    def extract_filename_from_download_url(self, url: str) -> str:
        """ë‹¤ìš´ë¡œë“œ URLì—ì„œ ì‹¤ì œ íŒŒì¼ëª… ì¶”ì¶œ"""
        try:
            # HEAD ìš”ì²­ìœ¼ë¡œ íŒŒì¼ëª… ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ë¹ ë¥¸ ë°©ë²•)
            response = self.session.head(url, timeout=10, allow_redirects=True)
            
            # Content-Disposition í—¤ë”ì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ
            content_disposition = response.headers.get('Content-Disposition', '')
            if content_disposition:
                # filename="íŒŒì¼ëª….í™•ì¥ì" íŒ¨í„´ ì°¾ê¸°
                filename_match = re.search(r'filename[*]?=(?:["\']?)([^"\';\r\n]+)', content_disposition)
                if filename_match:
                    filename = filename_match.group(1).strip()
                    # URL ë””ì½”ë”© ì²˜ë¦¬
                    try:
                        from urllib.parse import unquote
                        filename = unquote(filename, encoding='utf-8')
                    except:
                        pass
                    logger.debug(f"ğŸ“„ íŒŒì¼ëª… ì¶”ì¶œ ì„±ê³µ (í—¤ë”): {filename} from {url}")
                    return filename
            
            # Content-Typeì—ì„œ íŒŒì¼ í˜•ì‹ ì¶”ì •
            content_type = response.headers.get('Content-Type', '')
            if content_type:
                # MIME íƒ€ì…ì—ì„œ í™•ì¥ì ì¶”ì •
                mime_to_ext = {
                    'application/pdf': '.pdf',
                    'application/msword': '.doc',
                    'application/vnd.openxmlformats-officedocument.wordprocessingml.document': '.docx',
                    'application/vnd.hancom.hwp': '.hwp',
                    'text/plain': '.txt',
                    'application/vnd.ms-excel': '.xls',
                    'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet': '.xlsx'
                }
                
                for mime_type, ext in mime_to_ext.items():
                    if mime_type in content_type:
                        # URLì—ì„œ ID ì¶”ì¶œí•˜ì—¬ íŒŒì¼ëª… ìƒì„±
                        url_parts = url.split('/')
                        if len(url_parts) >= 2:
                            doc_id = url_parts[-2]  # download.do ì•ì˜ ID
                            filename = f"ë¬¸ì„œ_{doc_id}{ext}"
                            logger.debug(f"ğŸ“„ íŒŒì¼ëª… ì¶”ì¶œ ì„±ê³µ (MIME): {filename} from {url}")
                            return filename
            
            # ì‹¤ì œ GET ìš”ì²­ìœ¼ë¡œ ë‚´ìš© í™•ì¸ (ìµœí›„ ìˆ˜ë‹¨)
            try:
                response = self.session.get(url, timeout=10, stream=True)
                # ì‘ë‹µ í—¤ë” ì¬í™•ì¸
                content_disposition = response.headers.get('Content-Disposition', '')
                if content_disposition:
                    filename_match = re.search(r'filename[*]?=(?:["\']?)([^"\';\r\n]+)', content_disposition)
                    if filename_match:
                        filename = filename_match.group(1).strip()
                        try:
                            from urllib.parse import unquote
                            filename = unquote(filename, encoding='utf-8')
                        except:
                            pass
                        logger.debug(f"ğŸ“„ íŒŒì¼ëª… ì¶”ì¶œ ì„±ê³µ (GET í—¤ë”): {filename} from {url}")
                        return filename
                
                # ì‘ë‹µ ë‚´ìš©ì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ ì‹œë„ (HTML í˜ì´ì§€ì¸ ê²½ìš°)
                if 'text/html' in response.headers.get('Content-Type', ''):
                    # ì¼ë¶€ ë‚´ìš©ë§Œ ì½ì–´ì„œ íŒŒì‹±
                    content = response.content[:4096].decode('utf-8', errors='ignore')
                    soup = BeautifulSoup(content, 'html.parser')
                    
                    # í˜ì´ì§€ ì œëª©ì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ
                    title_tag = soup.find('title')
                    if title_tag and title_tag.string:
                        title = title_tag.string.strip()
                        return title
                    
                    # íŒŒì¼ ì •ë³´ê°€ ìˆëŠ” ìš”ì†Œ ì°¾ê¸°
                    file_info_selectors = [
                        '.file-name', '.filename', '.document-title', 
                        '[class*="file"]', '[class*="attach"]'
                    ]
                    
                    for selector in file_info_selectors:
                        elements = soup.select(selector)
                        for element in elements:
                            text = element.get_text().strip()
                            if text and len(text) < 100:  # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì œì™¸
                                logger.debug(f"ğŸ“„ íŒŒì¼ëª… ì¶”ì¶œ ì„±ê³µ (ìš”ì†Œ): {text} from {url}")
                                return text
                
            except Exception as e:
                logger.debug(f"GET ìš”ì²­ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # ëª¨ë“  ë°©ë²•ì´ ì‹¤íŒ¨í•œ ê²½ìš° URLì—ì„œ ID ì¶”ì¶œ
            url_parts = url.split('/')
            if len(url_parts) >= 2:
                doc_id = url_parts[-2]  # download.do ì•ì˜ ID
                filename = f"ë¬¸ì„œ_{doc_id}"
                logger.debug(f"ğŸ“„ íŒŒì¼ëª… ì¶”ì¶œ (ê¸°ë³¸ê°’): {filename} from {url}")
                return filename
            
            return "ë‹¤ìš´ë¡œë“œ_íŒŒì¼"
            
        except Exception as e:
            logger.debug(f"íŒŒì¼ëª… ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            # URLì—ì„œ ID ì¶”ì¶œí•˜ì—¬ ê¸°ë³¸ íŒŒì¼ëª… ìƒì„±
            try:
                url_parts = url.split('/')
                if len(url_parts) >= 2:
                    doc_id = url_parts[-2]
                    return f"ë¬¸ì„œ_{doc_id}"
            except:
                pass
            return "ë‹¤ìš´ë¡œë“œ_íŒŒì¼"

    def fetch_page(self, url: str, max_retries: int = 1) -> Tuple[bool, Any, str]:
        """ì„¤ì •ì— ë”°ë¼ requests ë˜ëŠ” seleniumì„ ì‚¬ìš©í•˜ì—¬ í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜´."""
        url = self.normalize_url(url)
        
        # JavaScript:ë¡œ ì‹œì‘í•˜ëŠ” URL ì²˜ë¦¬
        if url.startswith('javascript:'):
            logger.debug(f"JavaScript URL ê°ì§€: {url}")
            return False, None, url
        
        # ì¬ì‹œë„ íšŸìˆ˜ ì„¤ì •
        retries = 0
        
        while retries <= max_retries:
            try:
                # ë¨¼ì € requests ì‚¬ìš© ì‹œë„ (í™œì„±í™”ëœ ê²½ìš°)
                if self.use_requests:
                    try:
                        # User-Agent ëœë¤í™” (ìºì‹± ìš°íšŒ ë° ì°¨ë‹¨ ë°©ì§€)
                        if retries > 0:
                            self.session.headers.update({"User-Agent": random.choice(USER_AGENTS)})
                            
                        response = self.session.get(url, timeout=self.timeout)
                        
                        # ì—¬ê¸°ê°€ ìˆ˜ì •ëœ ë¶€ë¶„ - raise_for_status() í˜¸ì¶œ ì „ì— 500 ì—ëŸ¬ í™•ì¸
                        if 500 <= response.status_code < 600:  # ëª¨ë“  5xx ì—ëŸ¬ ì²˜ë¦¬
                            logger.warning(f"ì„œë²„ ì˜¤ë¥˜ ê°ì§€, ê±´ë„ˆëœë‹ˆë‹¤: {url}, ìƒíƒœ ì½”ë“œ: {response.status_code}")
                            return False, None, url
                            
                        response.raise_for_status()
                        
                        # ìë°”ìŠ¤í¬ë¦½íŠ¸ê°€ ë§ì€ í˜ì´ì§€ì¸ì§€ í™•ì¸
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        # í˜ì´ì§€ ì œëª©ì´ ì˜¤ë¥˜ ë©”ì‹œì§€ì¸ì§€ í™•ì¸
                        if self.check_page_title(response.text):
                            logger.warning(f"ì˜¤ë¥˜ í˜ì´ì§€ ì œëª© ê°ì§€, ê±´ë„ˆëœë‹ˆë‹¤: {url}")
                            return False, None, url
                        
                        # í˜ì´ì§€ê°€ ìë°”ìŠ¤í¬ë¦½íŠ¸ë¥¼ í•„ìš”ë¡œ í•˜ëŠ” ê²ƒ ê°™ìœ¼ë©´ Seleniumìœ¼ë¡œ ì „í™˜
                        noscript_content = soup.find('noscript')
                        js_required = noscript_content and len(noscript_content.text) > 1000
                        
                        if not js_required:
                            return True, soup, response.url
                            
                    except Timeout:
                        logger.warning(f"ìš”ì²­ íƒ€ì„ì•„ì›ƒ({retries+1}/{max_retries+1}): {url}")
                        retries += 1
                        if retries <= max_retries:
                            time.sleep(self.delay * retries)
                            continue
                        else:
                            logger.error(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ (íƒ€ì„ì•„ì›ƒ): {url}")
                            return False, None, url
                            
                    except ConnectionError:
                        logger.warning(f"ì—°ê²° ì˜¤ë¥˜({retries+1}/{max_retries+1}): {url}")
                        retries += 1
                        if retries <= max_retries:
                            time.sleep(self.delay * retries * 2)  # ì—°ê²° ì˜¤ë¥˜ ì‹œ ë” ê¸´ ëŒ€ê¸°
                            continue
                        else:
                            logger.error(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ (ì—°ê²° ì˜¤ë¥˜): {url}")
                            return False, None, url
                            
                    except HTTPError as e:
                        if e.response.status_code in [404, 403, 401]:
                            logger.warning(f"HTTP ì˜¤ë¥˜ {e.response.status_code}: {url}")
                            return False, None, url
                        else:
                            logger.warning(f"HTTP ì˜¤ë¥˜({retries+1}/{max_retries+1}): {url}, ìƒíƒœ: {e.response.status_code}")
                            retries += 1
                            if retries <= max_retries:
                                time.sleep(self.delay * retries)
                                continue
                            else:
                                logger.error(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ (HTTP ì˜¤ë¥˜): {url}")
                                return False, None, url
                                
                    except RequestException as e:
                        logger.warning(f"ìš”ì²­ ì˜¤ë¥˜({retries+1}/{max_retries+1}): {url}, ì˜¤ë¥˜: {e}")
                        retries += 1
                        if retries <= max_retries:
                            time.sleep(self.delay * retries)
                            continue
                        else:
                            logger.error(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ (ìš”ì²­ ì˜¤ë¥˜): {url}")
                            return False, None, url
                
                # requestsê°€ ì‹¤íŒ¨í•˜ê±°ë‚˜ ìë°”ìŠ¤í¬ë¦½íŠ¸ê°€ í•„ìš”í•œ ê²½ìš° Selenium ì‚¬ìš©    
                try:
                    self._init_selenium()
                    
                    # ìµœëŒ€ 2ë²ˆ ì¬ì‹œë„ 
                    for attempt in range(2):
                        try:
                            self.driver.get(url)
                            
                            # ë¡œë”© í›„ í˜ì´ì§€ì˜ ìƒíƒœë¥¼ í™•ì¸
                            if "500 error" in self.driver.page_source.lower() or "500 ì—ëŸ¬" in self.driver.page_source.lower():
                                logger.warning(f"500 ì—ëŸ¬ í˜ì´ì§€ ê°ì§€(Selenium), ê±´ë„ˆëœë‹ˆë‹¤: {url}")
                                return False, None, url
                            
                            # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
                            WebDriverWait(self.driver, self.timeout).until(
                                EC.presence_of_element_located((By.TAG_NAME, 'body'))
                            )
                            
                            html_content = self.driver.page_source
                            current_url = self.driver.current_url
                            
                            # í˜ì´ì§€ ì œëª©ì´ ì˜¤ë¥˜ ë©”ì‹œì§€ì¸ì§€ í™•ì¸
                            if self.check_page_title(html_content):
                                logger.warning(f"ì˜¤ë¥˜ í˜ì´ì§€ ì œëª© ê°ì§€, ê±´ë„ˆëœë‹ˆë‹¤: {url}")
                                return False, None, url
                            
                            soup = BeautifulSoup(html_content, 'html.parser')
                            return True, soup, current_url
                            
                        except TimeoutException:
                            if attempt < 1:  # ì²« ë²ˆì§¸ ì‹œë„ê°€ ì‹¤íŒ¨í•œ ê²½ìš°ì—ë§Œ ì¬ì‹œë„
                                logger.warning(f"í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ, ì¬ì‹œë„ ì¤‘: {url}")
                                continue
                            raise
                            
                except Exception as e:
                    logger.warning(f"Selenium ì²˜ë¦¬ ì˜¤ë¥˜({retries+1}/{max_retries+1}): {url}, ì˜¤ë¥˜: {e}")
                    retries += 1
                    if retries <= max_retries:
                        time.sleep(self.delay * retries)
                        continue
                    else:
                        logger.error(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ (Selenium ì˜¤ë¥˜): {url}")
                        return False, None, url
            
            except Exception as e:
                logger.warning(f"ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜({retries+1}/{max_retries+1}): {url}, ì˜¤ë¥˜: {e}")
                retries += 1
                if retries <= max_retries:
                    time.sleep(self.delay * retries)
                    continue
                else:
                    logger.error(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ (ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜): {url}")
                    return False, None, url
        
        # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ ì‹œ
        logger.error(f"ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨: {url}")
        return False, None, url

    def discover_urls_dfs(self, start_url: str, scope_patterns: Optional[List[str]] = None) -> Dict[str, Any]:
        """DFS ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ì •ì˜ëœ ë²”ìœ„ ë‚´ì˜ ëª¨ë“  URLì„ ë°œê²¬í•¨.
        
        Args:
            start_url: í¬ë¡¤ë§ ì‹œì‘ URL
            scope_patterns: í¬ë¡¤ë§ ë²”ìœ„ë¥¼ ì œí•œí•˜ëŠ” íŒ¨í„´ ëª©ë¡
            
        Returns:
            ë°œê²¬ëœ URLê³¼ íŠ¸ë¦¬ êµ¬ì¡°ê°€ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        
        # ì´ˆê¸°í™”
        start_url = self.normalize_url(start_url)
        parsed_url = urlparse(start_url)
        self.base_domain = parsed_url.netloc
        self.start_url = start_url  # ğŸ  ë©”ì¸ í˜ì´ì§€ ì‹ë³„ì„ ìœ„í•œ start_url ì €ì¥
        
        # ë²”ìœ„ íŒ¨í„´ ì„¤ì •
        if scope_patterns:
            self.scope_patterns = [p.lower() for p in scope_patterns]
            logger.debug(f"ìˆ˜ë™ ì„¤ì •ëœ ë²”ìœ„ íŒ¨í„´: {self.scope_patterns}")
        else:
            self.scope_patterns = self._auto_extract_scope_patterns(start_url)
            logger.debug(f"ìë™ ì¶”ì¶œëœ ë²”ìœ„ íŒ¨í„´: {self.scope_patterns}")
        
        # ë²”ìœ„ ì ìš© ê²°ê³¼ ë¡œê¹… (DEBUG ë ˆë²¨ë¡œ ë³€ê²½)
        if self.scope_patterns == ['']:
            logger.debug(f"í¬ë¡¤ë§ ë²”ìœ„: {self.base_domain} ë„ë©”ì¸ ì „ì²´")
        else:
            logger.debug(f"í¬ë¡¤ë§ ë²”ìœ„: {self.base_domain} ë„ë©”ì¸ì—ì„œ {self.scope_patterns} íŒ¨í„´ í¬í•¨ í˜ì´ì§€ë§Œ")
        
        # ì»¬ë ‰ì…˜ ì´ˆê¸°í™”
        self.all_page_urls.clear()
        self.all_doc_urls.clear()
        self.visited_urls.clear()
        self.url_to_node.clear()
        self.visit_order.clear()
        
        # ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ DFSë¥¼ ìœ„í•œ ì´ˆê¸°í™”
        self.global_navigation_map.clear()
        self.page_contexts.clear()
        self.used_breadcrumbs.clear()
        
        # ë£¨íŠ¸ ë…¸ë“œ ìƒì„±
        self.url_tree = URLTreeNode(start_url, None, 0)
        self.url_to_node[start_url] = self.url_tree
        
        # íŒŒì¼ ì €ì¥ ì„¤ì •
        timestamp = datetime.now().strftime('%Y%m%d_%H%M')
        scope_name = '_'.join(self.scope_patterns) if self.scope_patterns else 'full_domain'
        domain_name = self.base_domain.replace('.', '_')
        domain_dir = os.path.join(BASE_DIR, f"{timestamp}_{domain_name}_{scope_name}_dfs")
        os.makedirs(domain_dir, exist_ok=True)
        
        # ë¡œê¹… ì„¤ì •
        log_file = os.path.join(domain_dir, f"crawling_log_dfs_{timestamp}.txt")
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
        logger.addHandler(file_handler)
        
        logger.debug(f"DFS í¬ë¡¤ë§ ì‹œì‘: {start_url}")
        logger.debug(f"ê¸°ë³¸ ë„ë©”ì¸: {self.base_domain}")
        logger.debug(f"ë²”ìœ„ íŒ¨í„´: {self.scope_patterns}")
        logger.debug(f"ìµœëŒ€ í˜ì´ì§€: {self.max_pages}, ìµœëŒ€ ê¹Šì´: {self.max_depth}")
        
        try:
            # DFS ì‹¤í–‰
            self._dfs_crawl(self.url_tree)
            
            # íŠ¸ë¦¬ êµ¬ì¡° ì €ì¥
            tree_file = os.path.join(domain_dir, f"url_tree_{timestamp}.json")
            self._save_tree_structure(tree_file)
            
            # ë¬¸ì„œ URL ë³„ë„ íŒŒì¼ë¡œ ì €ì¥
            doc_urls_file = os.path.join(domain_dir, f"document_urls_{timestamp}.txt")
            self._save_document_urls(doc_urls_file)
            
            # í†µê³„ ìƒì„±
            stats = self._generate_tree_statistics()
            
            # ê²°ê³¼ JSON ì €ì¥ (í•„ìˆ˜ ì •ë³´ë§Œ í¬í•¨)
            json_data = {
                "timestamp": timestamp,
                "execution_time_seconds": time.time() - start_time,
                "base_url": start_url,
                "base_domain": self.base_domain,
                "scope_patterns": self.scope_patterns,
                "total_pages_discovered": len(self.all_page_urls),
                "total_documents_discovered": len(self.all_doc_urls),
                "max_depth": stats["max_depth"],
                "tree_statistics": {
                    "total_nodes": stats["total_nodes"],
                    "nodes_per_depth": stats["nodes_per_depth"],
                    "document_nodes": stats["document_nodes"],
                    "page_nodes": stats["page_nodes"],
                    "page_types": stats["page_types"],
                    "avg_load_time": stats["avg_load_time"]
                },
                "tree_file": tree_file,
                "doc_urls_file": doc_urls_file
            }
            
            json_file = os.path.join(domain_dir, f"dfs_results_{timestamp}.json")
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"\nâœ… DFS í¬ë¡¤ë§ ì™„ë£Œ:")
            logger.info(f"ğŸ“Š ë°œê²¬ëœ í˜ì´ì§€: {len(self.all_page_urls)}ê°œ")
            logger.info(f"ğŸ“„ ë°œê²¬ëœ ë¬¸ì„œ: {len(self.all_doc_urls)}ê°œ")
            logger.info(f"ğŸ“ ìµœëŒ€ ê¹Šì´: {stats['max_depth']}")
            
            # ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„ ë³´ê³ ì„œ ì¶œë ¥
            if stats['total_nodes'] > 0:
                logger.info(f"\n{self.generate_structure_report()}")
            
            # íŠ¸ë¦¬ êµ¬ì¡° ì‹œê°í™” ìƒì„± (ë‹¨ì¼ ê·¸ë˜í”„ë§Œ)
            visualization_file = ""
            if SimpleTreeVisualizer and self.url_tree and stats['total_nodes'] > 0:
                try:
                    logger.info("ğŸ¨ ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ê·¸ë˜í”„ ìƒì„± ì¤‘...")
                    visualizer = SimpleTreeVisualizer()
                    
                    # ë‹¨ì¼ ê·¸ë˜í”„ íŒŒì¼ ìƒì„±
                    visualization_file = visualizer.generate_single_graph(
                        self.url_tree, 
                        domain_dir, 
                        f"website_structure_{timestamp}"
                    )
                    
                    if visualization_file:
                        logger.info(f"âœ… ê·¸ë˜í”„ ì‹œê°í™” ìƒì„± ì™„ë£Œ:")
                        logger.info(f"   ğŸ“Š ê·¸ë˜í”„: {os.path.basename(visualization_file)}")
                        logger.info(f"ğŸ’¡ '{os.path.basename(visualization_file)}' íŒŒì¼ì„ ì—´ì–´ íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”!")
                    
                except Exception as e:
                    logger.error(f"ì‹œê°í™” ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    visualization_file = ""
            else:
                if not SimpleTreeVisualizer:
                    logger.warning("SimpleTreeVisualizerë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                elif not self.url_tree:
                    logger.warning("íŠ¸ë¦¬ êµ¬ì¡°ê°€ ì—†ì–´ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                elif stats['total_nodes'] == 0:
                    logger.warning("ë…¸ë“œê°€ ì—†ì–´ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            
            # íŒŒì¼ ê°œìˆ˜ ë° ì™„ë£Œ ë©”ì‹œì§€ ì¶œë ¥ (ì‹œê°í™” ì™„ë£Œ í›„)
            total_files = 4 + (1 if visualization_file else 0)
            logger.info(f"ğŸ—‚ï¸ ìƒì„±ëœ íŒŒì¼: {total_files}ê°œ")
            logger.info(f"   ğŸ“‹ ë¬¸ì„œ URL ëª©ë¡: {os.path.basename(doc_urls_file)}")
            logger.info(f"âš¡ ì‹¤í–‰ ì‹œê°„: {time.time() - start_time:.1f}ì´ˆ")
            
            # íŒŒì¼ ìƒì„± ëª©ë¡ ì—…ë°ì´íŠ¸
            files_generated = ["url_tree", "dfs_results", "crawling_log", "document_urls"]
            if visualization_file:
                files_generated.append("graph_visualization")
            
            return {
                "json_data": json_data,
                "page_urls": list(self.all_page_urls),
                "doc_urls": list(self.all_doc_urls),
                "tree_structure": self.url_tree.to_dict(),
                "tree_statistics": stats,
                "results_dir": domain_dir,
                "tree_file": tree_file,
                "doc_urls_file": doc_urls_file,
                "visualization_file": visualization_file,
                "execution_time": time.time() - start_time,
                "files_generated": files_generated
            }
            
        except Exception as e:
            logger.error(f"DFS í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return {
                "error": str(e),
                "execution_time": time.time() - start_time
            }
        
        finally:
            # ë¡œê·¸ í•¸ë“¤ëŸ¬ ì •ë¦¬
            if 'file_handler' in locals():
                logger.removeHandler(file_handler)
                file_handler.close()
            self.close_driver()
    
    def _dfs_crawl(self, current_node: URLTreeNode) -> None:
        """DFSë¥¼ ì‚¬ìš©í•˜ì—¬ ì¬ê·€ì ìœ¼ë¡œ URL í¬ë¡¤ë§"""
        # ë°©ë¬¸ í•œë„ ì²´í¬
        if len(self.visited_urls) >= self.max_pages:
            return
        
        # ê¹Šì´ ì œí•œ ì²´í¬
        if current_node.depth >= self.max_depth:
            logger.debug(f"ìµœëŒ€ ê¹Šì´ ë„ë‹¬: {current_node.url} (ê¹Šì´: {current_node.depth})")
            return
        
        url = current_node.url
        
        # ì´ë¯¸ ë°©ë¬¸í•œ URL ê±´ë„ˆë›°ê¸°
        if url in self.visited_urls:
            return
        
        # ë²”ìœ„ ì²´í¬
        if not self.is_in_scope(url) or self.should_exclude_url(url):
            return
        
        # ë°©ë¬¸ í‘œì‹œ
        self.visited_urls.add(url)
        self.visit_order.append(url)
        current_node.visited_at = datetime.now()
        
        progress = f"[{len(self.visited_urls)}/{self.max_pages}] ê¹Šì´ {current_node.depth}"
        logger.info(f"{progress} ë°©ë¬¸: {url}")
        
        try:
            # í˜ì´ì§€ ë¡œë”© ì‹œê°„ ì¸¡ì • ì‹œì‘
            start_time = time.time()
            
            # í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            success, content, current_url = self.fetch_page(url)
            
            # ë¡œë”© ì‹œê°„ ê¸°ë¡
            current_node.load_time = time.time() - start_time
            
            if not success:
                logger.warning(f"í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {url}")
                
                # ë¬¸ì„œ íŒŒì¼ì´ ì•„ë‹Œ ê²½ìš° íŠ¸ë¦¬ì—ì„œ ì œê±°
                if not current_node.is_document and not self.is_valid_file_url(url, url):
                    self._remove_failed_node(current_node)
                    logger.debug(f"ì ‘ê·¼ ì‹¤íŒ¨í•œ ì¼ë°˜ í˜ì´ì§€ ë…¸ë“œ ì œê±°: {url}")
                    return
                else:
                    # ë¬¸ì„œ íŒŒì¼ì¸ ê²½ìš° ìœ ì§€í•˜ë˜ ì ì ˆí•œ ì œëª©ê³¼ ê²½ë¡œ ì„¤ì •
                    self._handle_failed_document_node(current_node)
                    logger.debug(f"ì ‘ê·¼ ì‹¤íŒ¨í•œ ë¬¸ì„œ íŒŒì¼ ë…¸ë“œ ìœ ì§€: {url}")
                    return
            
            # URL ì—…ë°ì´íŠ¸ (ë¦¬ë‹¤ì´ë ‰íŠ¸ ë“±ìœ¼ë¡œ ë³€ê²½ëœ ê²½ìš°)
            if current_url != url:
                current_node.url = self.normalize_url(current_url)
                url = current_node.url
            
            # BeautifulSoup íŒŒì‹±
            if isinstance(content, BeautifulSoup):
                soup = content
                # requestsë¡œ ê°€ì ¸ì˜¨ ê²½ìš° responseì—ì„œ í¬ê¸° ì¶”ì •
                # ì‹¤ì œë¡œëŠ” ì´ ì •ë³´ê°€ ì œí•œì ì´ë¯€ë¡œ HTML ê¸¸ì´ë¡œ ì¶”ì •
                html_content = str(soup)
                current_node.file_size = len(html_content.encode('utf-8'))
            else:
                self._init_selenium()
                html_content = self.driver.page_source
                soup = BeautifulSoup(html_content, 'html.parser')
                # íŒŒì¼ í¬ê¸° ì¸¡ì •
                current_node.file_size = len(html_content.encode('utf-8'))
            
            # í˜ì´ì§€ ì œëª© ì¶”ì¶œ - ê²Œì‹œê¸€ ë³´ê¸° í˜ì´ì§€ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
            article_title = self._extract_article_title(soup, current_node.url)
            if article_title:
                current_node.page_title = article_title
            else:
                # ì¼ë°˜ í˜ì´ì§€ëŠ” title íƒœê·¸ ì‚¬ìš©
                title_tag = soup.find('title')
                if title_tag:
                    current_node.page_title = title_tag.get_text().strip()
            
            # ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„ì„ ìœ„í•œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
            current_node.page_type = current_node.classify_page_type(soup, self.start_url)
            
            # ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ë©”ë‰´ ê³„ì¸µ ê²½ë¡œ ì„¤ì •
            if current_node.parent is None:
                # ë£¨íŠ¸ ë…¸ë“œ: page_title ë˜ëŠ” ê¸°ë³¸ê°’ ì„¤ì •
                if current_node.page_title and current_node.page_title != "ì œëª©ì—†ìŒ":
                    clean_title = current_node.page_title.strip()
                    if ' - ' in clean_title:
                        clean_title = clean_title.split(' - ')[0].strip()
                    current_node.breadcrumb = clean_title
                else:
                    current_node.breadcrumb = "í™ˆ"
                # ì „ì—­ ë„¤ë¹„ê²Œì´ì…˜ ë§µ ì´ˆê¸°í™”
                self._update_global_navigation_map(soup, current_node)
                logger.debug(f"ğŸ  Root ê²½ë¡œ ì„¤ì •: {current_node.breadcrumb}")
            else:
                # ê²Œì‹œê¸€ ë³´ê¸° í˜ì´ì§€ì¸ ê²½ìš° ì œëª©ì´ ì´ë¯¸ ì¶”ì¶œë˜ì—ˆìœ¼ë¯€ë¡œ ë°”ë¡œ ì‚¬ìš©
                if self._is_article_view_page(current_node.url) and self._is_valid_article_title(current_node.page_title):
                    # ê²Œì‹œê¸€ì¸ ê²½ìš° ë¶€ëª¨ ê²½ë¡œ + ê²Œì‹œê¸€ ì œëª©
                    if current_node.parent and current_node.parent.breadcrumb:
                        current_node.breadcrumb = f"{current_node.parent.breadcrumb}/{current_node.page_title}"
                    else:
                        current_node.breadcrumb = current_node.page_title
                else:
                    # ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ë©”ë‰´ ê²½ë¡œ ê²°ì •
                    current_node.breadcrumb = self._determine_context_aware_breadcrumb(current_node, soup)
            
            # ë§í¬ ì¶”ì¶œ
            nav_links, content_links, doc_links, menu_info = self.extract_links(soup, url)
            current_node.doc_links = list(doc_links)
            current_node.navigation_links = list(nav_links)
            current_node.content_links = list(content_links)
            current_node.link_count = len(nav_links) + len(content_links) + len(doc_links)
            
            # ë©”ë‰´ êµ¬ì¡° ì •ë³´ íŒŒì‹±
            for menu_item in menu_info:
                if ':' in menu_item:
                    menu_type, menu_url, menu_text = menu_item.split(':', 2)
                    if menu_url == url:
                        current_node.menu_position = menu_type
                        current_node.is_navigation_node = True
                        current_node.navigation_level = current_node.depth  # ì„ì‹œ, ë‚˜ì¤‘ì— ì •í™•íˆ ê³„ì‚°
            
            # í˜„ì¬ URLì´ ë¬¸ì„œì¸ì§€ í™•ì¸
            if self.is_valid_file_url(url, url):
                current_node.is_document = True
                self.all_doc_urls.add(url)
                
                # ğŸ“„ ë‹¤ìš´ë¡œë“œ ë§í¬ì¸ ê²½ìš° íŒŒì¼ëª… ì¶”ì¶œ
                if 'download.do' in url and current_node.page_title in ["ì œëª©ì—†ìŒ", ""]:
                    filename = self.extract_filename_from_download_url(url)
                    current_node.page_title = filename
                    logger.info(f"ğŸ“„ í˜„ì¬ í˜ì´ì§€ íŒŒì¼ëª… ì¶”ì¶œ: {filename} from {url}")
                
                logger.debug(f"{progress} ë¬¸ì„œ í˜ì´ì§€ í™•ì¸: {url}")
            else:
                # ê²Œì‹œíŒ ëª©ë¡ í˜ì´ì§€ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ í˜ì´ì§€ URLë¡œ ì¶”ê°€
                if not self.is_list_page(url):
                    self.all_page_urls.add(url)
            
            # ë¬¸ì„œ URL ì¶”ê°€ (í˜ì´ì§€ì—ì„œ ë°œê²¬ëœ ë¬¸ì„œ ë§í¬ë“¤)
            for doc_url in doc_links:
                normalized_doc_url = self.normalize_url(doc_url)
                if normalized_doc_url and normalized_doc_url not in self.all_doc_urls:
                    self.all_doc_urls.add(normalized_doc_url)
                    logger.debug(f"{progress} ë¬¸ì„œ ë°œê²¬: {normalized_doc_url}")
                    
                    # ë¬¸ì„œ ë…¸ë“œ ìƒì„± (í†µê³„ì— ë°˜ì˜ë˜ë„ë¡)
                    if normalized_doc_url not in self.url_to_node:
                        doc_node = URLTreeNode(normalized_doc_url, current_node, current_node.depth + 1)
                        doc_node.is_document = True
                        doc_node.page_type = "document"
                        doc_node.visited_at = datetime.now()
                        doc_node.file_size = 0  # ì™¸ë¶€ íŒŒì¼ì´ë¯€ë¡œ í¬ê¸° ë¯¸ì¸¡ì •
                        
                        # ğŸ“„ ë‹¤ìš´ë¡œë“œ ë§í¬ì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ
                        if 'download.do' in normalized_doc_url:
                            filename = self.extract_filename_from_download_url(normalized_doc_url)
                            doc_node.page_title = filename
                            logger.info(f"ğŸ“„ íŒŒì¼ëª… ì¶”ì¶œ: {filename} from {normalized_doc_url}")
                        else:
                            doc_node.page_title = "ë‹¤ìš´ë¡œë“œ_íŒŒì¼"
                        
                        self.url_to_node[normalized_doc_url] = doc_node
                        current_node.children.append(doc_node)
                        logger.debug(f"ë¬¸ì„œ ë…¸ë“œ ìƒì„±: {normalized_doc_url} (ë¶€ëª¨: {url})")
            
            # í˜ì´ì§€ë„¤ì´ì…˜ URL ì¶”ì¶œ (Origin ìŠ¤íƒ€ì¼ ê³ ê¸‰ ì²˜ë¦¬)
            pagination_urls = self.handle_pagination(soup, url)
            
            # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ìì‹ ë…¸ë“œ ìƒì„± - ëª¨ë“  ë§í¬ ìˆ˜ì§‘í•˜ë˜ ì¤‘ë³µ ì œê±°
            # 1. ëª¨ë“  ë§í¬ë¥¼ í•©ì³ì„œ ì²˜ë¦¬ (ë„¤ë¹„ê²Œì´ì…˜ + ì½˜í…ì¸  + í˜ì´ì§€ë„¤ì´ì…˜)
            all_child_links = set(nav_links) | set(content_links) | set(pagination_urls)
            
            # í˜ì´ì§€ë„¤ì´ì…˜ URL ë¡œê¹…
            if pagination_urls:
                logger.debug(f"{progress} í˜ì´ì§€ë„¤ì´ì…˜ ë°œê²¬: {len(pagination_urls)}ê°œ ì¶”ê°€ í˜ì´ì§€")
            
            for child_url in all_child_links:
                normalized_child = self.normalize_url(child_url)
                
                if (normalized_child and 
                    normalized_child not in self.visited_urls and 
                    normalized_child not in self.url_to_node and
                    self.is_in_scope(normalized_child) and 
                    not self.should_exclude_url(normalized_child)):
                    
                    # ìì‹ ë…¸ë“œ ìƒì„±
                    child_node = current_node.add_child(normalized_child)
                    
                    # ë§í¬ ìœ í˜•ì— ë”°ë¥¸ ì†ì„± ì„¤ì •
                    if child_url in nav_links:
                        child_node.is_navigation_node = True
                        child_node.navigation_level = current_node.navigation_level + 1
                        child_node.logical_parent = current_node
                        current_node.logical_children.append(child_node)
                        logger.debug(f"ğŸ§­ ë„¤ë¹„ê²Œì´ì…˜ ìì‹ ë…¸ë“œ ì¶”ê°€: {normalized_child} (ë¶€ëª¨: {url})")
                    elif child_url in pagination_urls:
                        child_node.page_type = "general"  # í˜ì´ì§€ë„¤ì´ì…˜ë„ ì¼ë°˜ í˜ì´ì§€ë¡œ ë¶„ë¥˜
                        child_node.logical_parent = current_node
                        logger.debug(f"ğŸ“– í˜ì´ì§€ë„¤ì´ì…˜ ìì‹ ë…¸ë“œ ì¶”ê°€: {normalized_child} (ë¶€ëª¨: {url})")
                    else:
                        logger.debug(f"ğŸ“„ ì½˜í…ì¸  ìì‹ ë…¸ë“œ ì¶”ê°€: {normalized_child} (ë¶€ëª¨: {url})")
                    
                    self.url_to_node[normalized_child] = child_node
                
                # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë…¸ë“œì— ëŒ€í•œ ê´€ê³„ ì„¤ì •
                elif normalized_child in self.url_to_node:
                    existing_node = self.url_to_node[normalized_child]
                    
                    # ë„¤ë¹„ê²Œì´ì…˜ ë§í¬ì¸ ê²½ìš° ë…¼ë¦¬ì  ê´€ê³„ ì„¤ì •
                    if child_url in nav_links:
                        if not existing_node.logical_parent:
                            existing_node.logical_parent = current_node
                            current_node.logical_children.append(existing_node)
                        

                        logger.debug(f"ğŸ”— ë„¤ë¹„ê²Œì´ì…˜ ë…¼ë¦¬ì  ê´€ê³„ ì„¤ì •: {normalized_child} â† {url}")
                    

                    


            # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ DFS ì¬ê·€ í˜¸ì¶œ
            # 1. ë„¤ë¹„ê²Œì´ì…˜ ìì‹ë“¤ ë¨¼ì € íƒìƒ‰ (ìµœìš°ì„ )
            for child in current_node.children:
                if child.is_navigation_node and child.url not in self.visited_urls:
                    time.sleep(self.delay)
                    self._dfs_crawl(child)
                    
                    if len(self.visited_urls) >= self.max_pages:
                        break
            
            # 2. ì¼ë°˜ ì½˜í…ì¸  ìì‹ë“¤ íƒìƒ‰ (ë‚˜ë¨¸ì§€ ëª¨ë“  ìì‹ë“¤)
            for child in current_node.children:
                if (not child.is_navigation_node and 
                    child.url not in self.visited_urls):
                    time.sleep(self.delay)
                    self._dfs_crawl(child)
                    
                    if len(self.visited_urls) >= self.max_pages:
                        break
            
        except Exception as e:
            logger.error(f"URL ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {url}, {e}")
            current_node.error_status = f"error: {str(e)[:100]}"
            
            # ë¬¸ì„œ íŒŒì¼ì´ ì•„ë‹Œ ê²½ìš° ì˜¤ë¥˜ ë…¸ë“œë„ ì œê±°
            if not current_node.is_document and not self.is_valid_file_url(url, url):
                self._remove_failed_node(current_node)
                logger.debug(f"ì˜¤ë¥˜ ë°œìƒí•œ ì¼ë°˜ í˜ì´ì§€ ë…¸ë“œ ì œê±°: {url}")
            else:
                # ë¬¸ì„œ íŒŒì¼ì¸ ê²½ìš° ì˜¤ë¥˜ ìƒíƒœë¡œ ìœ ì§€
                self._handle_failed_document_node(current_node)
                logger.debug(f"ì˜¤ë¥˜ ë°œìƒí•œ ë¬¸ì„œ íŒŒì¼ ë…¸ë“œ ìœ ì§€: {url}")
    
    def _remove_failed_node(self, failed_node: URLTreeNode) -> None:
        """ì ‘ê·¼ì— ì‹¤íŒ¨í•œ ë…¸ë“œë¥¼ íŠ¸ë¦¬ì—ì„œ ì œê±°"""
        try:
            # ë¶€ëª¨ ë…¸ë“œì˜ children ë¦¬ìŠ¤íŠ¸ì—ì„œ ì œê±°
            if failed_node.parent and failed_node in failed_node.parent.children:
                failed_node.parent.children.remove(failed_node)
                logger.debug(f"ë¶€ëª¨ ë…¸ë“œì—ì„œ ì‹¤íŒ¨ ë…¸ë“œ ì œê±°: {failed_node.url}")
            
            # url_to_node ë§¤í•‘ì—ì„œ ì œê±°
            if failed_node.url in self.url_to_node:
                del self.url_to_node[failed_node.url]
                logger.debug(f"URL ë§¤í•‘ì—ì„œ ì‹¤íŒ¨ ë…¸ë“œ ì œê±°: {failed_node.url}")
            
            # visited_urlsì—ì„œë„ ì œê±° (ì¬ì‹œë„ ê°€ëŠ¥í•˜ë„ë¡)
            if failed_node.url in self.visited_urls:
                self.visited_urls.remove(failed_node.url)
            
            # visit_orderì—ì„œë„ ì œê±°
            if failed_node.url in self.visit_order:
                self.visit_order.remove(failed_node.url)
                
        except Exception as e:
            logger.error(f"ì‹¤íŒ¨ ë…¸ë“œ ì œê±° ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _handle_failed_document_node(self, doc_node: URLTreeNode) -> None:
        """ì ‘ê·¼ì— ì‹¤íŒ¨í•œ ë¬¸ì„œ ë…¸ë“œë¥¼ ì ì ˆíˆ ì²˜ë¦¬"""
        try:
            # ë¬¸ì„œë¡œ í‘œì‹œ
            doc_node.is_document = True
            doc_node.page_type = "document"
            
            # íŒŒì¼ëª… ì¶”ì¶œ ì‹œë„
            if 'download.do' in doc_node.url:
                filename = self.extract_filename_from_download_url(doc_node.url)
                if filename:
                    doc_node.page_title = f"{filename} (ì ‘ê·¼ë¶ˆê°€)"
                else:
                    doc_node.page_title = "ë‹¤ìš´ë¡œë“œ_íŒŒì¼ (ì ‘ê·¼ë¶ˆê°€)"
            else:
                # URLì—ì„œ íŒŒì¼ëª… ì¶”ì •
                from urllib.parse import urlparse
                parsed = urlparse(doc_node.url)
                path_parts = parsed.path.split('/')
                if path_parts:
                    last_part = path_parts[-1]
                    if '.' in last_part:
                        doc_node.page_title = f"{last_part} (ì ‘ê·¼ë¶ˆê°€)"
                    else:
                        doc_node.page_title = "ë¬¸ì„œíŒŒì¼ (ì ‘ê·¼ë¶ˆê°€)"
                else:
                    doc_node.page_title = "ë¬¸ì„œíŒŒì¼ (ì ‘ê·¼ë¶ˆê°€)"
            
            # ë¶€ëª¨ ê¸°ë°˜ ë©”ë‰´ ê²½ë¡œ ì„¤ì •
            if doc_node.parent and doc_node.parent.breadcrumb not in ["unknown", "Root URL"]:
                doc_node.breadcrumb = f"{doc_node.parent.breadcrumb}/ì²¨ë¶€íŒŒì¼"
            else:
                doc_node.breadcrumb = "ì²¨ë¶€íŒŒì¼"
            
            # ë¬¸ì„œ URL ëª©ë¡ì— ì¶”ê°€
            self.all_doc_urls.add(doc_node.url)
            
            logger.debug(f"ë¬¸ì„œ ë…¸ë“œ ì²˜ë¦¬ ì™„ë£Œ: {doc_node.page_title} - {doc_node.breadcrumb}")
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ë…¸ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            # ê¸°ë³¸ê°’ ì„¤ì •
            doc_node.page_title = "ë¬¸ì„œíŒŒì¼ (ì ‘ê·¼ë¶ˆê°€)"
            doc_node.breadcrumb = "ì²¨ë¶€íŒŒì¼"
    
    def _save_tree_structure(self, tree_file: str) -> None:
        """íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            tree_data = self.url_tree.to_dict() if self.url_tree else {}
            with open(tree_file, 'w', encoding='utf-8') as f:
                json.dump(tree_data, f, ensure_ascii=False, indent=2)
            logger.debug(f"íŠ¸ë¦¬ êµ¬ì¡° ì €ì¥ ì™„ë£Œ: {tree_file}")
        except Exception as e:
            logger.error(f"íŠ¸ë¦¬ êµ¬ì¡° ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _save_document_urls(self, doc_urls_file: str) -> None:
        """ë¬¸ì„œ URLë“¤ì„ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            # ë¬¸ì„œ URLì„ ì •ë ¬í•˜ì—¬ ì €ì¥ (ì¼ê´€ì„± ìˆëŠ” ìˆœì„œ)
            sorted_doc_urls = sorted(list(self.all_doc_urls))
            
            with open(doc_urls_file, 'w', encoding='utf-8') as f:
                for doc_url in sorted_doc_urls:
                    f.write(f"{doc_url}\n")
            
            logger.debug(f"ë¬¸ì„œ URL ì €ì¥ ì™„ë£Œ: {doc_urls_file} ({len(sorted_doc_urls)}ê°œ)")
        except Exception as e:
            logger.error(f"ë¬¸ì„œ URL ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _generate_tree_statistics(self) -> Dict[str, Any]:
        """íŠ¸ë¦¬ í†µê³„ ìƒì„± (í•„ìˆ˜ ì •ë³´ë§Œ)"""
        if not self.url_tree:
            return {}
        
        stats = {
            "total_nodes": 0,
            "max_depth": 0,
            "nodes_per_depth": {},
            "document_nodes": 0,
            "page_nodes": 0,
            "page_types": {},  # í˜ì´ì§€ íƒ€ì…ë³„ ë¶„í¬
            "avg_load_time": 0.0,
            "total_file_size": 0
        }
        
        load_times = []
        
        def traverse_tree(node: URLTreeNode, depth: int = 0):
            stats["total_nodes"] += 1
            stats["max_depth"] = max(stats["max_depth"], depth)
            
            # ê¹Šì´ë³„ ë…¸ë“œ ìˆ˜
            if depth not in stats["nodes_per_depth"]:
                stats["nodes_per_depth"][depth] = 0
            stats["nodes_per_depth"][depth] += 1
            
            # ë…¸ë“œ ìœ í˜•ë³„ ì¹´ìš´íŠ¸
            if node.is_document:
                stats["document_nodes"] += 1
            else:
                stats["page_nodes"] += 1
            
            # í˜ì´ì§€ íƒ€ì…ë³„ ë¶„í¬
            if node.page_type:
                if node.page_type not in stats["page_types"]:
                    stats["page_types"][node.page_type] = 0
                stats["page_types"][node.page_type] += 1
            
            # ë¡œë”© ì‹œê°„ ìˆ˜ì§‘
            if node.load_time > 0:
                load_times.append(node.load_time)
            
            # íŒŒì¼ í¬ê¸° ìˆ˜ì§‘
            if node.file_size > 0:
                stats["total_file_size"] += node.file_size
            
            # ìì‹ ë…¸ë“œ ì²˜ë¦¬
            for child in node.children:
                traverse_tree(child, depth + 1)
        
        traverse_tree(self.url_tree)
        
        # í‰ê·  ë¡œë”© ì‹œê°„ ê³„ì‚°
        if load_times:
            stats["avg_load_time"] = sum(load_times) / len(load_times)
        
        return stats

    
    def print_tree_structure(self, max_depth_display: int = 3) -> str:
        """íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì¶œë ¥ (ë””ë²„ê¹…ìš©)"""
        if not self.url_tree:
            return "íŠ¸ë¦¬ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        lines = []
        
        def print_node(node: URLTreeNode, prefix: str = "", is_last: bool = True, depth: int = 0):
            if depth > max_depth_display:
                return
            
            # ë…¸ë“œ í‘œì‹œ (ê°œì„ ëœ ì •ë³´ í¬í•¨)
            connector = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
            
            # ë…¸ë“œ íƒ€ì…ê³¼ ìƒíƒœ í‘œì‹œ
            node_type = "[DOC]" if node.is_document else f"[{node.page_type.upper()}]" if node.page_type else "[PAGE]"
            
            # ì¶”ê°€ ì •ë³´ í‘œì‹œ
            info_parts = []
            if node.page_title:
                info_parts.append(f"ì œëª©: {node.page_title[:30]}...")
            if node.load_time > 0:
                info_parts.append(f"ë¡œë”©: {node.load_time:.1f}s")
            if node.link_count > 0:
                info_parts.append(f"ë§í¬: {node.link_count}ê°œ")


            
            info_str = f" ({', '.join(info_parts)})" if info_parts else ""
            
            lines.append(f"{prefix}{connector}{node_type} {node.url}{info_str}")
            
            # ë©”ë‰´ ê³„ì¸µ ê²½ë¡œ í‘œì‹œ
            if node.breadcrumb != "unknown" and depth <= max_depth_display:
                lines.append(f"{prefix}{'    ' if is_last else 'â”‚   '}ğŸ“ ë©”ë‰´ê²½ë¡œ: {node.breadcrumb}")
            
            # ìì‹ ë…¸ë“œ í‘œì‹œ
            if depth < max_depth_display:
                child_prefix = prefix + ("    " if is_last else "â”‚   ")
                for i, child in enumerate(node.children):
                    is_last_child = (i == len(node.children) - 1)
                    print_node(child, child_prefix, is_last_child, depth + 1)
        
        print_node(self.url_tree)
        return "\n".join(lines)
    
    def generate_structure_report(self) -> str:
        """ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        if not self.url_tree:
            return "íŠ¸ë¦¬ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        stats = self._generate_tree_statistics()
        report_lines = []
        
        # ê¸°ë³¸ ì •ë³´
        report_lines.append("=" * 50)
        report_lines.append("ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„ ë³´ê³ ì„œ")
        report_lines.append("=" * 50)
        report_lines.append("ê¸°ë³¸ í†µê³„:")
        report_lines.append(f"  â€¢ ì´ í˜ì´ì§€ ìˆ˜: {stats['total_nodes']:,}ê°œ")
        report_lines.append(f"  â€¢ ìµœëŒ€ ê¹Šì´: {stats['max_depth']}ë ˆë²¨")
        report_lines.append(f"  â€¢ ë¬¸ì„œ íŒŒì¼: {stats['document_nodes']:,}ê°œ")
        report_lines.append(f"  â€¢ ì¼ë°˜ í˜ì´ì§€: {stats['page_nodes']:,}ê°œ")


        report_lines.append("")
        
        # í˜ì´ì§€ íƒ€ì… ë¶„í¬
        if stats['page_types']:
            report_lines.append("í˜ì´ì§€ íƒ€ì… ë¶„í¬:")
            for page_type, count in sorted(stats['page_types'].items(), key=lambda x: x[1], reverse=True):
                percentage = (count / stats['total_nodes']) * 100
                report_lines.append(f"  â€¢ {page_type}: {count:,}ê°œ ({percentage:.1f}%)")
            report_lines.append("")
        
        # ì„±ëŠ¥ ì§€í‘œ
        report_lines.append("ì„±ëŠ¥ ì§€í‘œ:")
        report_lines.append(f"  â€¢ í‰ê·  ë¡œë”© ì‹œê°„: {stats['avg_load_time']:.2f}ì´ˆ")
        report_lines.append(f"  â€¢ ì´ ë°ì´í„° í¬ê¸°: {stats['total_file_size'] / 1024 / 1024:.1f}MB")
        report_lines.append("")
        

        
        # ê°€ì¥ ë§ì´ ì°¸ì¡°ëœ í˜ì´ì§€ë“¤ (ë¹„í™œì„±í™”ë¨)
        # if stats['most_referenced_pages']:
        #     report_lines.append("ê°€ì¥ ë§ì´ ì°¸ì¡°ëœ í˜ì´ì§€ë“¤:")
        #     for i, (url, count) in enumerate(stats['most_referenced_pages'][:5], 1):
        #         report_lines.append(f"  {i}. {url} ({count}íšŒ ì°¸ì¡°)")
        #     report_lines.append("")
        
        # ê¹Šì´ë³„ ë¶„í¬
        report_lines.append("ê¹Šì´ë³„ ë…¸ë“œ ë¶„í¬:")
        for depth, count in sorted(stats['nodes_per_depth'].items()):
            percentage = (count / stats['total_nodes']) * 100
            bar = "â–ˆ" * min(int(percentage), 30)
            report_lines.append(f"  ê¹Šì´ {depth}: {count:,}ê°œ ({percentage:.1f}%) {bar}")
        report_lines.append("")
        
        # DFS ì‚¬ì´íŠ¸ë§µ ë° ì—°ê´€ê´€ê³„ ë¶„ì„
        if stats.get('total_menu_contexts', 0) > 0:
            report_lines.append("DFS ì‚¬ì´íŠ¸ë§µ ì—°ê´€ê´€ê³„ ë¶„ì„:")
            report_lines.append(f"  â€¢ ì´ ë©”ë‰´ ë§¥ë½: {stats.get('total_menu_contexts', 0):,}ê°œ")
            report_lines.append(f"  â€¢ ìµœëŒ€ ë©”ë‰´ ê¹Šì´: {stats.get('max_menu_depth', 0)}ë ˆë²¨")

            report_lines.append("  â€¢ ê¸´ ê²½ë¡œ ìš°ì„ ìœ¼ë¡œ ë¬¸ì„œ ë¬¸ë§¥ ë³´ì¡´")
            report_lines.append("")
        
        return "\n".join(report_lines)
    
    def generate_derived_files(self, output_dir: str, file_types: list = None) -> Dict[str, str]:
        """í•„ìš”ì‹œ íŒŒìƒ íŒŒì¼ë“¤ì„ ìƒì„± (ì„ íƒì )"""
        if not self.url_tree:
            return {"error": "íŠ¸ë¦¬ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        
        if file_types is None:
            file_types = ['csv', 'report', 'tree_viz']
        
        exports = {}
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        try:
            # CSV í˜•íƒœì˜ í”Œë« ë°ì´í„°
            if 'csv' in file_types:
                csv_file = os.path.join(output_dir, f"pages_data_{timestamp}.csv")
                self._export_to_csv(csv_file)
                exports['csv'] = csv_file
                logger.debug(f"CSV íŒŒì¼ ìƒì„±: {csv_file}")
            
            # ìƒì„¸ ë³´ê³ ì„œ í…ìŠ¤íŠ¸ íŒŒì¼
            if 'report' in file_types:
                report_file = os.path.join(output_dir, f"structure_report_{timestamp}.txt")
                with open(report_file, 'w', encoding='utf-8') as f:
                    f.write(self.generate_structure_report())
                exports['report'] = report_file
                logger.debug(f"êµ¬ì¡° ë¦¬í¬íŠ¸ ìƒì„±: {report_file}")
            
            # íŠ¸ë¦¬ êµ¬ì¡° ì‹œê°í™” í…ìŠ¤íŠ¸
            if 'tree_viz' in file_types:
                tree_file = os.path.join(output_dir, f"tree_structure_{timestamp}.txt")
                with open(tree_file, 'w', encoding='utf-8') as f:
                    f.write(self.print_tree_structure(max_depth_display=5))
                exports['tree_viz'] = tree_file
                logger.debug(f"íŠ¸ë¦¬ ì‹œê°í™” ìƒì„±: {tree_file}")
            
            # í†µê³„ ìš”ì•½ JSON (dfs_results.jsonì— ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆì§€ë§Œ ë³„ë„ íŒŒì¼ì´ í•„ìš”í•œ ê²½ìš°)
            if 'statistics' in file_types:
                stats_file = os.path.join(output_dir, f"statistics_{timestamp}.json")
                stats = self._generate_tree_statistics()
                with open(stats_file, 'w', encoding='utf-8') as f:
                    json.dump(stats, f, ensure_ascii=False, indent=2)
                exports['statistics'] = stats_file
                logger.debug(f"í†µê³„ íŒŒì¼ ìƒì„±: {stats_file}")
            
            logger.debug(f"íŒŒìƒ íŒŒì¼ ìƒì„± ì™„ë£Œ: {len(exports)}ê°œ íŒŒì¼")
            return exports
            
        except Exception as e:
            logger.error(f"íŒŒìƒ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _export_to_csv(self, csv_file: str) -> None:
        """í˜ì´ì§€ ë°ì´í„°ë¥¼ CSV í˜•íƒœë¡œ ë‚´ë³´ë‚´ê¸°"""
        try:
            import csv
            
            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                
                # í—¤ë” ì‘ì„±
                headers = [
                    'URL', 'ê¹Šì´', 'í˜ì´ì§€íƒ€ì…', 'ì œëª©', 'ë„¤ë¹„ê²Œì´ì…˜ê²½ë¡œ', 
                    'ìì‹ìˆ˜', 'ë§í¬ìˆ˜', 'ë¡œë”©ì‹œê°„', 'íŒŒì¼í¬ê¸°', 
                    'ë°©ë¬¸ì‹œê°„', 'ë¬¸ì„œì—¬ë¶€'
                ]
                writer.writerow(headers)
                
                # ë°ì´í„° ì‘ì„±
                def write_node_data(node: URLTreeNode):
                    row = [
                        node.url,
                        node.depth,
                        node.page_type,
                        node.page_title,
                        node.breadcrumb if node.breadcrumb != "unknown" else '',
                        len(node.children),
                        node.link_count,
                        f"{node.load_time:.2f}" if node.load_time > 0 else '',
                        node.file_size,
                        node.visited_at.isoformat() if node.visited_at else '',
                        'Yes' if node.is_document else 'No'
                    ]
                    writer.writerow(row)
                    
                    for child in node.children:
                        write_node_data(child)
                
                if self.url_tree:
                    write_node_data(self.url_tree)
                    
        except Exception as e:
            logger.error(f"CSV ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")



    def _auto_extract_scope_patterns(self, start_url: str) -> List[str]:
        """URL ê²½ë¡œì—ì„œ ì˜ë¯¸ìˆëŠ” ì„¸ê·¸ë¨¼íŠ¸ë“¤ì„ ìë™ ì¶”ì¶œ"""
        try:
            parsed_url = urlparse(start_url)
            path_parts = [part for part in parsed_url.path.split('/') if part]
        
            # ì œì™¸í•  ì¼ë°˜ì ì¸ ìš©ì–´ë“¤ë§Œ ìƒìˆ˜ë¡œ ì •ì˜ (ìµœì†Œí•œ)
            GENERIC_TERMS = [
                'index.do', 'index.html', 'index.htm', 'index.jsp', 'index.php', 'index.asp',
                'main.do', 'main.html', 'main.jsp', 'main.php',
                'home.do', 'home.html', 'home.jsp', 'home.php',
                'list.do', 'list.html', 'view.do', 'view.html',
                'page.do', 'page.html', 'default.do', 'default.html',
                'web', 'www', 'home', 'main', 'index', 'page', 'view', 'list', 'default', 'sites'
            ]
            
            # ìˆœìˆ˜ ê²½ë¡œ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ
            meaningful_segments = []
            
            for part in path_parts:
                # ì†Œë¬¸ì ë³€í™˜
                clean_part = part.lower()
                
                # íŒŒì¼ í™•ì¥ì ì œê±° (.do, .html ë“±)
                clean_part = re.sub(r'\.(do|html|htm|jsp|php|asp|aspx)$', '', clean_part)
                
                # ì¼ë°˜ì ì¸ ìš©ì–´ê°€ ì•„ë‹ˆê³ , ì˜ë¯¸ìˆëŠ” ê¸¸ì´ë©´ ì¶”ê°€
                if (clean_part not in GENERIC_TERMS and 
                    len(clean_part) > 1 and 
                    clean_part.isalnum()):
                    meaningful_segments.append(clean_part)
            
            # ê²°ê³¼ ê²€ì¦
            if not meaningful_segments:
                logger.debug("ì˜ë¯¸ìˆëŠ” ê²½ë¡œ ì„¸ê·¸ë¨¼íŠ¸ê°€ ì—†ì–´ ë„ë©”ì¸ ì „ì²´ë¥¼ ë²”ìœ„ë¡œ ì„¤ì •")
                return ['']
            
            logger.debug(f"ìë™ ì¶”ì¶œëœ ë²”ìœ„ íŒ¨í„´: {meaningful_segments}")
            logger.debug(f"ì›ë³¸ ê²½ë¡œ: {parsed_url.path}")
            logger.debug(f"ì •ì œëœ ì„¸ê·¸ë¨¼íŠ¸: {' â†’ '.join(meaningful_segments)}")
            
            return meaningful_segments
            
        except Exception as e:
            logger.error(f"ìë™ íŒ¨í„´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ['']  # ì‹¤íŒ¨ ì‹œ ì „ì²´ ë„ë©”ì¸


    

    
    def _determine_context_aware_breadcrumb(self, current_node: URLTreeNode, soup: BeautifulSoup) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ë¸Œë ˆë“œí¬ëŸ¼ ê²½ë¡œ ê²°ì • - ë‹¤ì¤‘ ì „ëµ ì‚¬ìš©"""
        try:
            # í˜ì´ì§€ ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
            self._update_page_context(current_node, soup)
            
            # ì „ëµ 1: HTML ë„¤ë¹„ê²Œì´ì…˜ ë¶„ì„ (ìµœìš°ì„ )
            nav_path = current_node.extract_breadcrumb(soup, current_node.url)
            if nav_path != "unknown":
                self._validate_and_store_path(current_node.url, nav_path)
                logger.debug(f"ğŸ¯ ë„¤ë¹„ê²Œì´ì…˜ ê¸°ë°˜ ê²½ë¡œ: {nav_path}")
                return nav_path
            
            # ì „ëµ 2: ì „ì—­ ë„¤ë¹„ê²Œì´ì…˜ ë§µ í™œìš©
            global_path = self._find_path_in_global_map(current_node.url)
            if global_path:
                logger.debug(f"ğŸ—ºï¸ ì „ì—­ ë§µ ê¸°ë°˜ ê²½ë¡œ: {global_path}")
                return global_path
            
            # ì „ëµ 3: ë¶€ëª¨ ê²½ë¡œ ìƒì† + í˜„ì¬ í˜ì´ì§€ ë¶„ì„
            inherited_path = self._build_inherited_breadcrumb(current_node, soup)
            if inherited_path != "unknown":
                logger.debug(f"ğŸ”— ìƒì† ê¸°ë°˜ ê²½ë¡œ: {inherited_path}")
                return inherited_path
            
            # ì „ëµ 4: URL íŒ¨í„´ ë¶„ì„ (ìµœí›„ ìˆ˜ë‹¨)
            pattern_path = self._infer_path_from_url(current_node.url)
            logger.debug(f"ğŸ” íŒ¨í„´ ê¸°ë°˜ ê²½ë¡œ: {pattern_path}")
            return pattern_path
            
        except Exception as e:
            logger.debug(f"ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ê²½ë¡œ ê²°ì • ì‹¤íŒ¨: {e}")
            return "unknown"
    
    def _build_inherited_breadcrumb(self, current_node: URLTreeNode, soup: BeautifulSoup) -> str:
        """ë¶€ëª¨ì˜ breadcrumbì„ ìƒì†ë°›ê³  ìì‹ ì˜ ì œëª©ì„ ì¶”ê°€í•˜ì—¬ ê³„ì¸µì  ê²½ë¡œ êµ¬ì„±"""
        try:
            # ë¶€ëª¨ ë…¸ë“œì˜ ë¸Œë ˆë“œí¬ëŸ¼ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
            parent_path = ""
            if current_node.parent and current_node.parent.breadcrumb != "unknown":
                parent_path = current_node.parent.breadcrumb
            
            # í˜„ì¬ í˜ì´ì§€ì˜ ì œëª© ì¶”ì¶œ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
            current_title = self._extract_current_page_title(current_node, soup)
            
            # ê³„ì¸µì  ê²½ë¡œ êµ¬ì„±
            if parent_path and current_title:
                inherited_path = f"{parent_path}/{current_title}"
                logger.debug(f"ğŸ”— ìƒì†ëœ ë©”ë‰´ ê²½ë¡œ: {inherited_path}")
                return inherited_path
            elif current_title:
                logger.debug(f"ğŸ†• ìƒˆë¡œìš´ ë©”ë‰´ ê²½ë¡œ: {current_title}")
                return current_title
            elif parent_path:
                # ì œëª©ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° URLì—ì„œ ì¶”ì¶œ ì‹œë„
                url_title = self._extract_title_from_url(current_node.url)
                if url_title:
                    inherited_path = f"{parent_path}/{url_title}"
                    logger.debug(f"ğŸ” URL ê¸°ë°˜ ë©”ë‰´ ê²½ë¡œ: {inherited_path}")
                    return inherited_path
                else:
                    logger.debug(f"âš ï¸ ë¶€ëª¨ ê²½ë¡œë§Œ ì‚¬ìš©: {parent_path}")
                    return parent_path
            else:
                # ë§ˆì§€ë§‰ ìˆ˜ë‹¨: URL íŒ¨í„´ ê¸°ë°˜ ì¶”ì •
                fallback_path = self._infer_path_from_url(current_node.url)
                logger.debug(f"ğŸ¯ ëŒ€ì²´ ê²½ë¡œ: {fallback_path}")
                return fallback_path
                
        except Exception as e:
            logger.debug(f"ìƒì†ëœ ë©”ë‰´ ê²½ë¡œ êµ¬ì„± ì‹¤íŒ¨: {e}")
            return "unknown"
    
    def _update_global_navigation_map(self, soup: BeautifulSoup, current_node: URLTreeNode) -> None:
        """ì „ì—­ ë„¤ë¹„ê²Œì´ì…˜ ë§µ ì—…ë°ì´íŠ¸"""
        try:
            # ë„¤ë¹„ê²Œì´ì…˜ êµ¬ì¡° ì¶”ì¶œ
            nav_structure = self._extract_navigation_structure(soup, current_node.url)
            
            # ì „ì—­ ë§µì— ë³‘í•© (ê¸°ì¡´ ì •ë³´ ë³´ì¡´)
            for nav_path, nav_info in nav_structure.items():
                if nav_path not in self.global_navigation_map:
                    self.global_navigation_map[nav_path] = nav_info
                else:
                    # ê¸°ì¡´ ì •ë³´ì™€ ë³‘í•©
                    self.global_navigation_map[nav_path].update(nav_info)
            
            logger.debug(f"ğŸ—ºï¸ ì „ì—­ ë„¤ë¹„ê²Œì´ì…˜ ë§µ ì—…ë°ì´íŠ¸: {len(nav_structure)}ê°œ í•­ëª© ì¶”ê°€")
            
        except Exception as e:
            logger.debug(f"ì „ì—­ ë„¤ë¹„ê²Œì´ì…˜ ë§µ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _update_page_context(self, current_node: URLTreeNode, soup: BeautifulSoup) -> None:
        """í˜ì´ì§€ë³„ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì—…ë°ì´íŠ¸"""
        try:
            page_context = {
                'url': current_node.url,
                'title': current_node.page_title,
                'page_type': current_node.page_type,
                'navigation_links': current_node.navigation_links,
                'parent_path': current_node.parent.breadcrumb if current_node.parent else None,
                'depth': current_node.depth
            }
            
            self.page_contexts[current_node.url] = page_context
            
        except Exception as e:
            logger.debug(f"í˜ì´ì§€ ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _extract_navigation_structure(self, soup: BeautifulSoup, base_url: str) -> Dict[str, Dict[str, Any]]:
        """í˜„ì¬ í˜ì´ì§€ì—ì„œ ë„¤ë¹„ê²Œì´ì…˜ êµ¬ì¡° ì¶”ì¶œ"""
        nav_structure = {}
        
        try:
            # ê¸°ì¡´ _analyze_ul_li_hierarchy í™œìš©
            nav_containers = soup.select([
                'nav', '.nav', '.navigation', '.menu', '.gnb', '.lnb',
                '[class*="menu"]', '[id*="menu"]', '[class*="nav"]', '[id*="nav"]'
            ])
            
            for nav in nav_containers:
                extracted_links, extracted_menu_info = self._analyze_ul_li_hierarchy(nav, base_url, depth=0)
                
                # ë©”ë‰´ ì •ë³´ë¥¼ êµ¬ì¡°í™”
                for menu_item in extracted_menu_info:
                    if ':' in menu_item:
                        parts = menu_item.split(':', 3)
                        if len(parts) >= 3:
                            menu_type, menu_url, menu_path = parts[0], parts[1], parts[2]
                            
                            nav_structure[menu_url] = {
                                'path': menu_path,
                                'type': menu_type,
                                'source_url': base_url
                            }
            
        except Exception as e:
            logger.debug(f"ë„¤ë¹„ê²Œì´ì…˜ êµ¬ì¡° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return nav_structure
    
    def _find_path_in_global_map(self, url: str) -> str:
        """ì „ì—­ ë„¤ë¹„ê²Œì´ì…˜ ë§µì—ì„œ URLì— í•´ë‹¹í•˜ëŠ” ê²½ë¡œ ì°¾ê¸°"""
        try:
            if url in self.global_navigation_map:
                return self.global_navigation_map[url].get('path', '')
            
            # URL ì •ê·œí™”í•˜ì—¬ ë‹¤ì‹œ ì‹œë„
            normalized_url = self.normalize_url(url)
            if normalized_url in self.global_navigation_map:
                return self.global_navigation_map[normalized_url].get('path', '')
            
        except Exception as e:
            logger.debug(f"ì „ì—­ ë§µì—ì„œ ê²½ë¡œ ì°¾ê¸° ì‹¤íŒ¨: {e}")
        
        return ""
    
    def _validate_and_store_path(self, url: str, path: str) -> bool:
        """ë¸Œë ˆë“œí¬ëŸ¼ ê²½ë¡œ ê²€ì¦ ë° ì €ì¥"""
        try:
            # ê¸°ë³¸ ê²€ì¦
            if not path or path == "unknown":
                return False
            
            # ê²½ë¡œ ê¸¸ì´ ê²€ì¦ (ë„ˆë¬´ ê¹Šì§€ ì•Šì€ì§€)
            if len(path.split('/')) > 6:
                logger.debug(f"ê²½ë¡œê°€ ë„ˆë¬´ ê¹ŠìŒ: {path}")
                return False
            
            # ì¤‘ë³µ ê²½ë¡œ ê²€ì¦ (ê°™ì€ ê²½ë¡œê°€ ë‹¤ë¥¸ URLì— ì‚¬ìš©ë˜ì§€ ì•Šì•˜ëŠ”ì§€)
            if path in self.used_breadcrumbs:
                logger.debug(f"ì¤‘ë³µ ê²½ë¡œ ë°œê²¬: {path}")
                # ì¤‘ë³µì´ì–´ë„ í—ˆìš© (ê°™ì€ í˜ì´ì§€ê°€ ì—¬ëŸ¬ ê²½ë¡œë¡œ ì ‘ê·¼ ê°€ëŠ¥)
            
            # ê²½ë¡œ ì €ì¥
            self.used_breadcrumbs.add(path)
            return True
            
        except Exception as e:
            logger.debug(f"ê²½ë¡œ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def _extract_current_page_title(self, current_node: URLTreeNode, soup: BeautifulSoup) -> str:
        """í˜„ì¬ í˜ì´ì§€ì˜ ì œëª©ì„ ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ì¶”ì¶œ"""
        try:
            # ê²Œì‹œê¸€ ë³´ê¸° í˜ì´ì§€ì˜ ê²½ìš° ê²Œì‹œê¸€ ì œëª© ì¶”ì¶œ
            if self._is_article_view_page(current_node.url):
                article_title = self._extract_article_title(soup, current_node.url)
                if article_title:
                    return article_title
            
            # artclList.do í˜ì´ì§€ì˜ ê²½ìš° "ê²Œì‹œíŒ" + í˜ì´ì§€ ë²ˆí˜¸ë¡œ ì„¤ì •
            if 'artclList.do' in current_node.url:
                # URLì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
                from urllib.parse import urlparse, parse_qs
                parsed = urlparse(current_node.url)
                query_params = parse_qs(parsed.query)
                page_num = query_params.get('page', [None])[0]
                
                base_title = "ê²Œì‹œíŒ"
                
                # í˜ì´ì§€ ë²ˆí˜¸ê°€ ìˆìœ¼ë©´ ì¶”ê°€
                if page_num and page_num != '1':  # 1í˜ì´ì§€ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ í‘œì‹œ
                    return f"{base_title} {page_num}page"
                return base_title
            
            # 1. ì´ë¯¸ ì¶”ì¶œëœ í˜ì´ì§€ ì œëª© ì‚¬ìš©
            if current_node.page_title and current_node.page_title != "ì œëª©ì—†ìŒ":
                clean_title = current_node.page_title.strip()
                # ì‚¬ì´íŠ¸ëª… ì œê±°
                if ' - ' in clean_title:
                    clean_title = clean_title.split(' - ')[0].strip()
                if clean_title and len(clean_title) <= 50:  # ë„ˆë¬´ ê¸´ ì œëª© ë°©ì§€
                    return clean_title
            
            # 2. h1, h2 íƒœê·¸ì—ì„œ ì¶”ì¶œ
            if soup:
                for tag in ['h1', 'h2', '.page-title', '.title', '.subject']:
                    element = soup.select_one(tag)
                    if element:
                        text = element.get_text().strip()
                        if text and len(text) <= 50:
                            return text
            
            # 3. URLì—ì„œ ì¶”ì¶œ
            url_title = self._extract_title_from_url(current_node.url)
            if url_title:
                return url_title
            
            return ""
            
        except Exception as e:
            logger.debug(f"í˜„ì¬ í˜ì´ì§€ ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ""
    

    
    def _extract_article_title(self, soup: BeautifulSoup, url: str) -> str:
        """ë²”ìš©ì ì¸ ê²Œì‹œê¸€ ì œëª© ì¶”ì¶œ"""
        try:
            # ê²Œì‹œê¸€ ë³´ê¸° í˜ì´ì§€ê°€ ì•„ë‹Œ ê²½ìš° ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
            if not self._is_article_view_page(url):
                return ""
            
            # íŒ¨í„´ 1: view-title, post-title ë“± ê²Œì‹œê¸€ ì œëª© ì „ìš© í´ë˜ìŠ¤
            title_selectors = [
                'h1.view-title', 'h2.view-title', 'h3.view-title',
                'h1.post-title', 'h2.post-title', 'h3.post-title', 
                'h1.article-title', 'h2.article-title', 'h3.article-title',
                '.view-title h1', '.view-title h2', '.post-title h1', '.post-title h2',
                '.article-title h1', '.article-title h2', '.entry-title',
                'h1.title', 'h2.title', 'h3.title',
                '.title h1', '.title h2', '.title h3'
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem and title_elem.get_text().strip():
                    title = title_elem.get_text().strip()
                    if self._is_valid_article_title(title):
                        return title
            
            # íŒ¨í„´ 2: ì²« ë²ˆì§¸ h1, h2 íƒœê·¸ (ì‚¬ì´íŠ¸ ì œëª©ì´ ì•„ë‹Œ ê²½ìš°)
            for tag in ['h1', 'h2']:
                heading = soup.find(tag)
                if heading and heading.get_text().strip():
                    title = heading.get_text().strip()
                    if self._is_valid_article_title(title):
                        return title
            
            # íŒ¨í„´ 3: title íƒœê·¸ì—ì„œ ì¶”ì¶œ (ì‚¬ì´íŠ¸ëª… ì œê±°)
            if soup.title and soup.title.string:
                title = soup.title.string.strip()
                # ì‚¬ì´íŠ¸ëª… ì œê±° ì‹œë„
                if ' - ' in title:
                    title_parts = title.split(' - ')
                    for part in title_parts:
                        part = part.strip()
                        if self._is_valid_article_title(part):
                            return part
                elif self._is_valid_article_title(title):
                    return title
            
            return ""
            
        except Exception as e:
            logger.debug(f"ê²Œì‹œê¸€ ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ""
    
    def _is_article_view_page(self, url: str) -> bool:
        """ê²Œì‹œê¸€ ë³´ê¸° í˜ì´ì§€ì¸ì§€ íŒë‹¨"""
        try:
            # ì¼ë°˜ì ì¸ ê²Œì‹œê¸€ ë³´ê¸° í˜ì´ì§€ íŒ¨í„´ë“¤
            article_patterns = [
                'artclView.do', 'articleView.do', 'boardView.do',
                'view.do', 'detail.do', 'read.do',
                '/view/', '/detail/', '/read/', '/article/',
                '/post/', '/board/view', '/bbs/view'
            ]
            
            url_lower = url.lower()
            return any(pattern.lower() in url_lower for pattern in article_patterns)
            
        except Exception as e:
            logger.debug(f"ê²Œì‹œê¸€ ë³´ê¸° í˜ì´ì§€ íŒë‹¨ ì‹¤íŒ¨: {e}")
            return False
    
    def _is_valid_article_title(self, title: str) -> bool:
        """ìœ íš¨í•œ ê²Œì‹œê¸€ ì œëª©ì¸ì§€ íŒë‹¨ (ì‚¬ì´íŠ¸ëª…ì´ë‚˜ ì¼ë°˜ì ì¸ ì œëª© ì œì™¸)"""
        try:
            if not title or not title.strip():
                return False
            
            title = title.strip()
            
            # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ì œëª© ì œì™¸
            if len(title) < 2 or len(title) > 200:
                return False
            
            # ì¼ë°˜ì ì¸ ì‚¬ì´íŠ¸ ì œëª©ì´ë‚˜ ê¸°ë³¸ í˜ì´ì§€ ì œëª© ì œì™¸
            invalid_titles = [
                'ì œëª©ì—†ìŒ', 'í˜ì´ì§€', 'í™ˆ', 'ë©”ì¸', 'ë¡œë”©', 'loading',
                'ì—ëŸ¬', 'error', '404', '500', 'ì ‘ê·¼ê¶Œí•œ',
                'ë¡œê·¸ì¸', 'login', 'íšŒì›ê°€ì…', 'join'
            ]
            
            title_lower = title.lower()
            for invalid in invalid_titles:
                if invalid.lower() in title_lower:
                    return False
            
            # ì‚¬ì´íŠ¸ëª…ìœ¼ë¡œ ë³´ì´ëŠ” íŒ¨í„´ ì œì™¸ (ëŒ€í•™êµ, íšŒì‚¬ëª… ë“±)
            site_patterns = ['ëŒ€í•™êµ', 'ëŒ€í•™', 'university', 'íšŒì‚¬', 'company', 'ê¸°ê´€', 'ì¬ë‹¨']
            if any(pattern in title for pattern in site_patterns) and len(title) < 15:
                return False
            
            return True
            
        except Exception as e:
            logger.debug(f"ê²Œì‹œê¸€ ì œëª© ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨: {e}")
            return False
    

    
    def _get_page_type_title(self, current_node: URLTreeNode) -> str:
        """í˜ì´ì§€ íƒ€ì…ì— ë”°ë¥¸ ê¸°ë³¸ ì œëª© ë°˜í™˜"""
        if current_node.is_document:
            return "ë¬¸ì„œ"
        elif current_node.page_type == "board":
            return "ê²Œì‹œíŒ"
        elif current_node.page_type == "main":
            return "ë©”ì¸"
        else:
            return "í˜ì´ì§€"
    
    def _extract_title_from_url(self, url: str) -> str:
        """URLì—ì„œ ì˜ë¯¸ìˆëŠ” ì œëª© ì¶”ì¶œ"""
        try:
            from urllib.parse import urlparse, parse_qs
            parsed = urlparse(url)
            
            # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ì—ì„œ ì •ë³´ ì¶”ì¶œ
            query_params = parse_qs(parsed.query)
            
            # artclView.do ê°™ì€ ê²Œì‹œê¸€ ë³´ê¸° í˜ì´ì§€ì¸ ê²½ìš°
            if 'artclView.do' in url:
                if 'artclNo' in query_params:
                    return f"ê²Œì‹œê¸€_{query_params['artclNo'][0]}"
                return "ê²Œì‹œê¸€"
            
            # artclList.do ê°™ì€ ê²Œì‹œíŒ ëª©ë¡ í˜ì´ì§€ì¸ ê²½ìš°
            if 'artclList.do' in url:
                return "ê²Œì‹œíŒ"
            
            # ë‹¤ìš´ë¡œë“œ íŒŒì¼ì¸ ê²½ìš°
            if any(pattern in url.lower() for pattern in ['download', 'file']):
                return "ì²¨ë¶€íŒŒì¼"
            
            # URL ê²½ë¡œì—ì„œ ì˜ë¯¸ìˆëŠ” ë¶€ë¶„ ì¶”ì¶œ
            path_parts = [p for p in parsed.path.split('/') if p and p not in ['index.do', 'main.do']]
            if path_parts:
                last_part = path_parts[-1]
                # íŒŒì¼ í™•ì¥ì ì œê±°
                clean_part = re.sub(r'\.(do|html|htm|jsp|php)$', '', last_part)
                
                # ì˜ë¯¸ìˆëŠ” ì´ë¦„ìœ¼ë¡œ ë³€í™˜
                segment_mappings = {
                    'artcllist': 'ëª©ë¡',
                    'artclview': 'ìƒì„¸ë³´ê¸°',
                    'professor': 'êµìˆ˜ì§„',
                    'faculty': 'êµìˆ˜ì§„',
                    'curriculum': 'êµìœ¡ê³¼ì •',
                    'research': 'ì—°êµ¬',
                    'notice': 'ê³µì§€ì‚¬í•­',
                    'news': 'ì†Œì‹',
                    'intro': 'ì†Œê°œ',
                    'about': 'ì†Œê°œ',
                    'subview': 'í˜ì´ì§€'
                }
                
                return segment_mappings.get(clean_part.lower(), clean_part)
            
            return ""
            
        except Exception as e:
            logger.debug(f"URL ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ""

def main(start_url, scope=None, max_pages=1000, delay=1.0, timeout=20, use_requests=True, verbose=False, max_depth=10, generate_extra_files=None):
    """ë§¤ê°œë³€ìˆ˜ë¡œ í¬ë¡¤ëŸ¬ ì‹¤í–‰ (DFS ë°©ì‹)"""
    if verbose:
        logger.setLevel(logging.INFO)
    else:
        logger.setLevel(logging.WARNING)
    
    # ì‹œì‘ ì‹œê°„ ê¸°ë¡
    start_time = time.time()
    
    # URL ì •ê·œí™”
    if not start_url.startswith(('http://', 'https://')):
        start_url = 'https://' + start_url
    
    logger.info(f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘: {start_url}")
    
    if generate_extra_files:
        logger.debug(f"ğŸ“Š ì¶”ê°€ íŒŒì¼ ìƒì„± ì˜ˆì •: {generate_extra_files}")
    
    with ScopeLimitedCrawler(max_pages=max_pages, delay=delay, timeout=timeout, 
                            use_requests=use_requests, max_depth=max_depth) as crawler:
        
        # DFS í¬ë¡¤ë§ ì‹¤í–‰
        results = crawler.discover_urls_dfs(start_url, scope)
            
        # ì¶”ê°€ íŒŒì¼ ìƒì„± (ì˜µì…˜)
        if generate_extra_files and results and "results_dir" in results:
            logger.debug(f"ğŸ“Š ì¶”ê°€ íŒŒì¼ ìƒì„± ì¤‘...")
            extra_files = crawler.generate_derived_files(results["results_dir"], generate_extra_files)
            results["extra_files"] = extra_files
            logger.debug(f"âœ… ì¶”ê°€ íŒŒì¼ ìƒì„± ì™„ë£Œ: {len(extra_files)}ê°œ")
            
        # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
        execution_time = time.time() - start_time
        
        if results:
            results["execution_time_seconds"] = execution_time
            base_files = 5 if results.get("visualization_file") else 4  # ê¸°ë³¸ 4ê°œ + ê·¸ë˜í”„ 1ê°œ
            extra_files_count = len(results.get("extra_files", {})) if generate_extra_files else 0
            total_files = base_files + extra_files_count
            
            logger.info(f"ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ! ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ, íŒŒì¼: {total_files}ê°œ")
        
        return results

def extract_document_urls_from_results(results: Dict[str, Any]) -> List[str]:
    """
    í¬ë¡¤ë§ ê²°ê³¼ì—ì„œ ë¬¸ì„œ URL ëª©ë¡ì„ ì¶”ì¶œ
    
    Args:
        results: discover_urls ë©”ì„œë“œì˜ ë°˜í™˜ê°’
        
    Returns:
        ë¬¸ì„œ URL ëª©ë¡
    """
    if not results or "doc_urls" not in results:
        return []
    
    doc_urls = results["doc_urls"]
    if isinstance(doc_urls, set):
        return list(doc_urls)
    elif isinstance(doc_urls, list):
        return doc_urls
    else:
        return []

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜ˆì‹œ
    results = main(
        start_url="https://hansung.ac.kr/sites/CSE/index.do",
        scope=None,
        max_pages=99999,
        delay=0.5,
        timeout=10,
        use_requests=True,
        verbose=True,
        max_depth=10
    )
    
    if results and "error" not in results:
        print(f"âœ… DFS í¬ë¡¤ë§ ì™„ë£Œ (4ê°œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜):")
        print(f"   ğŸ“Š í˜ì´ì§€: {len(results.get('page_urls', []))}ê°œ")
        print(f"   ğŸ“„ ë¬¸ì„œ: {len(results.get('doc_urls', []))}ê°œ")
        print(f"   ğŸ·ï¸ ë¶„ë¥˜: Main(1), document, board, general")
        if results.get('doc_urls_file'):
            print(f"   ğŸ“‹ ë¬¸ì„œ URL íŒŒì¼: {os.path.basename(results['doc_urls_file'])}")
    else:
        print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {results.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")