# url listë¥¼ ë°›ì•„ì„œ í¬ë¡¤ë§ ì‹œì‘ -> html_Structuring.py í˜¸ì¶œ ( -> Jina, artclView í¬ë¡¤ë§ ì‹œì‘ - > í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥) -> í¬ë¡¤ë§ ê²°ê³¼ ë°˜í™˜

import os
import sys
import tempfile
from pathlib import Path
from datetime import datetime
from services.crawling_service.html_Structuring import crawl_from_file

def main(page_id, url_list):
    """
    ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜
    Args:
        page_id (str): í˜ì´ì§€ ID
        url_list (list): URL ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ [{'url': 'http://...', 'date': '2025-01-01'}, ...]
    Returns:
        dict: í¬ë¡¤ë§ ê²°ê³¼
    """
    try:
        print(f"ğŸš€ í†µí•© í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì‹œì‘ - Page ID: {page_id}")
        
        if not url_list:
            return {
                "success": False,
                "error": "í¬ë¡¤ë§í•  URLì´ ì—†ìŠµë‹ˆë‹¤."
            }
        
        # URL ë¦¬ìŠ¤íŠ¸ì—ì„œ URLë§Œ ì¶”ì¶œ
        urls = [item['url'] for item in url_list if 'url' in item]
        
        if not urls:
            print(f"crawling_and_structuring.py: ğŸ”„ ìœ íš¨í•œ URLì´ ì—†ìŠµë‹ˆë‹¤.")
            return {
                "success": False,
                "error": "ìœ íš¨í•œ URLì´ ì—†ìŠµë‹ˆë‹¤."
            }
        
        # ì„ì‹œ URL íŒŒì¼ ìƒì„±
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as temp_file:
            for url in urls:
                temp_file.write(f"{url}\n")
            temp_url_file = temp_file.name
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì • - document_routes.pyì™€ ë™ì¼í•œ ë°©ì‹ ì‚¬ìš©
        # Flaskê°€ backendì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ ../data/input/ ì‚¬ìš©
        url_base_path = Path(f"../data/input/{page_id}_url")
        url_input_path = url_base_path / "input"
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        url_input_path.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ“ ì„ì‹œ URL íŒŒì¼: {temp_url_file}")
        print(f"ğŸ’¾ ì €ì¥ ê²½ë¡œ: {url_input_path}")
        print(f"ğŸ”— í¬ë¡¤ë§í•  URL ê°œìˆ˜: {len(urls)}")
        
        try:
            # í†µí•© í¬ë¡¤ë§ ì‹¤í–‰
            results = crawl_from_file(
                url_file_path=temp_url_file,
                page_id=page_id,
                output_base_dir=str(url_input_path),
                verbose=True
            )
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.unlink(temp_url_file)
            
            # ê²°ê³¼ ì²˜ë¦¬
            if "error" not in results:
                print("âœ… í†µí•© í¬ë¡¤ë§ ì„±ê³µ!")
                return {
                    "success": True,
                    "results": {
                        "page_id": page_id,
                        "total_success_count": results.get('total_success_count', 0),
                        "output_dir": str(url_input_path),
                        "artcl_results": results.get('artcl_results', {}),
                        "jina_results": results.get('jina_results', {}),
                        "execution_time": results.get('execution_time', 'N/A'),
                        "errors": results.get('errors', [])
                    }
                }
            else:
                return {
                    "success": False,
                    "error": f"í¬ë¡¤ë§ ì‹¤íŒ¨: {results.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
                }
                
        except Exception as e:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ (ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„)
            if os.path.exists(temp_url_file):
                os.unlink(temp_url_file)
            raise e
            
    except Exception as e:
        print(f"ğŸ’¥ ì˜ˆì™¸ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "error": f"í¬ë¡¤ë§ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}"
        }

# def test_integrated_crawling():
#     """í†µí•© í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
#     print("ğŸš€ í†µí•© í¬ë¡¤ë§ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
#     # í…ŒìŠ¤íŠ¸ìš© URL íŒŒì¼ ê²½ë¡œ
#     url_file_path = Path(__file__).parent.parent.parent / "data/crawling/20250526_0412_hansung_ac_kr_sites_hansung/page_urls_20250526_0412.txt"
    
#     # ì‚¬ìš©ì ì§€ì • ì €ì¥ ê²½ë¡œ
#     custom_output_dir = Path(__file__).parent.parent.parent / "data" / "crawling" / "20250526_0412_hansung_ac_kr_sites_hansung"
    
#     if not url_file_path.exists():
#         print(f"âŒ URL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {url_file_path}")
#         return
    
#     print(f"ğŸ“ URL íŒŒì¼: {url_file_path}")
#     print(f"ğŸ’¾ ì €ì¥ ê²½ë¡œ: {custom_output_dir}")
    
#     try:
#         # í†µí•© í¬ë¡¤ë§ ì‹¤í–‰
#         results = crawl_from_file(
#             str(url_file_path),
#             output_base_dir=str(custom_output_dir),
#             max_workers=3,
#             delay_range=(1.0, 2.0),
#             verbose=True
#         )
        
#         # ê²°ê³¼ ì¶œë ¥
#         if "error" not in results:
#             print("\nâœ… í†µí•© í¬ë¡¤ë§ ì„±ê³µ!")
#             print(f"ğŸ“Š ì´ ì„±ê³µí•œ íŒŒì¼: {results.get('total_success_count', 0)}ê°œ")
#             print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {results.get('output_base_dir', 'N/A')}")
            
#             # artclView ê²°ê³¼
#             artcl_results = results.get('artcl_results', {})
#             if artcl_results:
#                 print(f"ğŸ”— artclView í¬ë¡¤ë§: {artcl_results.get('success_count', 0)}ê°œ íŒŒì¼")
#                 print(f"   ğŸ“ ì²¨ë¶€íŒŒì¼: {artcl_results.get('attachment_count', 0)}ê°œ")
#                 print(f"   ğŸ“‚ ì €ì¥ ê²½ë¡œ: {artcl_results.get('output_dir', 'N/A')}")
            
#             # Jina ê²°ê³¼
#             jina_results = results.get('jina_results', {})
#             if jina_results:
#                 print(f"ğŸ¤– Jina í¬ë¡¤ë§: {jina_results.get('success_count', 0)}ê°œ íŒŒì¼")
#                 print(f"   ğŸ“‚ ì €ì¥ ê²½ë¡œ: {jina_results.get('output_dir', 'N/A')}")
            
#             # ì˜¤ë¥˜ ì •ë³´
#             errors = results.get('errors', [])
#             if errors:
#                 print(f"âš ï¸  ë°œìƒí•œ ì˜¤ë¥˜: {len(errors)}ê°œ")
#                 for error in errors:
#                     print(f"   - {error}")
            
#             print(f"â±ï¸  ì‹¤í–‰ ì‹œê°„: {results.get('execution_time', 'N/A')}")
            
#         else:
#             print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {results.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            
#     except Exception as e:
#         print(f"ğŸ’¥ ì˜ˆì™¸ ë°œìƒ: {e}")
#         import traceback
#         traceback.print_exc()

# if __name__ == "__main__":
#     test_integrated_crawling() 