import asyncio
from concurrent.futures import ThreadPoolExecutor
from sentence_transformers import SentenceTransformer
import tiktoken
import re
import time
import json
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from openai import OpenAI
import pandas as pd
import requests
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import os
from dotenv import load_dotenv
from graphrag.config.enums import ModelType
from graphrag.config.models.language_model_config import LanguageModelConfig
from graphrag.language_model.manager import ModelManager
from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey
from graphrag.query.indexer_adapters import (
    read_indexer_communities,
    read_indexer_covariates,
    read_indexer_entities,
    read_indexer_relationships,
    read_indexer_reports,
    read_indexer_text_units,
)
from graphrag.query.question_gen.local_gen import LocalQuestionGen
from graphrag.query.structured_search.local_search.mixed_context import (
    LocalSearchMixedContext,
)
from graphrag.query.structured_search.local_search.search import LocalSearch
from graphrag.vector_stores.lancedb import LanceDBVectorStore
from graphrag.query.structured_search.global_search.community_context import (
    GlobalCommunityContext,
)

load_dotenv()
api_key = os.getenv("GRAPHRAG_API_KEY", "").strip()
client= OpenAI(api_key=api_key)

@dataclass
class QAEvaluation:
    question: str
    answer: str
    contexts: List[str]
    ground_truth: Optional[str] = None

class LLMEvaluator:
    """LLMì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self, api_key: str = None, model: str = "gpt-4o-mini"):
        self.model = model
    
    def extract_statements(self, text: str) -> List[str]:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì—ì„œ ì§„ìˆ  ì¶”ì¶œ"""
        prompt = f"""
                ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ê°œë³„ì ì¸ ì‚¬ì‹¤ì  ì§„ìˆ ë“¤ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”. ê° ì§„ìˆ ì€ í•˜ë‚˜ì˜ ì™„ì „í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

                í…ìŠ¤íŠ¸: {text}

                ê²°ê³¼ë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”:
                1. [ì§„ìˆ 1]
                2. [ì§„ìˆ 2]
                3. [ì§„ìˆ 3]
                ...

                ì§„ìˆ ì´ ì—†ìœ¼ë©´ "ì§„ìˆ  ì—†ìŒ"ì´ë¼ê³  ë‹µí•´ì£¼ì„¸ìš”.
                """
        
        try:
            response = client.chat.completions.create(model=self.model, 
                                                      messages=[{"role": "user", "content": prompt}], 
                                                      temperature=0)
            
            content = response.choices[0].message.content
            
            # ë²ˆí˜¸ê°€ ìˆëŠ” ë¦¬ìŠ¤íŠ¸ì—ì„œ ì§„ìˆ  ì¶”ì¶œ
            statements = []
            for line in content.split('\n'):
                line = line.strip()
                if re.match(r'\d+\.', line):
                    statement = re.sub(r'^\d+\.\s*', '', line).strip()
                    if statement and statement != "ì§„ìˆ  ì—†ìŒ":
                        statements.append(statement)
            #print("Extracted statements:", statements)
            return statements
            
        except Exception as e:
            print(f"LLM ì§„ìˆ  ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            # í´ë°±: ê°„ë‹¨í•œ ë¬¸ì¥ ë¶„ë¦¬
            return self._fallback_extract_statements(text)
    
    def _fallback_extract_statements(self, text: str) -> List[str]:
        """LLM ì‹¤íŒ¨ ì‹œ í´ë°± ì§„ìˆ  ì¶”ì¶œ"""
        sentences = re.split(r'[.!?ã€‚]', text)
        return [s.strip() for s in sentences if len(s.strip()) > 10]
    
    def check_statement_support(self, statement: str, contexts: List[str]) -> float:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ì§„ìˆ ì´ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì–¼ë§ˆë‚˜ ì§ì ‘ì ìœ¼ë¡œ ì§€ì›ë˜ëŠ”ì§€ ì ìˆ˜í™” (0.0 ~ 1.0)"""
        contexts_text = "\n".join(contexts)

        prompt = f"""
        Statementì™€ Contextê°€ ì£¼ì–´ì¡ŒìŠµë‹ˆë‹¤. Statementê°€ Contextì— ì–¼ë§ˆë‚˜ ì˜ ë’·ë°›ì¹¨ë˜ëŠ”ì§€ ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ ì ìˆ˜ë¥¼ 0.0~1.0 ì‚¬ì´ë¡œ ë§¤ê²¨ ì£¼ì„¸ìš”.

        Statement: "{statement}"

        Context:
        {contexts_text}

        íŒì • ê¸°ì¤€:
        - ì™„ì „íˆ ë™ì¼í•œ ë¬¸ì¥ì´ ìˆê±°ë‚˜, ì‚¬ì‹¤ì ìœ¼ë¡œ ëª…ì‹œë˜ì–´ ìˆìœ¼ë©´ 1.0ì 
        - ì¶”ë¡ ì„ ì•½ê°„ ìš”êµ¬í•˜ì§€ë§Œ, ë’·ë°›ì¹¨ì´ ë¶„ëª…í•˜ë©´ 0.9ì 
        - ë³µì¡í•œ ì¶”ë¡ ì´ í•„ìš”í•œ ê²½ìš° 0.8ì 
        - ì•½ê°„ ê´€ë ¨ì€ ìˆì§€ë§Œ ë¶ˆí™•ì‹¤í•˜ê±°ë‚˜ ì¼ë¶€ ëˆ„ë½ëœ ê²½ìš° 0.6ì 
        - ì „í˜€ ì–¸ê¸‰ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ëª¨ìˆœë˜ëŠ” ê²½ìš° 0.0ì 

        í˜•ì‹: ì ìˆ˜ë§Œ ìˆ«ì í˜•íƒœë¡œ ë°˜í™˜ (ì˜ˆ: "0.8")
        """

        try:
            response = client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0
            )
            content = response.choices[0].message.content.strip()
            print("ì§€ì› ì ìˆ˜ ì‘ë‹µ ë‚´ìš©:", content)

            # ì ìˆ˜ íŒŒì‹±
            score = float(re.findall(r"([0-1](?:\.\d+)?)", content)[0])
            return max(0.0, min(score, 1.0))

        except Exception as e:
            print(f"LLM ì§€ì› ì ìˆ˜ í™•ì¸ ì˜¤ë¥˜: {e}")
            # fallback: í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ì‚¬ë„
            return self._fallback_check_support(statement, contexts)
    
    def _fallback_check_support(self, statement: str, contexts: List[str]) -> bool:
        """LLM ì‹¤íŒ¨ ì‹œ í´ë°± ì§€ì› í™•ì¸"""
        from difflib import SequenceMatcher

        joined_context = " ".join(contexts)
        ratio = SequenceMatcher(None, statement, joined_context).ratio()
        return round(min(max(ratio, 0.0), 1.0), 2)
    
    def generate_reverse_question(self, answer: str) -> str:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì—ì„œ ì—­ì§ˆë¬¸ ìƒì„±"""
        prompt = f"""
                    ë‹¤ìŒ ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ ì›ë˜ ì§ˆë¬¸ì´ ë¬´ì—‡ì´ì—ˆì„ì§€ ì¶”ì¸¡í•˜ì—¬ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

                    ë‹µë³€: {answer}

                    ìƒì„±ëœ ì§ˆë¬¸ë§Œ ë‹µí•´ì£¼ì„¸ìš”. ì„¤ëª…ì€ ë¶ˆí•„ìš”í•©ë‹ˆë‹¤.
                    """
        
        try:
            response = client.chat.completions.create(model=self.model, 
                                                      messages=[{"role": "user", "content": prompt}], 
                                                      temperature=0)
            print("generate reverse question ë‹µë³€ : ",  response.choices[0].message.content)
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            print(f"LLM ì—­ì§ˆë¬¸ ìƒì„± ì˜¤ë¥˜: {e}")
            return self._fallback_generate_question(answer)
    
    def _fallback_generate_question(self, answer: str) -> str:
        """LLM ì‹¤íŒ¨ ì‹œ í´ë°± ì§ˆë¬¸ ìƒì„±"""
        # ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì§ˆë¬¸ ìƒì„±
        if re.search(r'\d+ë…„', answer):
            return "ì–¸ì œì¸ê°€ìš”?"
        elif re.search(r'\d+í•™ì ', answer):
            return "ëª‡ í•™ì ì¸ê°€ìš”?"
        elif "ìœ„ì¹˜" in answer or "ì£¼ì†Œ" in answer:
            return "ì–´ë””ì¸ê°€ìš”?"
        else:
            return "ë¬´ì—‡ì¸ê°€ìš”?"

class AccuracyCalculator:
    """ì •í™•ë„ ê³„ì‚° ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, llm_evaluator: LLMEvaluator = None):
        self.weights = [0.45, 0.35, 0.20]  # faithfulness, relevancy, recall
        self.metric_names = ['faithfulness', 'answer_relevancy','context_recall']
        self.llm_evaluator = llm_evaluator or LLMEvaluator()
        self.vectorizer = TfidfVectorizer()
        self.model = "gpt-4o-mini"
        self.embedding_model = SentenceTransformer("paraphrase-MiniLM-L6-v2")

    def safe_llm_call(self, prompt: str, temperature: float = 0) -> str:
        """ì•ˆì „í•œ LLM í˜¸ì¶œ ë©”ì„œë“œ - ëˆ„ë½ëœ ë©”ì„œë“œ ì¶”ê°€"""
        try:
            response = client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"LLM í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            return ""

    def calculate_faithfulness(self, answer: str, contexts: List[str]) -> float:
        """ì‹ ì‹¤ì„± ê³„ì‚°: LLMìœ¼ë¡œ í‰ê°€, ì½”ë“œë¡œ ê³„ì‚°"""
        print("ì‹ ì‹¤ì„± ê³„ì‚° ì‹œì‘")
        start_time = time.time()

        if not answer.strip():
            return 0.0
        
        # LLMìœ¼ë¡œ ì§„ìˆ  ì¶”ì¶œ
        statements = self.llm_evaluator.extract_statements(answer)
        
        if not statements:
            elapsed = time.time() - start_time
            print(f"ì‹ ì‹¤ì„± ê³„ì‚° ì¢…ë£Œ: {elapsed:.2f}ì´ˆ")
            return 1.0  # ì§„ìˆ ì´ ì—†ìœ¼ë©´ ì™„ë²½í•œ ì‹ ì‹¤ì„±
        
        # LLMìœ¼ë¡œ ê° ì§„ìˆ ì˜ ì§€ì› ì—¬ë¶€ í™•ì¸
        total_score = 0.0
        for statement in statements:
            score = self.llm_evaluator.check_statement_support(statement, contexts)
            print(f"ğŸ§¾ ì§„ìˆ : {statement} â†’ ì ìˆ˜: {score}")
            total_score += score

        faithfulness = total_score / len(statements)
        print(f"âœ… ì‹ ì‹¤ì„± ì ìˆ˜ (ì •ëŸ‰): {faithfulness:.3f}")
        print(f"â±ï¸ ì™„ë£Œ ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
        return round(faithfulness, 3)
    
    def calculate_relevancy(self, question: str, answer: str) -> float:
        """ë‹µë³€ ê´€ë ¨ì„± ê³„ì‚°: ì½”ë“œë¡œ ê³„ì‚°"""

        print("ê´€ë ¨ì„± ê³„ì‚° ì‹œì‘")
        start_time = time.time()

        if not question.strip() or not answer.strip():
            return 0.0
        
        # LLMìœ¼ë¡œ ì—­ì§ˆë¬¸ ìƒì„±
        reverse_question = self.llm_evaluator.generate_reverse_question(answer)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ì€ ì½”ë“œë¡œ
        similarity = self._calculate_cosine_similarity(question, reverse_question)
        print("similarity ì ìˆ˜: ", similarity)
        
        similarity = min(1.0, similarity)
        elapsed = time.time() - start_time
        print(f"âœ… ê´€ë ¨ì„± ê³„ì‚° ì™„ë£Œ: {elapsed:.2f}ì´ˆ")
        return round(similarity, 3)
    
    # def calculate_precision(self, contexts: List[str], question: str) -> float:
    #     """
    #     Context Precision ê³„ì‚° (LLM ê¸°ë°˜, Batch ì²˜ë¦¬)
    #     â†’ ìœ ìš©í•œ context(ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ)ì˜ ë¹„ìœ¨
    #     """
    #     import time
    #     print("ğŸ“Š Context Precision ê³„ì‚° ì‹œì‘")
    #     start_time = time.time()

    #     if not contexts or not question.strip():
    #         return 0.0

    #     is_useful_list = self._check_contexts_usefulness_batch(contexts, question)
    #     relevant_count = sum(is_useful_list)

    #     precision = relevant_count / len(contexts)
    #     elapsed = time.time() - start_time
    #     print(f"âœ… Context Precision ê³„ì‚° ì™„ë£Œ: {elapsed:.2f}ì´ˆ")
    #     print(f"ìœ ìš©í•œ ë¬¸ì„œ ìˆ˜: {relevant_count}/{len(contexts)}")
    #     print("precision ê°’: ", precision)
    #     return round(min(precision + 0.5, 1.0), 3)
    
    def _check_contexts_usefulness_batch(self, contexts: List[str], question: str) -> List[bool]:
        """
        ì—¬ëŸ¬ contextë¥¼ í•œ ë²ˆì— í‰ê°€í•˜ì—¬ ìœ ìš© ì—¬ë¶€ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
        """
        prompt = f"""
    ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ê° ì»¨í…ìŠ¤íŠ¸ê°€ ìœ ìš©í•œì§€ íŒì •í•´ì£¼ì„¸ìš”.

    ì§ˆë¬¸: "{question}"

    ê° ì»¨í…ìŠ¤íŠ¸ë§ˆë‹¤ "ìœ ìš©í•¨" ë˜ëŠ” "ìœ ìš©í•˜ì§€ ì•ŠìŒ"ìœ¼ë¡œë§Œ íŒì •í•´ ì£¼ì„¸ìš”.

    í˜•ì‹:
    1. ìœ ìš©í•¨
    2. ìœ ìš©í•˜ì§€ ì•ŠìŒ
    ...

    ì»¨í…ìŠ¤íŠ¸ ëª©ë¡:
    """

        for i, ctx in enumerate(contexts, start=1):
            prompt += f"{i}. {ctx}\n"

        try:
            response = client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0
            )
            content = response.choices[0].message.content.strip()
            print("LLM ì‘ë‹µ ê²°ê³¼:\n", content)

            # ê²°ê³¼ íŒŒì‹±
            lines = content.splitlines()
            result_flags = []
            for line in lines:
                if "ìœ ìš©í•¨" in line:
                    result_flags.append(True)
                elif "ìœ ìš©í•˜ì§€ ì•ŠìŒ" in line:
                    result_flags.append(False)

            # fallback: ë¦¬ìŠ¤íŠ¸ ê¸¸ì´ ì•ˆ ë§ìœ¼ë©´ ì „ë¶€ False
            if len(result_flags) != len(contexts):
                print("âš ï¸ íŒì • ìˆ˜ ë¶ˆì¼ì¹˜, ì „ë¶€ False ì²˜ë¦¬")
                return [False] * len(contexts)

            return result_flags

        except Exception as e:
            print(f"LLM ë°°ì¹˜ íŒì • ì˜¤ë¥˜: {e}")
            return [self._fallback_check_usefulness(ctx, question) for ctx in contexts]
    
    def _fallback_check_usefulness(self, context: str, question: str) -> bool:
        """LLM ì‹¤íŒ¨ ì‹œ í´ë°± ìœ ìš©ì„± íŒì •"""
        question_words = set(re.findall(r'\w+', question.lower()))
        context_words = set(re.findall(r'\w+', context.lower()))
        
        if not context_words:
            return False
        
        # ì§ˆë¬¸ ë‹¨ì–´ì™€ ì»¨í…ìŠ¤íŠ¸ ë‹¨ì–´ì˜ ê²¹ì¹˜ëŠ” ë¹„ìœ¨ ê³„ì‚°
        overlap = len(question_words.intersection(context_words))
        relevance_ratio = overlap / len(question_words)
        
        # 15% ì´ìƒ ê²¹ì¹˜ë©´ ìœ ìš©í•œ ë¬¸ì„œë¡œ ê°„ì£¼
        return relevance_ratio >= 0.15
    

    def calculate_recall(self, contexts: List[str], answer: str, question: str) -> float:
        """ì»¨í…ìŠ¤íŠ¸ ì¬í˜„ìœ¨ ê³„ì‚°: Ground Truthì˜ ì •ë³´ê°€ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì— ì–¼ë§ˆë‚˜ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€"""
        print("recall ê³„ì‚° ì‹œì‘")
        start_time = time.time()

        required_info = self._extract_key_information(question)

        if not required_info or not contexts:
            print("required_info ë˜ëŠ” context ì—†ìŒ")
            return 1.0

        #print(f"ê²€ì¦í•  í•µì‹¬ ì •ë³´: {required_info}")
        #print(f"ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸: {contexts}")
        #print(f"AI ë‹µë³€: {answer}")
        
        total_score = 0
        max_score = len(required_info)
        
        contexts_combined = ' '.join(contexts)
        
        for i, info in enumerate(required_info):
            print(f"\n--- í•µì‹¬ ì •ë³´ {i+1}: '{info}' ê²€ì¦ ---")
            
            # ë‹¨ê³„ 2: í•µì‹¬ ì •ë³´ê°€ ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ”ì§€ í™•ì¸
            context_has_info = self._check_info_in_context(info, contexts_combined)
            #print(f"ì»¨í…ìŠ¤íŠ¸ì— ì •ë³´ ì¡´ì¬: {context_has_info}")
            
            if not context_has_info:
                print(f"âŒ ì»¨í…ìŠ¤íŠ¸ì— '{info}' ì •ë³´ ì—†ìŒ - 0ì ")
                continue
            
            # ë‹¨ê³„ 3: ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ê°€ ë‹µë³€ì— ì œëŒ€ë¡œ ë°˜ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
            answer_accuracy = self._check_answer_accuracy(info, contexts_combined, answer)
            print(f"ë‹µë³€ ì •í™•ë„: {answer_accuracy}")
            
            if answer_accuracy >= 0.8:  # 80% ì´ìƒ ì •í™•
                score = 1.0
                print(f"âœ… ì™„ì „í•œ ì •ë³´ ë°˜ì˜ - 1ì ")
            elif answer_accuracy >= 0.5:  # 50% ì´ìƒ ì •í™• (ë¶€ë¶„ì )
                score = 0.7
                print(f"âš ï¸ ë¶€ë¶„ì  ì •ë³´ ë°˜ì˜ - 0.5ì ")
            elif answer_accuracy >= 0.2:
                score = 0.4
                print("ë¶€ì •í™• (ë§ì€ ì˜¤ë¥˜ ë˜ëŠ” ì™œê³¡)")
            else:  # 50% ë¯¸ë§Œ
                score = 0.0
                print(f"âŒ ë¶€ì •í™•í•œ ì •ë³´ ë°˜ì˜ - 0ì ")
            
            total_score += score
        
        # ìµœì¢… recall ê³„ì‚°
        recall = total_score / max_score if max_score > 0 else 1.0
        
        print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼:")
        print(f"ì´ ì ìˆ˜: {total_score}/{max_score}")
        print(f"Context Recall: {recall}")
        
        elapsed = time.time() - start_time
        print(f"âœ… recall ê³„ì‚° ì™„ë£Œ: {elapsed:.2f}ì´ˆ")

        return round(recall, 3)
    
    def _check_info_in_context(self, info: str, contexts: str) -> bool:
        """í•µì‹¬ ì •ë³´ê°€ ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ”ì§€ í™•ì¸"""
        
        prompt = f"""
        ë‹¤ìŒ í•µì‹¬ ì •ë³´ê°€ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.
        
        í•µì‹¬ ì •ë³´: "{info}"
        ì»¨í…ìŠ¤íŠ¸: "{contexts}"
        
        íŒë‹¨ ê¸°ì¤€:
        1. í•µì‹¬ ì •ë³´ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ”ê°€?
        2. í•µì‹¬ ì •ë³´ë¥¼ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ” ë‚´ìš©ì´ ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ”ê°€?
        3. ìœ ì‚¬í•œ ì˜ë¯¸ì˜ í‘œí˜„ì´ ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ”ê°€?
        
        "ì˜ˆ" ë˜ëŠ” "ì•„ë‹ˆì˜¤"ë¡œë§Œ ë‹µí•´ì£¼ì„¸ìš”.
        """
        
        try:
            response = client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0
            )
            
            answer = response.choices[0].message.content.strip().lower()
            return "ì˜ˆ" in answer or "yes" in answer
            
        except Exception as e:
            print(f"ì»¨í…ìŠ¤íŠ¸ í™•ì¸ ì‹¤íŒ¨: {e}")
            # í´ë°±: í‚¤ì›Œë“œ ìœ ì‚¬ë„
            return self._semantic_similarity(info, contexts) > 0.3

    def _check_answer_accuracy(self, info: str, context: str, answer: str) -> float:
        """ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ê°€ ë‹µë³€ì— ì •í™•íˆ ë°˜ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        
        prompt = f"""
        ë‹¤ìŒ ìƒí™©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:
        
        ìš”êµ¬ëœ ì •ë³´: "{info}"
        ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´: "{context}"
        AIì˜ ë‹µë³€: "{answer}"
        
        AIì˜ ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ì •í™•í•˜ê²Œ ë°˜ì˜í–ˆëŠ”ì§€ í‰ê°€í•´ì£¼ì„¸ìš”.
        
        í‰ê°€ ê¸°ì¤€:
        1. ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ë¥¼ ì •í™•íˆ ê·¸ëŒ€ë¡œ ì‚¬ìš©í–ˆëŠ”ê°€?
        2. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì„ ì¶”ê°€í–ˆëŠ”ê°€? (ê°ì  ìš”ì†Œ)
        3. ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ë¥¼ ì™œê³¡í–ˆëŠ”ê°€? (ê°ì  ìš”ì†Œ)
        4. ì»¨í…ìŠ¤íŠ¸ì˜ ì¼ë¶€ ì •ë³´ë§Œ ì‚¬ìš©í–ˆëŠ”ê°€? (ê°ì  ìš”ì†Œ)
        
        ì ìˆ˜ë¥¼ 0.0ì—ì„œ 1.0 ì‚¬ì´ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.
        - 1.0: ì™„ë²½í•˜ê²Œ ì •í™•
        - 0.8: ëŒ€ë¶€ë¶„ ì •í™• (ì•½ê°„ì˜ í‘œí˜„ ì°¨ì´)
        - 0.5: ë¶€ë¶„ì ìœ¼ë¡œ ì •í™• (ì¼ë¶€ ì •ë³´ ëˆ„ë½ ë˜ëŠ” ì¶”ê°€)
        - 0.2: ë¶€ì •í™• (ë§ì€ ì˜¤ë¥˜ ë˜ëŠ” ì™œê³¡)
        - 0.0: ì™„ì „íˆ í‹€ë¦¼
        
        ì ìˆ˜ë§Œ ìˆ«ìë¡œ ë‹µí•´ì£¼ì„¸ìš” (ì˜ˆ: 0.8)
        """
        
        try:
            response = client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0
            )
            
            score_text = response.choices[0].message.content.strip()
            
            # ìˆ«ì ì¶”ì¶œ
            import re
            score_match = re.search(r'(\d+\.?\d*)', score_text)
            if score_match:
                score = float(score_match.group(1))
                return min(1.0, max(0.0, score))  # 0.0~1.0 ë²”ìœ„ë¡œ ì œí•œ
            else:
                return 0.5  # ê¸°ë³¸ê°’
                
        except Exception as e:
            print(f"ë‹µë³€ ì •í™•ë„ í™•ì¸ ì‹¤íŒ¨: {e}")
            # í´ë°±: ì˜ë¯¸ì  ìœ ì‚¬ë„
            return self._semantic_similarity(context, answer)

    def _semantic_similarity(self, text1: str, text2: str) -> float:
        """ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° (ì„ë² ë”© ê¸°ë°˜)"""
        
        try:
            # 1. OpenAI ì„ë² ë”© ì‚¬ìš© (ì¶”ì²œ)
            embedding1 = self._get_embedding(text1)
            embedding2 = self._get_embedding(text2)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarity = cosine_similarity([embedding1], [embedding2])[0][0]
            return float(similarity)
            
        except Exception as e:
            print(f"ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            # í´ë°±: TF-IDF ê¸°ë°˜ ìœ ì‚¬ë„
            return self._fallback_semantic_similarity(text1, text2)

    def _get_embedding(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ë²¡í„° íšë“"""
        
        try:
            response = client.embeddings.create(
                model="text-embedding-3-small",  # ë˜ëŠ” "text-embedding-ada-002"
                input=text
            )
            return response.data[0].embedding
            
        except Exception as e:
            print(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: ëœë¤ ë²¡í„° (ì‹¤ì œë¡œëŠ” ë‹¤ë¥¸ ë°©ë²• ì‚¬ìš©)
            raise e

    def _fallback_semantic_similarity(self, text1: str, text2: str) -> float:
        """í´ë°±: TF-IDF ê¸°ë°˜ ì˜ë¯¸ì  ìœ ì‚¬ë„"""
        
        # TF-IDF ë²¡í„°í™”
        vectorizer = TfidfVectorizer(stop_words='english')
        tfidf_matrix = vectorizer.fit_transform([text1, text2])
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
        return float(similarity)

    def _extract_key_information(self, question: str) -> List[str]:
        """ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ë‹µë³€ì— í•„ìš”í•œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ"""
        
        prompt = f"""
        ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ 'ì™„ì „í•œ ë‹µë³€ì„ ìœ„í•´ í•„ìš”í•œ í•µì‹¬ ì •ë³´ ê°œë…'ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

        - ë‚ ì§œ, ìˆ«ì, ì‹œê¸° ë“± êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ê°’ì€ ì œì™¸í•©ë‹ˆë‹¤.
        - ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë“±ì¥í•œ ê°œë…ì–´(ëª…ì‚¬ ì¤‘ì‹¬)ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ, ë‹µë³€ì„ ìœ„í•´ í•„ìš”í•œ ì£¼ìš” ì •ë³´ í•­ëª©ì„ ì¼ë°˜í™”ëœ í˜•íƒœë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
        - ì •ë³´ëŠ” "ë¬´ì—‡ì— ëŒ€í•œ ì •ë³´ì¸ê°€?"ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê°„ê²°í•˜ê³  ì¼ë°˜í™”ëœ í˜•íƒœë¡œ ê¸°ìˆ í•´ì£¼ì„¸ìš”.
        - ë‹¨ì–´ ì¤‘ì‹¬, ê°œë… ì¤‘ì‹¬ì˜ ì¶”ì¶œì„ ëª©í‘œë¡œ í•˜ë©°, ì„¤ëª…ì´ë‚˜ ë²”ì£¼ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

        ì˜ˆì‹œ:
        ì§ˆë¬¸: "í•œì„±ëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ê³¼ êµìˆ˜ì§„ì€ ëª‡ ëª…ì´ê³ , ì£¼ìš” ì—°êµ¬ë¶„ì•¼ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
        í•µì‹¬ ì •ë³´:
        1. êµìˆ˜ì§„ ì¸ì›
        2. ì—°êµ¬ë¶„ì•¼
        3. ì»´í“¨í„°ê³µí•™ê³¼

        ì§ˆë¬¸: "íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ì˜ íŠ¹ì§•ê³¼ ì‚¬ìš©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?"
        í•µì‹¬ ì •ë³´:
        1. ë¦¬ìŠ¤íŠ¸ íŠ¹ì§•
        2. ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©ë²•
        3. íŒŒì´ì¬

        ì§ˆë¬¸: "2023í•™ë…„ 2ì›” ì…í•™ìƒì€ ëª‡ ë…„ë„ì— ì¡¸ì—…ì´ì•¼?"
        í•µì‹¬ ì •ë³´:
        1. ì…í•™ìƒ
        2. ì¡¸ì—…
        3. ì—°ë„

        ê²°ê³¼ëŠ” ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”:
        1. [í•µì‹¬ ì •ë³´ 1]
        2. [í•µì‹¬ ì •ë³´ 2]
        3. [í•µì‹¬ ì •ë³´ 3]

        í•µì‹¬ ì •ë³´ê°€ ì—†ë‹¤ë©´ "ì •ë³´ ì—†ìŒ"ì´ë¼ê³  ë‹µí•´ì£¼ì„¸ìš”.

        ì§ˆë¬¸: {question}
        """
        
        try:
            response = client.chat.completions.create(
                model="gpt-4.1",
                messages=[{"role": "user", "content": prompt}],
                temperature=0
            )
            
            content = response.choices[0].message.content
            print(f"ì§ˆë¬¸ì—ì„œ ì¶”ì¶œëœ í•µì‹¬ ì •ë³´: {content}")
            
            # ë²ˆí˜¸ê°€ ìˆëŠ” ë¦¬ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
            key_info = []
            for line in content.split('\n'):
                line = line.strip()
                if re.match(r'\d+\.', line):
                    info = re.sub(r'^\d+\.\s*', '', line).strip()
                    if info and info != "ì •ë³´ ì—†ìŒ":
                        key_info.append(info)
            
            return key_info if key_info else ["ê¸°ë³¸ ì •ë³´"]
            
        except Exception as e:
            print(f"í•µì‹¬ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ["ê¸°ë³¸ ì •ë³´"]

    def _fallback_extract_key_info(self, text: str) -> List[str]:
        """í´ë°±: ê°„ë‹¨í•œ ë¬¸ì¥ ë¶„ë¦¬"""
        sentences = re.split(r'[.!?ã€‚]', text)
        return [s.strip() for s in sentences if len(s.strip()) > 10]
    
    def _calculate_cosine_similarity(self, text1: str, text2: str) -> float:
        """SBERT ê¸°ë°˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            emb1 = self.embedding_model.encode(text1, convert_to_tensor=True)
            emb2 = self.embedding_model.encode(text2, convert_to_tensor=True)

            emb1_np = emb1.cpu().detach().numpy().reshape(1, -1)
            emb2_np = emb2.cpu().detach().numpy().reshape(1, -1)

            similarity = cosine_similarity(emb1_np, emb2_np)[0][0]
            return float(similarity)
        except Exception as e:
            print(f"[ì˜¤ë¥˜] SBERT ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0


    def calculate_accuracy(self, question: str, answer: str, contexts: List[str]) -> Dict[str, Any]:
        """ë©”ì¸ ì •í™•ë„ ê³„ì‚° í•¨ìˆ˜"""
        
        # 1. ê° ë©”íŠ¸ë¦­ ê³„ì‚° (LLM í‰ê°€ + ì½”ë“œ ê³„ì‚°)
        faithfulness = self.calculate_faithfulness(answer, contexts)
        relevancy = self.calculate_relevancy(question, answer)
        #precision = self.calculate_precision(contexts, question)
        recall = self.calculate_recall(contexts, answer, question)
        
        # 2. ê°€ì¤‘ì¹˜ ì ìš©í•˜ì—¬ ìµœì¢… ì ìˆ˜ (ìˆœìˆ˜ ì½”ë“œ ê³„ì‚°)
        #metrics = [faithfulness, relevancy, precision, recall]
        metrics = [faithfulness, relevancy, recall]
        total_score = sum(w * m for w, m in zip(self.weights, metrics))
        
        # 3. ë“±ê¸‰ íŒì •
        grade_info = self._get_grade(total_score)
        
        # 4. ê²°ê³¼ ë°˜í™˜
        return {
            'total_accuracy': round(total_score, 3),
            'percentage': round(total_score * 100, 1),
            'grade': grade_info['grade'],
            'level': grade_info['level'],
            'metrics': {
                name: score for name, score in zip(self.metric_names, metrics)
            },
            'weights': {
                name: weight for name, weight in zip(self.metric_names, self.weights)
            },
            'detailed_breakdown': {
                f"{name} ({weight})": f"{score} Ã— {weight} = {round(score * weight, 3)}"
                for name, score, weight in zip(self.metric_names, metrics, self.weights)
            },
            'metric_names': self.metric_names,
        }
    
    def _get_grade(self, total_score: float) -> Dict[str, str]:
        """ë“±ê¸‰ íŒì • (ìˆœìˆ˜ ì½”ë“œ)"""
        if total_score >= 0.95:
            return {"grade": "A+", "level": "ìµœìš°ìˆ˜"}
        elif total_score >= 0.85:
            return {"grade": "A", "level": "ìš°ìˆ˜"}
        elif total_score >= 0.75:
            return {"grade": "B", "level": "ì–‘í˜¸"}
        elif total_score >= 0.65:
            return {"grade": "C", "level": "ê¸°ë³¸"}
        else:
            return {"grade": "D", "level": "ë¯¸í¡"}


def read_csv_as_text_list(file_path: str) -> list[str]:
    try:
        df = pd.read_csv(file_path, encoding='utf-8-sig')
        # ëª¨ë“  ì…€ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•´ì„œ í•œ ì¤„ì”© í•©ì¹¨
        lines = df.astype(str).apply(lambda row: ' | '.join(row), axis=1).tolist()
        return lines
    except Exception as e:
        print(f"CSV ì½ê¸° ì˜¤ë¥˜ ({file_path}): {e}")
        return []

thread_pool = ThreadPoolExecutor(max_workers=5)

GRAPHRAG_API_KEY = os.getenv("GRAPHRAG_API_KEY")
GRAPHRAG_LLM_MODEL = "gpt-4o-mini"
GRAPHRAG_EMBEDDING_MODEL = "text-embedding-3-small"

def run_async(coro):
    """Run a coroutine in a new event loop in a separate thread"""
    loop = asyncio.new_event_loop()
    
    try:
        return loop.run_until_complete(coro)
    finally:
        loop.close()
        
def run_local_query(query_text):
    # INPUT_DIR í•˜ë“œì½”ë”© ì œê±°
    INPUT_DIR = "/Users/jy/Documents/Domain_QA_Gen/data/input/1747480728566_í•™êµ/output"
    LANCEDB_URI = f"{INPUT_DIR}/lancedb"
    
    # Load data
    entity_df = pd.read_parquet(f"{INPUT_DIR}/entities.parquet")
    community_df = pd.read_parquet(f"{INPUT_DIR}/communities.parquet")
    entities = read_indexer_entities(entity_df, community_df, 2)

    description_embedding_store = LanceDBVectorStore(collection_name="default-entity-description")
    description_embedding_store.connect(db_uri=LANCEDB_URI)

    relationship_df = pd.read_parquet(f"{INPUT_DIR}/relationships.parquet")
    relationships = read_indexer_relationships(relationship_df)

    report_df = pd.read_parquet(f"{INPUT_DIR}/community_reports.parquet")
    reports = read_indexer_reports(report_df, community_df, 2)

    text_unit_df = pd.read_parquet(f"{INPUT_DIR}/text_units.parquet")
    text_units = read_indexer_text_units(text_unit_df)

    # ëª¨ë¸ ì„¤ì •
    chat_config = LanguageModelConfig(
        api_key=GRAPHRAG_API_KEY,
        type=ModelType.OpenAIChat,
        model=GRAPHRAG_LLM_MODEL,
        max_retries=20,
    )
    chat_model = ModelManager().get_or_create_chat_model(
        name="local_search",
        model_type=ModelType.OpenAIChat,
        config=chat_config,
    )
    token_encoder = tiktoken.encoding_for_model(GRAPHRAG_LLM_MODEL)

    embedding_config = LanguageModelConfig(
        api_key=GRAPHRAG_API_KEY,
        type=ModelType.OpenAIEmbedding,
        model=GRAPHRAG_EMBEDDING_MODEL,
        max_retries=20,
    )
    text_embedder = ModelManager().get_or_create_embedding_model("local_search_embedding", ModelType.OpenAIEmbedding, config=embedding_config,)

    context_builder = LocalSearchMixedContext(
        community_reports=reports,
        text_units=text_units,
        entities=entities,
        relationships=relationships,
        entity_text_embeddings=description_embedding_store,
        embedding_vectorstore_key=EntityVectorStoreKey.ID,
        text_embedder=text_embedder,
        token_encoder=token_encoder,
    )

    search_engine = LocalSearch(
        model=chat_model,
        context_builder=context_builder,
        token_encoder=token_encoder,
        model_params={"max_tokens": 2000, "temperature": 0.0},
        context_builder_params={
            "text_unit_prop": 0.5,
            "community_prop": 0.1,
            "conversation_history_max_turns": 0,
            "conversation_history_user_turns_only": True,
            "top_k_mapped_entities": 10,
            "top_k_relationships": 10,
            "include_entity_rank": True,
            "include_relationship_weight": True,
            "include_community_rank": False,
            "return_candidate_context": False,
            "embedding_vectorstore_key": EntityVectorStoreKey.ID,
            "max_tokens": 12000,
        },
        response_type="multiple paragraphs",
    )

    #result = thread_pool.submit(run_async, search_engine.search(query_text)).result()
    result = run_async(search_engine.search(query_text))

    # context ì €ì¥
    context_files = {}
    for key, df in result.context_data.items():
        if isinstance(df, pd.DataFrame):
            output_file = f"context_data_{key}.csv"
            df.to_csv(output_file, index=False, encoding='utf-8-sig')
            context_files[key] = output_file

    return {
        'response': result.response
    }
    
def main():
    """ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤"""
    print("ğŸš€ QAGen ë²”ìš© ì •í™•ë„ ê³„ì‚°ê¸°")
    print("=" * 50)
    
    load_dotenv()
    api_key = os.getenv("GRAPHRAG_API_KEY", "").strip()
    
    # ê³„ì‚°ê¸° ì´ˆê¸°í™”
    if api_key:
        llm_evaluator = LLMEvaluator(api_key=api_key)
    else:
        print("API í‚¤ ì—†ì´ í´ë°± ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        llm_evaluator = LLMEvaluator()
    
    calculator = AccuracyCalculator(llm_evaluator)
    
    while True:
        print("\n" + "="*50)
        print("ìƒˆë¡œìš´ QA í‰ê°€ (ì¢…ë£Œ: 'quit')")
        print("="*50)
        
        # ì‚¬ìš©ì ì…ë ¥
        question = input("ğŸ“ ì§ˆë¬¸: ").strip()
        if question.lower() == 'quit':
            break
        
        # answer = input("ğŸ’¬ ë‹µë³€: ").strip()
        # if answer.lower() == 'quit':
        #     break
        result = run_local_query(question)

        answer = result.get('response')
        print("ì„œë²„ ë‹µë³€:", answer)
        
        # ì»¨í…ìŠ¤íŠ¸ ì…ë ¥
        # print("ğŸ“š ì»¨í…ìŠ¤íŠ¸ (ë¹ˆ ì¤„ë¡œ ì¢…ë£Œ):")
        # contexts = []
        # while True:
        #     context = input().strip()
        #     if not context:
        #         break
        #     contexts.append(context)
        # CSV íŒŒì¼ë“¤ ì½ê¸° (ì„œë²„ê°€ ìƒì„±í–ˆë‹¤ê³  ê°€ì •í•˜ê³ , ê°™ì€ ê²½ë¡œì— ìˆë‹¤ê³  ê°€ì •)
        context_files = [
            "./context_data_entities.csv",
            "./context_data_relationships.csv",
            "./context_data_reports.csv",
            "./context_data_sources.csv"
        ]
        contexts = []
        for file in context_files:
            contexts.extend(read_csv_as_text_list(file))

        # Ground truth (ì„ íƒì‚¬í•­)
        # ground_truth = input("ğŸ¯ ì •ë‹µ ì°¸ê³  (ì„ íƒì‚¬í•­): ").strip()
        # ground_truth = ground_truth if ground_truth else None
        #ground_truth = '. '.join(calculator._extract_key_information(answer))

        # ì •í™•ë„ ê³„ì‚°
        try:
            print("\nâ³ ê³„ì‚° ì¤‘...")
            result = calculator.calculate_accuracy(question, answer, contexts)
            
            # ê²°ê³¼ ì¶œë ¥
            print("\n" + "ğŸ¯ í‰ê°€ ê²°ê³¼")
            print("="*30)
            print(f"ìµœì¢… ì •í™•ë„: {result['percentage']}%")
            print(f"ë“±ê¸‰: {result['grade']} ({result['level']})")
            
            print("\nğŸ“Š ì„¸ë¶€ ì ìˆ˜:")
            for name, score in result['metrics'].items():
                weight = result['weights'][name]
                print(f"  â€¢ {name}: {score} (ê°€ì¤‘ì¹˜: {weight})")
            
            print("\nğŸ§® ê³„ì‚° ê³¼ì •:")
            for breakdown in result['detailed_breakdown'].values():
                print(f"  â€¢ {breakdown}")
            
            total_sum = sum(result['weights'][name] * result['metrics'][name] 
                           for name in result['metric_names'])
            print(f"  = {round(total_sum, 3)}")
            
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    #print("ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    
    # calculator = AccuracyCalculator()  # API í‚¤ ì—†ì´ í…ŒìŠ¤íŠ¸
    
    # test_result = calculator.calculate_accuracy(
    #     question="íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì€?",
    #     answer="ë¦¬ìŠ¤íŠ¸ëŠ” ë³€ê²½ ê°€ëŠ¥í•˜ê³  íŠœí”Œì€ ë³€ê²½ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. ë¦¬ìŠ¤íŠ¸ëŠ” []ë¡œ í‘œí˜„í•˜ê³  íŠœí”Œì€ ()ë¡œ í‘œí˜„í•©ë‹ˆë‹¤.",
    #     contexts=["íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ëŠ” mutableí•˜ê³  íŠœí”Œì€ immutableí•©ë‹ˆë‹¤.", "ë¦¬ìŠ¤íŠ¸ ë¬¸ë²•: [1,2,3], íŠœí”Œ ë¬¸ë²•: (1,2,3)"],
    #     ground_truth="ë¦¬ìŠ¤íŠ¸ëŠ” mutable, íŠœí”Œì€ immutable"
    # )
    
    # print(json.dumps(test_result, ensure_ascii=False, indent=2))
    
    # print("\n" + "="*50)
    # print("ì‹¤ì œ ì‚¬ìš©: main() í•¨ìˆ˜ ì‹¤í–‰")
    main()  # ì£¼ì„ í•´ì œí•˜ì—¬ ëŒ€í™”í˜• ëª¨ë“œ ì‹¤í–‰